{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"iEVuaU8KRwEk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install tensorboardX\n","! pip install tensorrt"],"metadata":{"id":"HZEX7Jv0QsKp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install cython -U\n","!git clone https://github.com/pytorch/fairseq.git\n","%cd fairseq\n","!pip install --quiet --editable .\n","!pip install --quiet sentencepiece"],"metadata":{"id":"DeF5UwpQQxK8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Baseline\n"],"metadata":{"id":"1dmGV-ODybQn"}},{"cell_type":"code","source":["import tensorflow as tf\n","with tf.device('/device:GPU:0'):\n","  ! MAX_EPOCH=10 \\\n","      LR=1e-05 \\\n","      BATCH_SIZE=32 \\\n","      CUDA_VISIBLE_DEVICES=0 fairseq-train /content/drive/MyDrive/NLP/anli/R1/orig/bin \\\n","          --save-dir /content/drive/MyDrive/NLP/checkpoints/R1/baseline \\\n","          --reset-optimizer  \\\n","          --reset-dataloader  \\\n","          --reset-meters  \\\n","          --best-checkpoint-metric accuracy  \\\n","          --maximize-best-checkpoint-metric  \\\n","          --no-epoch-checkpoints \\\n","          --find-unused-parameters \\\n","          --distributed-world-size 1 \\\n","          --task sentence_prediction  \\\n","          --num-classes 3  \\\n","          --init-token 0  \\\n","          --separator-token 2   \\\n","          --max-positions 512  \\\n","          --shorten-method \"truncate\"  \\\n","          --arch roberta \\\n","          --dropout 0.1  \\\n","          --attention-dropout 0.1  \\\n","          --weight-decay 0.1  \\\n","          --criterion sentence_prediction  \\\n","          --classification-head-name 'anli' \\\n","          --optimizer adam  \\\n","          --adam-betas '(0.9, 0.98)'  \\\n","          --adam-eps 1e-06  \\\n","          --clip-norm 0.0  \\\n","          --lr-scheduler inverse_sqrt  \\\n","          --lr 1e-05 \\\n","          --fp16  \\\n","          --fp16-init-scale 4  \\\n","          --threshold-loss-scale 1  \\\n","          --fp16-scale-window 128  \\\n","          --batch-size 32  \\\n","          --required-batch-size-multiple 1  \\\n","          --max-tokens 4400 \\\n","          --update-freq 1  \\\n","          --max-update 123873 \\\n","          --max-epoch 5 \\\n","          --seed 100"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XgcRPmTwQ1tK","executionInfo":{"status":"ok","timestamp":1686484690679,"user_tz":-120,"elapsed":674407,"user":{"displayName":"Louise Leibbrandt","userId":"11762724927321883770"}},"outputId":"64b76a58-c8fe-4206-971c-88f3e3d2067e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-11 11:47:00.347975: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2023-06-11 11:47:01 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n","2023-06-11 11:47:05 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 100, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4400, 'batch_size': 32, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 123873, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/drive/MyDrive/NLP/checkpoints/R1/baseline', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=100, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=1.0, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4400, batch_size=32, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4400, batch_size_valid=32, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta', max_epoch=5, max_update=123873, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/content/drive/MyDrive/NLP/checkpoints/R1/baseline', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='/content/drive/MyDrive/NLP/anli/R1/orig/bin', num_classes=3, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, d2v2_multi=False, classification_head_name='anli', regression_target=False, report_mcc=False, report_acc_and_f1=False, report_pearson_and_spearman=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.1, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, max_positions=512, dropout=0.1, attention_dropout=0.1, no_seed_provided=False, encoder_layers=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_attention_heads=12, activation_dropout=0.0, pooler_dropout=0.0, max_source_positions=512, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta'), 'task': {'_name': 'sentence_prediction', 'data': '/content/drive/MyDrive/NLP/anli/R1/orig/bin', 'num_classes': 3, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 512, 'regression_target': False, 'classification_head_name': 'anli', 'seed': 100, 'd2v2_multi': False}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'anli', 'regression_target': False, 'report_mcc': False, 'report_acc_and_f1': False, 'report_pearson_and_spearman': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2023-06-11 11:47:07 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n","2023-06-11 11:47:07 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n","2023-06-11 11:47:10 | INFO | fairseq_cli.train | RobertaModel(\n","  (encoder): RobertaEncoder(\n","    (sentence_encoder): TransformerEncoder(\n","      (dropout_module): FairseqDropout()\n","      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","      (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (layers): ModuleList(\n","        (0-11): 12 x TransformerEncoderLayerBase(\n","          (self_attn): MultiheadAttention(\n","            (dropout_module): FairseqDropout()\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout_module): FairseqDropout()\n","          (activation_dropout_module): FairseqDropout()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (lm_head): RobertaLMHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (classification_heads): ModuleDict(\n","    (anli): RobertaClassificationHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","      (out_proj): Linear(in_features=768, out_features=3, bias=True)\n","    )\n","  )\n",")\n","2023-06-11 11:47:10 | INFO | fairseq_cli.train | task: SentencePredictionTask\n","2023-06-11 11:47:10 | INFO | fairseq_cli.train | model: RobertaModel\n","2023-06-11 11:47:10 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n","2023-06-11 11:47:10 | INFO | fairseq_cli.train | num. shared model params: 125,289,564 (num. trained: 125,289,564)\n","2023-06-11 11:47:10 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n","2023-06-11 11:47:11 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin/input0/valid\n","2023-06-11 11:47:12 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin/input1/valid\n","2023-06-11 11:47:13 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin/label/valid\n","2023-06-11 11:47:13 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 1000\n","2023-06-11 11:47:18 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n","2023-06-11 11:47:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-11 11:47:18 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n","2023-06-11 11:47:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-11 11:47:18 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2023-06-11 11:47:18 | INFO | fairseq_cli.train | max tokens per device = 4400 and max sentences per device = 32\n","2023-06-11 11:47:18 | INFO | fairseq.trainer | Preparing to load checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/baseline/checkpoint_last.pt\n","2023-06-11 11:47:18 | INFO | fairseq.trainer | No existing checkpoint found /content/drive/MyDrive/NLP/checkpoints/R1/baseline/checkpoint_last.pt\n","2023-06-11 11:47:18 | INFO | fairseq.trainer | loading train data for epoch 1\n","2023-06-11 11:47:20 | INFO | fairseq.data.data_utils | loaded 10,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin/input0/train\n","2023-06-11 11:47:21 | INFO | fairseq.data.data_utils | loaded 10,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin/input1/train\n","2023-06-11 11:47:22 | INFO | fairseq.data.data_utils | loaded 10,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin/label/train\n","2023-06-11 11:47:22 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 10000\n","2023-06-11 11:47:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 11:47:22 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-11 11:47:22 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-11 11:47:22 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-11 11:47:22 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n","2023-06-11 11:47:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 11:47:22 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-11 11:47:22 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-11 11:47:22 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-11 11:47:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 377\n","epoch 001:   0% 0/377 [00:00<?, ?it/s]2023-06-11 11:47:22 | INFO | fairseq.trainer | begin training epoch 1\n","2023-06-11 11:47:22 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 001: 100% 376/377 [01:46<00:00,  3.96it/s, loss=1.572, nll_loss=0.016, accuracy=39.7, wps=9324.7, ups=3.69, wpb=2530.2, bsz=26.4, num_updates=300, lr=7.5e-07, gnorm=5.493, loss_scale=16, train_wall=27, gb_free=10.5, wall=90]2023-06-11 11:49:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 11:49:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   2% 1/40 [00:00<00:04,  8.81it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   8% 3/40 [00:00<00:03, 11.54it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  12% 5/40 [00:00<00:03, 11.65it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  18% 7/40 [00:00<00:02, 11.93it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  22% 9/40 [00:00<00:02, 12.01it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  28% 11/40 [00:00<00:02, 12.32it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  32% 13/40 [00:01<00:02, 12.26it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  38% 15/40 [00:01<00:02, 12.31it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  42% 17/40 [00:01<00:01, 12.24it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  48% 19/40 [00:01<00:01, 13.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  52% 21/40 [00:01<00:01, 12.47it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  57% 23/40 [00:01<00:01, 11.82it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  62% 25/40 [00:02<00:01, 12.16it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  68% 27/40 [00:02<00:01, 12.55it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  72% 29/40 [00:02<00:00, 13.13it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  78% 31/40 [00:02<00:00, 12.40it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  82% 33/40 [00:02<00:00, 12.28it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  88% 35/40 [00:02<00:00, 11.92it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  92% 37/40 [00:03<00:00, 12.34it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  98% 39/40 [00:03<00:00, 13.08it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 11:49:13 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.596 | nll_loss 0.016 | accuracy 33.4 | wps 30732.6 | wpb 2452.7 | bsz 25 | num_updates 377\n","2023-06-11 11:49:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 377 updates\n","2023-06-11 11:49:13 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/baseline/checkpoint_best.pt\n","2023-06-11 11:49:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/baseline/checkpoint_best.pt\n","2023-06-11 11:49:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/baseline/checkpoint_best.pt (epoch 1 @ 377 updates, score 33.4) (writing took 22.347076770999934 seconds)\n","2023-06-11 11:49:35 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2023-06-11 11:49:35 | INFO | train | epoch 001 | loss 1.568 | nll_loss 0.016 | accuracy 40.1 | wps 7434 | ups 2.91 | wpb 2553.7 | bsz 26.5 | num_updates 377 | lr 9.425e-07 | gnorm 5.461 | loss_scale 16 | train_wall 105 | gb_free 10.5 | wall 137\n","2023-06-11 11:49:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 11:49:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 377\n","epoch 002:   0% 0/377 [00:00<?, ?it/s]2023-06-11 11:49:35 | INFO | fairseq.trainer | begin training epoch 2\n","2023-06-11 11:49:35 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 002: 100% 376/377 [01:48<00:00,  3.85it/s, loss=1.563, nll_loss=0.016, accuracy=41.4, wps=9503.4, ups=3.72, wpb=2556.6, bsz=26.6, num_updates=700, lr=1.75e-06, gnorm=5.082, loss_scale=128, train_wall=26, gb_free=10.3, wall=231]2023-06-11 11:51:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 11:51:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   2% 1/40 [00:00<00:04,  9.32it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   8% 3/40 [00:00<00:03, 11.48it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  12% 5/40 [00:00<00:03, 11.38it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  18% 7/40 [00:00<00:02, 11.81it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  22% 9/40 [00:00<00:02, 11.86it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  28% 11/40 [00:00<00:02, 12.37it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  32% 13/40 [00:01<00:02, 12.33it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  38% 15/40 [00:01<00:02, 12.34it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  42% 17/40 [00:01<00:01, 12.23it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  48% 19/40 [00:01<00:01, 13.07it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  52% 21/40 [00:01<00:01, 12.33it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  57% 23/40 [00:01<00:01, 11.74it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  62% 25/40 [00:02<00:01, 12.17it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  68% 27/40 [00:02<00:01, 12.63it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  72% 29/40 [00:02<00:00, 13.05it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  78% 31/40 [00:02<00:00, 12.32it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  82% 33/40 [00:02<00:00, 12.27it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  88% 35/40 [00:02<00:00, 11.92it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  92% 37/40 [00:03<00:00, 12.36it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  98% 39/40 [00:03<00:00, 12.98it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 11:51:27 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 1.643 | nll_loss 0.017 | accuracy 33.3 | wps 30574.1 | wpb 2452.7 | bsz 25 | num_updates 754 | best_accuracy 33.4\n","2023-06-11 11:51:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 754 updates\n","2023-06-11 11:51:27 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/baseline/checkpoint_last.pt\n","2023-06-11 11:51:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/baseline/checkpoint_last.pt\n","2023-06-11 11:51:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/baseline/checkpoint_last.pt (epoch 2 @ 754 updates, score 33.3) (writing took 8.701346182000066 seconds)\n","2023-06-11 11:51:36 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2023-06-11 11:51:36 | INFO | train | epoch 002 | loss 1.563 | nll_loss 0.016 | accuracy 41.2 | wps 7972.5 | ups 3.12 | wpb 2553.7 | bsz 26.5 | num_updates 754 | lr 1.885e-06 | gnorm 5.136 | loss_scale 128 | train_wall 106 | gb_free 10.6 | wall 257\n","2023-06-11 11:51:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 11:51:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 377\n","epoch 003:   0% 0/377 [00:00<?, ?it/s]2023-06-11 11:51:36 | INFO | fairseq.trainer | begin training epoch 3\n","2023-06-11 11:51:36 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 003: 100% 376/377 [01:50<00:00,  3.70it/s, loss=1.559, nll_loss=0.016, accuracy=42.1, wps=9371.3, ups=3.55, wpb=2636.3, bsz=27.5, num_updates=1100, lr=2.75e-06, gnorm=4.401, loss_scale=1024, train_wall=28, gb_free=10.5, wall=360]2023-06-11 11:53:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 11:53:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   2% 1/40 [00:00<00:06,  5.88it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   5% 2/40 [00:00<00:05,  7.10it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  10% 4/40 [00:00<00:03,  9.60it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  15% 6/40 [00:00<00:03, 10.26it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  20% 8/40 [00:00<00:02, 10.69it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  25% 10/40 [00:00<00:02, 11.46it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  30% 12/40 [00:01<00:02, 11.40it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  35% 14/40 [00:01<00:02, 11.24it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  40% 16/40 [00:01<00:02, 11.33it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  45% 18/40 [00:01<00:01, 11.34it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  50% 20/40 [00:01<00:01, 11.80it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  55% 22/40 [00:02<00:01, 11.68it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  60% 24/40 [00:02<00:01, 11.60it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  65% 26/40 [00:02<00:01, 11.98it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  70% 28/40 [00:02<00:00, 12.44it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  75% 30/40 [00:02<00:00, 12.42it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  80% 32/40 [00:02<00:00, 12.34it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  85% 34/40 [00:02<00:00, 11.92it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  90% 36/40 [00:03<00:00, 12.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  95% 38/40 [00:03<00:00, 12.42it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset: 100% 40/40 [00:03<00:00, 12.55it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 11:53:30 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 1.636 | nll_loss 0.017 | accuracy 33.3 | wps 29201.9 | wpb 2452.7 | bsz 25 | num_updates 1131 | best_accuracy 33.4\n","2023-06-11 11:53:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1131 updates\n","2023-06-11 11:53:30 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/baseline/checkpoint_last.pt\n","2023-06-11 11:53:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/baseline/checkpoint_last.pt\n","2023-06-11 11:53:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/baseline/checkpoint_last.pt (epoch 3 @ 1131 updates, score 33.3) (writing took 11.635070240999994 seconds)\n","2023-06-11 11:53:42 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2023-06-11 11:53:42 | INFO | train | epoch 003 | loss 1.562 | nll_loss 0.016 | accuracy 40.9 | wps 7646.7 | ups 2.99 | wpb 2553.7 | bsz 26.5 | num_updates 1131 | lr 2.8275e-06 | gnorm 4.662 | loss_scale 1024 | train_wall 108 | gb_free 10.4 | wall 383\n","2023-06-11 11:53:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 11:53:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 377\n","epoch 004:   0% 0/377 [00:00<?, ?it/s]2023-06-11 11:53:42 | INFO | fairseq.trainer | begin training epoch 4\n","2023-06-11 11:53:42 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 004:  75% 281/377 [01:24<00:28,  3.41it/s, loss=1.547, nll_loss=0.016, accuracy=42.4, wps=9057.8, ups=3.57, wpb=2536.4, bsz=26.3, num_updates=1400, lr=3.5e-06, gnorm=4.712, loss_scale=4096, train_wall=27, gb_free=10.3, wall=465]2023-06-11 11:55:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004: 100% 376/377 [01:50<00:00,  4.09it/s, loss=1.536, nll_loss=0.016, accuracy=43.3, wps=9498.5, ups=3.65, wpb=2601.8, bsz=27.2, num_updates=1500, lr=3.75e-06, gnorm=4.394, loss_scale=4096, train_wall=27, gb_free=10.5, wall=492]2023-06-11 11:55:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 11:55:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   2% 1/40 [00:00<00:06,  6.35it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   5% 2/40 [00:00<00:05,  7.28it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  10% 4/40 [00:00<00:03,  9.44it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  15% 6/40 [00:00<00:03, 10.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  20% 8/40 [00:00<00:03, 10.56it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  25% 10/40 [00:00<00:02, 11.22it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  30% 12/40 [00:01<00:02, 11.45it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  35% 14/40 [00:01<00:02, 11.30it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  40% 16/40 [00:01<00:02, 11.33it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  45% 18/40 [00:01<00:01, 11.25it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  50% 20/40 [00:01<00:01, 11.67it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  55% 22/40 [00:02<00:01, 11.68it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  60% 24/40 [00:02<00:01, 11.53it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  65% 26/40 [00:02<00:01, 11.80it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  70% 28/40 [00:02<00:00, 12.29it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  75% 30/40 [00:02<00:00, 12.38it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  80% 32/40 [00:02<00:00, 12.34it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  85% 34/40 [00:02<00:00, 11.89it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  90% 36/40 [00:03<00:00, 12.00it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  95% 38/40 [00:03<00:00, 12.51it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset: 100% 40/40 [00:03<00:00, 12.69it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 11:55:35 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 1.637 | nll_loss 0.017 | accuracy 33 | wps 29088.4 | wpb 2452.7 | bsz 25 | num_updates 1507 | best_accuracy 33.4\n","2023-06-11 11:55:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1507 updates\n","2023-06-11 11:55:35 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/baseline/checkpoint_last.pt\n","2023-06-11 11:55:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/baseline/checkpoint_last.pt\n","2023-06-11 11:55:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/baseline/checkpoint_last.pt (epoch 4 @ 1507 updates, score 33.0) (writing took 8.469287827000016 seconds)\n","2023-06-11 11:55:44 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2023-06-11 11:55:44 | INFO | train | epoch 004 | loss 1.548 | nll_loss 0.016 | accuracy 41.8 | wps 7845.7 | ups 3.07 | wpb 2552.3 | bsz 26.5 | num_updates 1507 | lr 3.7675e-06 | gnorm 4.502 | loss_scale 4096 | train_wall 107 | gb_free 10.7 | wall 506\n","2023-06-11 11:55:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 11:55:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 377\n","epoch 005:   0% 0/377 [00:00<?, ?it/s]2023-06-11 11:55:44 | INFO | fairseq.trainer | begin training epoch 5\n","2023-06-11 11:55:44 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 005:  34% 129/377 [00:39<01:22,  3.01it/s, loss=1.524, nll_loss=0.016, accuracy=44.4, wps=6027.7, ups=2.35, wpb=2568.5, bsz=26.7, num_updates=1600, lr=4e-06, gnorm=4.956, loss_scale=8192, train_wall=30, gb_free=10.5, wall=535]2023-06-11 11:56:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  71% 267/377 [01:23<00:30,  3.59it/s, loss=1.517, nll_loss=0.016, accuracy=44.4, wps=8513, ups=3.41, wpb=2493.8, bsz=25.8, num_updates=1700, lr=4.25e-06, gnorm=5.626, loss_scale=4096, train_wall=29, gb_free=10.4, wall=564]2023-06-11 11:57:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005: 100% 376/377 [01:55<00:00,  3.98it/s, loss=1.516, nll_loss=0.016, accuracy=45, wps=7635.3, ups=2.96, wpb=2581.1, bsz=26.8, num_updates=1800, lr=4.5e-06, gnorm=6.07, loss_scale=4096, train_wall=32, gb_free=10.5, wall=598]2023-06-11 11:57:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 11:57:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 005 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   2% 1/40 [00:00<00:04,  8.87it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   8% 3/40 [00:00<00:03, 11.35it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  12% 5/40 [00:00<00:03, 11.56it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  18% 7/40 [00:00<00:02, 11.84it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  22% 9/40 [00:00<00:02, 11.88it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  28% 11/40 [00:00<00:02, 12.31it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  32% 13/40 [00:01<00:02, 12.25it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  38% 15/40 [00:01<00:02, 12.31it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  42% 17/40 [00:01<00:01, 12.21it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  48% 19/40 [00:01<00:01, 12.93it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  52% 21/40 [00:01<00:01, 12.43it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  57% 23/40 [00:01<00:01, 11.97it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  62% 25/40 [00:02<00:01, 12.50it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  68% 27/40 [00:02<00:01, 12.86it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  72% 29/40 [00:02<00:00, 13.23it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  78% 31/40 [00:02<00:00, 12.24it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  82% 33/40 [00:02<00:00, 11.54it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  88% 35/40 [00:02<00:00, 11.51it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  92% 37/40 [00:03<00:00, 12.17it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  98% 39/40 [00:03<00:00, 12.85it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 11:57:43 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 1.744 | nll_loss 0.018 | accuracy 31.8 | wps 30484.1 | wpb 2452.7 | bsz 25 | num_updates 1882 | best_accuracy 33.4\n","2023-06-11 11:57:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1882 updates\n","2023-06-11 11:57:43 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/baseline/checkpoint_last.pt\n","2023-06-11 11:57:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/baseline/checkpoint_last.pt\n","2023-06-11 11:57:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/baseline/checkpoint_last.pt (epoch 5 @ 1882 updates, score 31.8) (writing took 11.37649245600005 seconds)\n","2023-06-11 11:57:54 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n","2023-06-11 11:57:54 | INFO | train | epoch 005 | loss 1.512 | nll_loss 0.016 | accuracy 45.1 | wps 7343.2 | ups 2.88 | wpb 2552.5 | bsz 26.5 | num_updates 1882 | lr 4.705e-06 | gnorm 5.81 | loss_scale 4096 | train_wall 112 | gb_free 10.5 | wall 636\n","2023-06-11 11:57:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 11:57:54 | INFO | fairseq_cli.train | done training in 632.0 seconds\n"]}]},{"cell_type":"code","source":["from fairseq.models.roberta import RobertaModel\n","roberta = RobertaModel.from_pretrained(\n","    '/content/drive/MyDrive/NLP/checkpoints/R1/baseline',\n","    checkpoint_file='checkpoint_best.pt',\n","    data_name_or_path='/content/drive/MyDrive/NLP/anli/R1/orig/bin'\n",")\n","roberta.eval()"],"metadata":{"id":"XVYVF3X6UXLw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["genre = ['R1','R2','R3']\n","for g in genre:\n","  input0 = open('/content/drive/MyDrive/NLP/anli/'+g+'/orig/test.raw.input0', \"r\")\n","  input1 = open('/content/drive/MyDrive/NLP/anli/'+g+'/orig/test.raw.input1', \"r\")\n","  label = open('/content/drive/MyDrive/NLP/anli/'+g+'/orig/test.raw.label', \"r\")\n","\n","  accuracy = 0\n","  total = 0\n","  for (x1, x2, y) in zip(input0, input1, label):\n","    tokens = roberta.encode(x1, x2)\n","    idx = roberta.predict('anli', tokens).argmax().item()\n","    dictionary = roberta.task.label_dictionary\n","    pred = dictionary[idx + dictionary.nspecial]\n","    total = total + 1\n","    if  (pred == y.strip()) :\n","      accuracy = accuracy + 1\n","\n","  print(g,\": \",accuracy)\n","  print(g,\": \",total)"],"metadata":{"id":"n76AOnxOUbui","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686507406073,"user_tz":-120,"elapsed":875072,"user":{"displayName":"Louise Leibbrandt","userId":"11762724927321883770"}},"outputId":"579a30aa-8a8f-430b-9e92-22db31ebb20a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["R1 :  332\n","R1 :  1000\n","R2 :  333\n","R2 :  1000\n","R3 :  396\n","R3 :  1200\n"]}]},{"cell_type":"markdown","source":["# EDA"],"metadata":{"id":"Q2IqijoNylVU"}},{"cell_type":"code","source":["import tensorflow as tf\n","with tf.device('/device:GPU:0'):\n","  ! MAX_EPOCH=10 \\\n","      LR=1e-05 \\\n","      BATCH_SIZE=32 \\\n","      CUDA_VISIBLE_DEVICES=0 fairseq-train /content/drive/MyDrive/NLP/anli/R1/orig/bin_eda2 \\\n","          --save-dir /content/drive/MyDrive/NLP/checkpoints/R1/eda \\\n","          --reset-optimizer  \\\n","          --reset-dataloader  \\\n","          --reset-meters  \\\n","          --best-checkpoint-metric accuracy  \\\n","          --maximize-best-checkpoint-metric  \\\n","          --no-epoch-checkpoints \\\n","          --find-unused-parameters \\\n","          --distributed-world-size 1 \\\n","          --task sentence_prediction  \\\n","          --num-classes 3  \\\n","          --init-token 0  \\\n","          --separator-token 2   \\\n","          --max-positions 512  \\\n","          --shorten-method \"truncate\"  \\\n","          --arch roberta \\\n","          --dropout 0.1  \\\n","          --attention-dropout 0.1  \\\n","          --weight-decay 0.1  \\\n","          --criterion sentence_prediction  \\\n","          --classification-head-name 'anli' \\\n","          --optimizer adam  \\\n","          --adam-betas '(0.9, 0.98)'  \\\n","          --adam-eps 1e-06  \\\n","          --clip-norm 0.0  \\\n","          --lr-scheduler inverse_sqrt  \\\n","          --lr 1e-05 \\\n","          --fp16  \\\n","          --fp16-init-scale 4  \\\n","          --threshold-loss-scale 1  \\\n","          --fp16-scale-window 128  \\\n","          --batch-size 32  \\\n","          --required-batch-size-multiple 1  \\\n","          --max-tokens 4400 \\\n","          --update-freq 1  \\\n","          --max-update 123873 \\\n","          --max-epoch 5 \\\n","          --seed 100"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MiSWyek8Uc0F","executionInfo":{"status":"ok","timestamp":1686487152752,"user_tz":-120,"elapsed":2428744,"user":{"displayName":"Louise Leibbrandt","userId":"11762724927321883770"}},"outputId":"f159e67c-8160-4d1a-eb72-77e5fc401374"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-11 11:58:52.823986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2023-06-11 11:58:55 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n","2023-06-11 11:59:01 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 100, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4400, 'batch_size': 32, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 123873, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/drive/MyDrive/NLP/checkpoints/R1/eda', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=100, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=1.0, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4400, batch_size=32, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4400, batch_size_valid=32, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta', max_epoch=5, max_update=123873, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/content/drive/MyDrive/NLP/checkpoints/R1/eda', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='/content/drive/MyDrive/NLP/anli/R1/orig/bin_eda2', num_classes=3, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, d2v2_multi=False, classification_head_name='anli', regression_target=False, report_mcc=False, report_acc_and_f1=False, report_pearson_and_spearman=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.1, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, max_positions=512, dropout=0.1, attention_dropout=0.1, no_seed_provided=False, encoder_layers=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_attention_heads=12, activation_dropout=0.0, pooler_dropout=0.0, max_source_positions=512, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta'), 'task': {'_name': 'sentence_prediction', 'data': '/content/drive/MyDrive/NLP/anli/R1/orig/bin_eda2', 'num_classes': 3, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 512, 'regression_target': False, 'classification_head_name': 'anli', 'seed': 100, 'd2v2_multi': False}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'anli', 'regression_target': False, 'report_mcc': False, 'report_acc_and_f1': False, 'report_pearson_and_spearman': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2023-06-11 11:59:02 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n","2023-06-11 11:59:02 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n","2023-06-11 11:59:05 | INFO | fairseq_cli.train | RobertaModel(\n","  (encoder): RobertaEncoder(\n","    (sentence_encoder): TransformerEncoder(\n","      (dropout_module): FairseqDropout()\n","      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","      (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (layers): ModuleList(\n","        (0-11): 12 x TransformerEncoderLayerBase(\n","          (self_attn): MultiheadAttention(\n","            (dropout_module): FairseqDropout()\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout_module): FairseqDropout()\n","          (activation_dropout_module): FairseqDropout()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (lm_head): RobertaLMHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (classification_heads): ModuleDict(\n","    (anli): RobertaClassificationHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","      (out_proj): Linear(in_features=768, out_features=3, bias=True)\n","    )\n","  )\n",")\n","2023-06-11 11:59:05 | INFO | fairseq_cli.train | task: SentencePredictionTask\n","2023-06-11 11:59:05 | INFO | fairseq_cli.train | model: RobertaModel\n","2023-06-11 11:59:05 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n","2023-06-11 11:59:05 | INFO | fairseq_cli.train | num. shared model params: 125,289,564 (num. trained: 125,289,564)\n","2023-06-11 11:59:05 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n","2023-06-11 11:59:06 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_eda2/input0/valid\n","2023-06-11 11:59:07 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_eda2/input1/valid\n","2023-06-11 11:59:08 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_eda2/label/valid\n","2023-06-11 11:59:08 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 1000\n","2023-06-11 11:59:15 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n","2023-06-11 11:59:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-11 11:59:15 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n","2023-06-11 11:59:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-11 11:59:15 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2023-06-11 11:59:15 | INFO | fairseq_cli.train | max tokens per device = 4400 and max sentences per device = 32\n","2023-06-11 11:59:15 | INFO | fairseq.trainer | Preparing to load checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/eda/checkpoint_last.pt\n","2023-06-11 11:59:15 | INFO | fairseq.trainer | No existing checkpoint found /content/drive/MyDrive/NLP/checkpoints/R1/eda/checkpoint_last.pt\n","2023-06-11 11:59:15 | INFO | fairseq.trainer | loading train data for epoch 1\n","2023-06-11 11:59:16 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_eda2/input0/train\n","2023-06-11 11:59:18 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_eda2/input1/train\n","2023-06-11 11:59:19 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_eda2/label/train\n","2023-06-11 11:59:19 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 50000\n","2023-06-11 11:59:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 11:59:19 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-11 11:59:19 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-11 11:59:19 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-11 11:59:19 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n","2023-06-11 11:59:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 11:59:19 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-11 11:59:19 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-11 11:59:19 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-11 11:59:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1640\n","epoch 001:   0% 0/1640 [00:00<?, ?it/s]2023-06-11 11:59:20 | INFO | fairseq.trainer | begin training epoch 1\n","2023-06-11 11:59:20 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 001:  94% 1537/1640 [06:52<00:27,  3.74it/s, loss=1.543, nll_loss=0.018, accuracy=41.9, wps=9922.8, ups=3.73, wpb=2656.9, bsz=30.3, num_updates=1500, lr=3.75e-06, gnorm=4.113, loss_scale=8192, train_wall=26, gb_free=10.6, wall=407]2023-06-11 12:06:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8192.0\n","epoch 001:  96% 1568/1640 [07:01<00:19,  3.70it/s, loss=1.543, nll_loss=0.018, accuracy=41.9, wps=9922.8, ups=3.73, wpb=2656.9, bsz=30.3, num_updates=1500, lr=3.75e-06, gnorm=4.113, loss_scale=8192, train_wall=26, gb_free=10.6, wall=407]2023-06-11 12:06:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 001: 100% 1639/1640 [07:19<00:00,  3.77it/s, loss=1.534, nll_loss=0.017, accuracy=43.7, wps=9787.6, ups=3.69, wpb=2656, bsz=30.3, num_updates=1600, lr=4e-06, gnorm=4.303, loss_scale=4096, train_wall=27, gb_free=10.5, wall=435]2023-06-11 12:06:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 12:06:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   2% 1/40 [00:00<00:05,  7.06it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   5% 2/40 [00:00<00:04,  7.75it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  10% 4/40 [00:00<00:03,  9.35it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  15% 6/40 [00:00<00:03, 10.18it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  20% 8/40 [00:00<00:02, 10.74it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  25% 10/40 [00:00<00:02, 11.38it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  30% 12/40 [00:01<00:02, 11.52it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  35% 14/40 [00:01<00:02, 11.53it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  40% 16/40 [00:01<00:02, 11.78it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  45% 18/40 [00:01<00:01, 11.95it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  50% 20/40 [00:01<00:01, 12.51it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  55% 22/40 [00:01<00:01, 12.62it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  60% 24/40 [00:02<00:01, 12.39it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  65% 26/40 [00:02<00:01, 12.60it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  70% 28/40 [00:02<00:00, 12.98it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  75% 30/40 [00:02<00:00, 12.93it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  80% 32/40 [00:02<00:00, 12.81it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  85% 34/40 [00:02<00:00, 12.26it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  90% 36/40 [00:03<00:00, 12.31it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  95% 38/40 [00:03<00:00, 12.90it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset: 100% 40/40 [00:03<00:00, 12.89it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 12:06:43 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.647 | nll_loss 0.017 | accuracy 33.6 | wps 30058 | wpb 2452.7 | bsz 25 | num_updates 1638\n","2023-06-11 12:06:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1638 updates\n","2023-06-11 12:06:43 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda/checkpoint_best.pt\n","2023-06-11 12:06:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda/checkpoint_best.pt\n","2023-06-11 12:07:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/eda/checkpoint_best.pt (epoch 1 @ 1638 updates, score 33.6) (writing took 33.76585208100005 seconds)\n","2023-06-11 12:07:17 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2023-06-11 12:07:17 | INFO | train | epoch 001 | loss 1.559 | nll_loss 0.018 | accuracy 41.3 | wps 9226.4 | ups 3.46 | wpb 2669.2 | bsz 30.5 | num_updates 1638 | lr 4.095e-06 | gnorm 4.472 | loss_scale 4096 | train_wall 431 | gb_free 10.6 | wall 481\n","2023-06-11 12:07:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 12:07:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1640\n","epoch 002:   0% 0/1640 [00:00<?, ?it/s]2023-06-11 12:07:17 | INFO | fairseq.trainer | begin training epoch 2\n","2023-06-11 12:07:17 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 002:   4% 60/1640 [00:17<07:33,  3.48it/s]2023-06-11 12:07:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  13% 210/1640 [00:58<06:26,  3.70it/s, loss=1.496, nll_loss=0.017, accuracy=47.1, wps=9479.3, ups=3.55, wpb=2666.6, bsz=30.4, num_updates=1800, lr=4.5e-06, gnorm=5.082, loss_scale=4096, train_wall=27, gb_free=10.6, wall=528]2023-06-11 12:08:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  22% 360/1640 [01:38<05:52,  3.63it/s, loss=1.482, nll_loss=0.017, accuracy=46.7, wps=9895, ups=3.7, wpb=2671.2, bsz=30.4, num_updates=1900, lr=4.75e-06, gnorm=6.928, loss_scale=4096, train_wall=26, gb_free=10.8, wall=555]2023-06-11 12:08:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  31% 502/1640 [02:16<04:49,  3.93it/s, loss=1.449, nll_loss=0.017, accuracy=49.9, wps=10051.9, ups=3.76, wpb=2675.3, bsz=30.5, num_updates=2100, lr=5.25e-06, gnorm=8.826, loss_scale=4096, train_wall=26, gb_free=10.5, wall=608]2023-06-11 12:09:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  39% 642/1640 [02:53<04:20,  3.84it/s, loss=1.398, nll_loss=0.016, accuracy=52.4, wps=10047.5, ups=3.74, wpb=2685.1, bsz=30.7, num_updates=2200, lr=5.5e-06, gnorm=7.805, loss_scale=4096, train_wall=26, gb_free=11.1, wall=635]2023-06-11 12:10:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  47% 774/1640 [03:28<03:49,  3.78it/s, loss=1.416, nll_loss=0.016, accuracy=52.6, wps=9911, ups=3.73, wpb=2654.8, bsz=30.5, num_updates=2400, lr=6e-06, gnorm=9.489, loss_scale=4096, train_wall=26, gb_free=10.6, wall=689]2023-06-11 12:10:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  55% 909/1640 [04:05<03:24,  3.57it/s, loss=1.375, nll_loss=0.016, accuracy=54.7, wps=10033.4, ups=3.74, wpb=2684.6, bsz=30.6, num_updates=2500, lr=6.25e-06, gnorm=8.589, loss_scale=4096, train_wall=26, gb_free=10.8, wall=715]2023-06-11 12:11:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  63% 1039/1640 [04:39<02:31,  3.95it/s, loss=1.354, nll_loss=0.015, accuracy=55.2, wps=9829.9, ups=3.73, wpb=2633.6, bsz=30.1, num_updates=2600, lr=6.5e-06, gnorm=8.984, loss_scale=4096, train_wall=26, gb_free=11, wall=742]2023-06-11 12:11:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  73% 1191/1640 [05:19<02:01,  3.70it/s, loss=1.339, nll_loss=0.015, accuracy=56.6, wps=10397, ups=3.83, wpb=2714.6, bsz=30.8, num_updates=2800, lr=7e-06, gnorm=9.091, loss_scale=8192, train_wall=26, gb_free=10.5, wall=795]2023-06-11 12:12:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  81% 1324/1640 [05:54<01:22,  3.85it/s, loss=1.313, nll_loss=0.015, accuracy=57.5, wps=10103.7, ups=3.78, wpb=2672.1, bsz=30.5, num_updates=2900, lr=7.25e-06, gnorm=9.666, loss_scale=4096, train_wall=26, gb_free=10.8, wall=822]2023-06-11 12:13:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  90% 1479/1640 [06:35<00:47,  3.38it/s, loss=1.297, nll_loss=0.015, accuracy=59.5, wps=9821.4, ups=3.77, wpb=2602.4, bsz=29.8, num_updates=3100, lr=7.75e-06, gnorm=10.017, loss_scale=8192, train_wall=26, gb_free=10.5, wall=875]2023-06-11 12:13:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  99% 1617/1640 [07:12<00:06,  3.75it/s, loss=1.304, nll_loss=0.015, accuracy=58.3, wps=9919.3, ups=3.7, wpb=2682.7, bsz=30.7, num_updates=3200, lr=8e-06, gnorm=10.162, loss_scale=4096, train_wall=26, gb_free=10.7, wall=902]2023-06-11 12:14:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002: 100% 1639/1640 [07:17<00:00,  4.01it/s, loss=1.304, nll_loss=0.015, accuracy=58.3, wps=9919.3, ups=3.7, wpb=2682.7, bsz=30.7, num_updates=3200, lr=8e-06, gnorm=10.162, loss_scale=4096, train_wall=26, gb_free=10.7, wall=902]2023-06-11 12:14:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 12:14:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   2% 1/40 [00:00<00:04,  9.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   8% 3/40 [00:00<00:03, 11.60it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  12% 5/40 [00:00<00:03, 11.66it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  18% 7/40 [00:00<00:02, 12.00it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  22% 9/40 [00:00<00:02, 11.99it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  28% 11/40 [00:00<00:02, 12.56it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  32% 13/40 [00:01<00:02, 12.54it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  38% 15/40 [00:01<00:01, 12.61it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  42% 17/40 [00:01<00:01, 12.42it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  48% 19/40 [00:01<00:01, 13.21it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  52% 21/40 [00:01<00:01, 12.41it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  57% 23/40 [00:01<00:01, 11.84it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  62% 25/40 [00:02<00:01, 12.28it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  68% 27/40 [00:02<00:01, 12.63it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  72% 29/40 [00:02<00:00, 13.21it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  78% 31/40 [00:02<00:00, 12.38it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  82% 33/40 [00:02<00:00, 12.25it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  88% 35/40 [00:02<00:00, 11.92it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  92% 37/40 [00:03<00:00, 12.30it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  98% 39/40 [00:03<00:00, 12.83it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 12:14:38 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 1.823 | nll_loss 0.019 | accuracy 33.8 | wps 30801.6 | wpb 2452.7 | bsz 25 | num_updates 3266 | best_accuracy 33.8\n","2023-06-11 12:14:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3266 updates\n","2023-06-11 12:14:38 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda/checkpoint_best.pt\n","2023-06-11 12:14:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda/checkpoint_best.pt\n","2023-06-11 12:15:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/eda/checkpoint_best.pt (epoch 2 @ 3266 updates, score 33.8) (writing took 48.93929937799976 seconds)\n","2023-06-11 12:15:27 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2023-06-11 12:15:27 | INFO | train | epoch 002 | loss 1.381 | nll_loss 0.016 | accuracy 53.8 | wps 8867.2 | ups 3.32 | wpb 2669.5 | bsz 30.5 | num_updates 3266 | lr 8.165e-06 | gnorm 8.574 | loss_scale 4096 | train_wall 428 | gb_free 10.7 | wall 972\n","2023-06-11 12:15:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 12:15:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1640\n","epoch 003:   0% 0/1640 [00:00<?, ?it/s]2023-06-11 12:15:27 | INFO | fairseq.trainer | begin training epoch 3\n","2023-06-11 12:15:27 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 003:   2% 35/1640 [00:09<07:00,  3.82it/s, loss=1.274, nll_loss=0.014, accuracy=60.2, wps=3335.2, ups=1.26, wpb=2647.4, bsz=30.1, num_updates=3300, lr=8.25e-06, gnorm=9.493, loss_scale=4096, train_wall=26, gb_free=10.3, wall=981]2023-06-11 12:15:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  18% 294/1640 [01:22<05:56,  3.78it/s, loss=1.224, nll_loss=0.014, accuracy=61.8, wps=9649.8, ups=3.57, wpb=2702.3, bsz=30.9, num_updates=3500, lr=8.75e-06, gnorm=10.02, loss_scale=4096, train_wall=27, gb_free=10.6, wall=1038]2023-06-11 12:16:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  26% 423/1640 [01:56<05:43,  3.55it/s, loss=1.24, nll_loss=0.014, accuracy=62.1, wps=9787.1, ups=3.7, wpb=2647.6, bsz=30.2, num_updates=3600, lr=9e-06, gnorm=10.137, loss_scale=4096, train_wall=26, gb_free=10.4, wall=1065]2023-06-11 12:17:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  34% 557/1640 [02:32<04:58,  3.62it/s, loss=1.242, nll_loss=0.014, accuracy=61.9, wps=9984.4, ups=3.76, wpb=2654.4, bsz=30.3, num_updates=3800, lr=9.5e-06, gnorm=10.309, loss_scale=4096, train_wall=26, gb_free=10.8, wall=1119]2023-06-11 12:18:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  42% 687/1640 [03:06<04:40,  3.40it/s, loss=1.184, nll_loss=0.014, accuracy=63.1, wps=10256.6, ups=3.83, wpb=2679.9, bsz=30.6, num_updates=3900, lr=9.75e-06, gnorm=10.279, loss_scale=4096, train_wall=26, gb_free=10.6, wall=1145]2023-06-11 12:18:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  50% 820/1640 [03:42<03:27,  3.96it/s, loss=1.186, nll_loss=0.014, accuracy=63.5, wps=9795, ups=3.68, wpb=2662.5, bsz=30.4, num_updates=4000, lr=1e-05, gnorm=10.066, loss_scale=4096, train_wall=27, gb_free=10.9, wall=1172]2023-06-11 12:19:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  58% 958/1640 [04:18<03:10,  3.57it/s, loss=1.222, nll_loss=0.014, accuracy=62.3, wps=10193.3, ups=3.79, wpb=2687.6, bsz=30.7, num_updates=4200, lr=9.759e-06, gnorm=8.814, loss_scale=4096, train_wall=26, gb_free=10.8, wall=1226]2023-06-11 12:19:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  62% 1013/1640 [04:32<02:32,  4.12it/s, loss=1.222, nll_loss=0.014, accuracy=62.3, wps=10193.3, ups=3.79, wpb=2687.6, bsz=30.7, num_updates=4200, lr=9.759e-06, gnorm=8.814, loss_scale=4096, train_wall=26, gb_free=10.8, wall=1226]2023-06-11 12:20:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  70% 1148/1640 [05:09<02:07,  3.85it/s, loss=1.161, nll_loss=0.013, accuracy=64.1, wps=9876, ups=3.73, wpb=2647.2, bsz=30.2, num_updates=4400, lr=9.53463e-06, gnorm=9.645, loss_scale=4096, train_wall=26, gb_free=10.9, wall=1279]2023-06-11 12:20:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  80% 1309/1640 [05:52<01:31,  3.62it/s, loss=1.146, nll_loss=0.013, accuracy=65.1, wps=9889, ups=3.7, wpb=2670.8, bsz=30.6, num_updates=4500, lr=9.42809e-06, gnorm=9.202, loss_scale=2048, train_wall=26, gb_free=10.5, wall=1306]2023-06-11 12:21:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  96% 1570/1640 [07:00<00:18,  3.88it/s, loss=1.125, nll_loss=0.013, accuracy=66.1, wps=10031.6, ups=3.79, wpb=2645.2, bsz=30.2, num_updates=4800, lr=9.12871e-06, gnorm=9.447, loss_scale=4096, train_wall=26, gb_free=10.6, wall=1385]2023-06-11 12:22:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003: 100% 1639/1640 [07:19<00:00,  3.58it/s, loss=1.125, nll_loss=0.013, accuracy=66.1, wps=10031.6, ups=3.79, wpb=2645.2, bsz=30.2, num_updates=4800, lr=9.12871e-06, gnorm=9.447, loss_scale=4096, train_wall=26, gb_free=10.6, wall=1385]2023-06-11 12:22:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 12:22:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   2% 1/40 [00:00<00:04,  9.36it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   8% 3/40 [00:00<00:03, 11.81it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  12% 5/40 [00:00<00:03, 11.54it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  18% 7/40 [00:00<00:02, 11.80it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  22% 9/40 [00:00<00:02, 11.74it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  28% 11/40 [00:00<00:02, 12.37it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  32% 13/40 [00:01<00:02, 12.45it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  38% 15/40 [00:01<00:01, 12.51it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  42% 17/40 [00:01<00:01, 12.42it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  48% 19/40 [00:01<00:01, 13.30it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  52% 21/40 [00:01<00:01, 12.59it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  57% 23/40 [00:01<00:01, 12.11it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  62% 25/40 [00:02<00:01, 12.49it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  68% 27/40 [00:02<00:00, 13.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  72% 29/40 [00:02<00:00, 13.36it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  78% 31/40 [00:02<00:00, 12.50it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  82% 33/40 [00:02<00:00, 12.35it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  88% 35/40 [00:02<00:00, 12.04it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  92% 37/40 [00:02<00:00, 12.55it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  98% 39/40 [00:03<00:00, 13.24it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 12:22:50 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 1.967 | nll_loss 0.02 | accuracy 32.7 | wps 31085.8 | wpb 2452.7 | bsz 25 | num_updates 4895 | best_accuracy 33.8\n","2023-06-11 12:22:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4895 updates\n","2023-06-11 12:22:50 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda/checkpoint_last.pt\n","2023-06-11 12:23:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda/checkpoint_last.pt\n","2023-06-11 12:23:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/eda/checkpoint_last.pt (epoch 3 @ 4895 updates, score 32.7) (writing took 11.053182631000254 seconds)\n","2023-06-11 12:23:01 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2023-06-11 12:23:01 | INFO | train | epoch 003 | loss 1.182 | nll_loss 0.014 | accuracy 63.9 | wps 9578.8 | ups 3.59 | wpb 2668.7 | bsz 30.5 | num_updates 4895 | lr 9.03969e-06 | gnorm 9.759 | loss_scale 4096 | train_wall 429 | gb_free 10.6 | wall 1425\n","2023-06-11 12:23:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 12:23:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1640\n","epoch 004:   0% 0/1640 [00:00<?, ?it/s]2023-06-11 12:23:01 | INFO | fairseq.trainer | begin training epoch 4\n","2023-06-11 12:23:01 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 004:   4% 68/1640 [00:20<07:12,  3.64it/s, loss=1.111, nll_loss=0.013, accuracy=66.8, wps=6343.8, ups=2.4, wpb=2645.8, bsz=30, num_updates=4900, lr=9.03508e-06, gnorm=8.83, loss_scale=4096, train_wall=27, gb_free=10.9, wall=1427]2023-06-11 12:23:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  11% 182/1640 [00:53<07:40,  3.16it/s, loss=1.055, nll_loss=0.012, accuracy=68.2, wps=8786.8, ups=3.27, wpb=2687.4, bsz=30.7, num_updates=5000, lr=8.94427e-06, gnorm=9.453, loss_scale=4096, train_wall=30, gb_free=10.9, wall=1457]2023-06-11 12:23:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  27% 445/1640 [02:02<05:09,  3.86it/s, loss=1.087, nll_loss=0.012, accuracy=67.9, wps=10093.4, ups=3.79, wpb=2666.7, bsz=30.6, num_updates=5300, lr=8.68744e-06, gnorm=9.372, loss_scale=4096, train_wall=26, gb_free=11.1, wall=1538]2023-06-11 12:25:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  35% 581/1640 [02:38<04:26,  3.98it/s, loss=1.085, nll_loss=0.012, accuracy=67.7, wps=10161.2, ups=3.75, wpb=2712.6, bsz=31, num_updates=5400, lr=8.60663e-06, gnorm=8.454, loss_scale=4096, train_wall=26, gb_free=10.4, wall=1565]2023-06-11 12:25:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  43% 702/1640 [03:10<04:09,  3.76it/s, loss=1.055, nll_loss=0.012, accuracy=69, wps=10234.8, ups=3.79, wpb=2703.2, bsz=30.8, num_updates=5500, lr=8.52803e-06, gnorm=9.397, loss_scale=4096, train_wall=26, gb_free=10.4, wall=1591]2023-06-11 12:26:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  59% 964/1640 [04:20<03:12,  3.51it/s, loss=1.021, nll_loss=0.012, accuracy=69.5, wps=9975.7, ups=3.7, wpb=2697.2, bsz=30.7, num_updates=5800, lr=8.30455e-06, gnorm=9.264, loss_scale=4096, train_wall=27, gb_free=10.8, wall=1671]2023-06-11 12:27:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  67% 1093/1640 [04:55<02:39,  3.43it/s, loss=1.032, nll_loss=0.012, accuracy=69.6, wps=9565.5, ups=3.62, wpb=2644.7, bsz=30.1, num_updates=5900, lr=8.23387e-06, gnorm=8.978, loss_scale=4096, train_wall=27, gb_free=10.8, wall=1699]2023-06-11 12:27:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  75% 1232/1640 [05:31<01:54,  3.56it/s, loss=1.013, nll_loss=0.012, accuracy=70.4, wps=10062.7, ups=3.78, wpb=2661.2, bsz=30.5, num_updates=6100, lr=8.09776e-06, gnorm=9.273, loss_scale=4096, train_wall=26, gb_free=10.9, wall=1752]2023-06-11 12:28:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  84% 1373/1640 [06:08<01:07,  3.96it/s, loss=0.991, nll_loss=0.011, accuracy=71.4, wps=9890.4, ups=3.71, wpb=2667.2, bsz=30.5, num_updates=6200, lr=8.03219e-06, gnorm=8.937, loss_scale=4096, train_wall=26, gb_free=10.8, wall=1779]2023-06-11 12:29:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  91% 1500/1640 [06:42<00:39,  3.58it/s, loss=1.034, nll_loss=0.012, accuracy=68.7, wps=10139.7, ups=3.82, wpb=2651.8, bsz=30.3, num_updates=6300, lr=7.96819e-06, gnorm=9.22, loss_scale=4096, train_wall=26, gb_free=10.8, wall=1805]2023-06-11 12:29:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004: 100% 1639/1640 [07:18<00:00,  3.82it/s, loss=1.012, nll_loss=0.012, accuracy=70.2, wps=10065.2, ups=3.78, wpb=2660.5, bsz=30.4, num_updates=6500, lr=7.84465e-06, gnorm=9.584, loss_scale=2048, train_wall=26, gb_free=10.8, wall=1858]2023-06-11 12:30:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 12:30:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   2% 1/40 [00:00<00:05,  7.60it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   5% 2/40 [00:00<00:05,  7.17it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  10% 4/40 [00:00<00:03,  9.57it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  15% 6/40 [00:00<00:03, 10.09it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  20% 8/40 [00:00<00:03, 10.62it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  25% 10/40 [00:00<00:02, 11.24it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  30% 12/40 [00:01<00:02, 11.41it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  35% 14/40 [00:01<00:02, 11.17it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  40% 16/40 [00:01<00:02, 11.30it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  45% 18/40 [00:01<00:01, 11.32it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  50% 20/40 [00:01<00:01, 11.84it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  55% 22/40 [00:01<00:01, 11.70it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  60% 24/40 [00:02<00:01, 11.54it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  65% 26/40 [00:02<00:01, 11.90it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  70% 28/40 [00:02<00:00, 12.35it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  75% 30/40 [00:02<00:00, 12.50it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  80% 32/40 [00:02<00:00, 12.41it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  85% 34/40 [00:02<00:00, 11.98it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  90% 36/40 [00:03<00:00, 12.08it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  95% 38/40 [00:03<00:00, 12.49it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset: 100% 40/40 [00:03<00:00, 12.83it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 12:30:23 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 2.086 | nll_loss 0.021 | accuracy 34.8 | wps 29040.4 | wpb 2452.7 | bsz 25 | num_updates 6525 | best_accuracy 34.8\n","2023-06-11 12:30:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6525 updates\n","2023-06-11 12:30:23 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda/checkpoint_best.pt\n","2023-06-11 12:30:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda/checkpoint_best.pt\n","2023-06-11 12:31:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/eda/checkpoint_best.pt (epoch 4 @ 6525 updates, score 34.8) (writing took 56.228056653000294 seconds)\n","2023-06-11 12:31:20 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2023-06-11 12:31:20 | INFO | train | epoch 004 | loss 1.041 | nll_loss 0.012 | accuracy 69.2 | wps 8722.4 | ups 3.27 | wpb 2669.6 | bsz 30.5 | num_updates 6525 | lr 7.8296e-06 | gnorm 9.268 | loss_scale 4096 | train_wall 430 | gb_free 10.5 | wall 1924\n","2023-06-11 12:31:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 12:31:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1640\n","epoch 005:   0% 0/1640 [00:00<?, ?it/s]2023-06-11 12:31:20 | INFO | fairseq.trainer | begin training epoch 5\n","2023-06-11 12:31:20 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 005:   7% 118/1640 [00:37<07:04,  3.58it/s, loss=0.96, nll_loss=0.011, accuracy=72.5, wps=2946, ups=1.11, wpb=2663.2, bsz=30.3, num_updates=6600, lr=7.78499e-06, gnorm=8.978, loss_scale=4096, train_wall=30, gb_free=10.9, wall=1948]2023-06-11 12:31:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  16% 260/1640 [01:15<06:07,  3.75it/s, loss=0.92, nll_loss=0.01, accuracy=73.8, wps=9142.1, ups=3.43, wpb=2664.4, bsz=30.4, num_updates=6700, lr=7.72667e-06, gnorm=9.522, loss_scale=4096, train_wall=28, gb_free=10.7, wall=1978]2023-06-11 12:32:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  24% 394/1640 [01:51<05:20,  3.89it/s, loss=0.911, nll_loss=0.01, accuracy=73.2, wps=10003.9, ups=3.76, wpb=2663.2, bsz=30.5, num_updates=6900, lr=7.61387e-06, gnorm=9.79, loss_scale=4096, train_wall=26, gb_free=10.6, wall=2032]2023-06-11 12:33:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  33% 543/1640 [02:30<04:53,  3.73it/s, loss=0.934, nll_loss=0.011, accuracy=72.9, wps=10084.3, ups=3.78, wpb=2665.2, bsz=30.4, num_updates=7000, lr=7.55929e-06, gnorm=9.528, loss_scale=4096, train_wall=26, gb_free=10.9, wall=2058]2023-06-11 12:33:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  41% 672/1640 [03:05<04:14,  3.80it/s, loss=0.938, nll_loss=0.011, accuracy=72, wps=10200.2, ups=3.76, wpb=2710.2, bsz=30.9, num_updates=7100, lr=7.50587e-06, gnorm=9.102, loss_scale=4096, train_wall=26, gb_free=10.5, wall=2085]2023-06-11 12:34:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  51% 829/1640 [03:46<03:22,  4.01it/s, loss=0.936, nll_loss=0.011, accuracy=72, wps=10066.7, ups=3.75, wpb=2683.4, bsz=30.6, num_updates=7300, lr=7.40233e-06, gnorm=9.53, loss_scale=4096, train_wall=26, gb_free=10.7, wall=2138]2023-06-11 12:35:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  59% 969/1640 [04:23<02:44,  4.07it/s, loss=0.896, nll_loss=0.01, accuracy=73.6, wps=9871.2, ups=3.74, wpb=2640.9, bsz=30.1, num_updates=7400, lr=7.35215e-06, gnorm=9.416, loss_scale=4096, train_wall=26, gb_free=10.8, wall=2165]2023-06-11 12:35:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  67% 1101/1640 [04:58<02:30,  3.58it/s, loss=0.908, nll_loss=0.01, accuracy=73.6, wps=10198.5, ups=3.76, wpb=2711, bsz=31, num_updates=7600, lr=7.25476e-06, gnorm=8.928, loss_scale=4096, train_wall=26, gb_free=10.6, wall=2218]2023-06-11 12:36:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  76% 1246/1640 [05:36<01:38,  4.01it/s, loss=0.905, nll_loss=0.01, accuracy=73.4, wps=9937.2, ups=3.77, wpb=2636.5, bsz=30, num_updates=7700, lr=7.2075e-06, gnorm=9.362, loss_scale=4096, train_wall=26, gb_free=10.5, wall=2244]2023-06-11 12:36:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  84% 1383/1640 [06:11<01:04,  4.00it/s, loss=0.854, nll_loss=0.01, accuracy=75, wps=10079.5, ups=3.78, wpb=2668.5, bsz=30.7, num_updates=7800, lr=7.16115e-06, gnorm=8.805, loss_scale=4096, train_wall=26, gb_free=10.4, wall=2271]2023-06-11 12:37:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  92% 1513/1640 [06:46<00:35,  3.61it/s, loss=0.964, nll_loss=0.011, accuracy=71.9, wps=9962.4, ups=3.71, wpb=2688.4, bsz=30.7, num_updates=8000, lr=7.07107e-06, gnorm=9.831, loss_scale=4096, train_wall=26, gb_free=11.1, wall=2324]2023-06-11 12:38:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005: 100% 1639/1640 [07:20<00:00,  4.12it/s, loss=0.903, nll_loss=0.01, accuracy=72.9, wps=9926.2, ups=3.7, wpb=2680.1, bsz=30.5, num_updates=8100, lr=7.02728e-06, gnorm=9.589, loss_scale=4096, train_wall=26, gb_free=10.7, wall=2351]2023-06-11 12:38:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 12:38:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 005 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   2% 1/40 [00:00<00:04,  8.36it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   8% 3/40 [00:00<00:03, 11.03it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  12% 5/40 [00:00<00:03, 11.31it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  18% 7/40 [00:00<00:02, 11.80it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  22% 9/40 [00:00<00:02, 12.00it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  28% 11/40 [00:00<00:02, 12.42it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  32% 13/40 [00:01<00:02, 12.37it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  38% 15/40 [00:01<00:02, 12.49it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  42% 17/40 [00:01<00:01, 12.41it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  48% 19/40 [00:01<00:01, 13.23it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  52% 21/40 [00:01<00:01, 12.61it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  57% 23/40 [00:01<00:01, 11.87it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  62% 25/40 [00:02<00:01, 12.33it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  68% 27/40 [00:02<00:01, 12.91it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  72% 29/40 [00:02<00:00, 13.35it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  78% 31/40 [00:02<00:00, 12.47it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  82% 33/40 [00:02<00:00, 12.36it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  88% 35/40 [00:02<00:00, 12.09it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  92% 37/40 [00:02<00:00, 12.58it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  98% 39/40 [00:03<00:00, 13.26it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 12:38:43 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 2.199 | nll_loss 0.022 | accuracy 32.8 | wps 31064.2 | wpb 2452.7 | bsz 25 | num_updates 8154 | best_accuracy 34.8\n","2023-06-11 12:38:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 8154 updates\n","2023-06-11 12:38:43 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda/checkpoint_last.pt\n","2023-06-11 12:38:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda/checkpoint_last.pt\n","2023-06-11 12:38:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/eda/checkpoint_last.pt (epoch 5 @ 8154 updates, score 32.8) (writing took 11.368300591000207 seconds)\n","2023-06-11 12:38:55 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n","2023-06-11 12:38:55 | INFO | train | epoch 005 | loss 0.921 | nll_loss 0.011 | accuracy 72.8 | wps 9557.3 | ups 3.58 | wpb 2669.3 | bsz 30.5 | num_updates 8154 | lr 7.00398e-06 | gnorm 9.482 | loss_scale 4096 | train_wall 431 | gb_free 10.8 | wall 2379\n","2023-06-11 12:38:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 12:38:55 | INFO | fairseq_cli.train | done training in 2374.8 seconds\n"]}]},{"cell_type":"code","source":["from fairseq.models.roberta import RobertaModel\n","roberta = RobertaModel.from_pretrained(\n","    '/content/drive/MyDrive/NLP/checkpoints/R1/eda',\n","    checkpoint_file='checkpoint_best.pt',\n","    data_name_or_path='/content/drive/MyDrive/NLP/anli/R1/orig/bin_eda2'\n",")\n","roberta.eval()"],"metadata":{"id":"jUXhvYuu0T0U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["genre = ['R1','R2','R3']\n","for g in genre:\n","  input0 = open('/content/drive/MyDrive/NLP/anli/'+g+'/orig/test.raw.input0', \"r\")\n","  input1 = open('/content/drive/MyDrive/NLP/anli/'+g+'/orig/test.raw.input1', \"r\")\n","  label = open('/content/drive/MyDrive/NLP/anli/'+g+'/orig/test.raw.label', \"r\")\n","\n","  accuracy = 0\n","  total = 0\n","  for (x1, x2, y) in zip(input0, input1, label):\n","    tokens = roberta.encode(x1, x2)\n","    idx = roberta.predict('anli', tokens).argmax().item()\n","    dictionary = roberta.task.label_dictionary\n","    pred = dictionary[idx + dictionary.nspecial]\n","    total = total + 1\n","    if  (pred == y.strip()) :\n","      accuracy = accuracy + 1\n","\n","\n","  print(g,\": \",accuracy)\n","  print(g,\": \",total)"],"metadata":{"id":"Te_hHbFa0iYn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686508313397,"user_tz":-120,"elapsed":872749,"user":{"displayName":"Louise Leibbrandt","userId":"11762724927321883770"}},"outputId":"850ac88a-eeb7-417b-8edf-7952852682e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["R1 :  317\n","R1 :  1000\n","R2 :  353\n","R2 :  1000\n","R3 :  415\n","R3 :  1200\n"]}]},{"cell_type":"markdown","source":["# UDA"],"metadata":{"id":"M_Y7-jFuy3Xf"}},{"cell_type":"code","source":["import tensorflow as tf\n","with tf.device('/device:GPU:0'):\n","  ! MAX_EPOCH=10 \\\n","      LR=1e-05 \\\n","      BATCH_SIZE=32 \\\n","      CUDA_VISIBLE_DEVICES=0 fairseq-train /content/drive/MyDrive/NLP/anli/R1/orig/bin_uda \\\n","          --save-dir /content/drive/MyDrive/NLP/checkpoints/R1/uda \\\n","          --reset-optimizer  \\\n","          --reset-dataloader  \\\n","          --reset-meters  \\\n","          --best-checkpoint-metric accuracy  \\\n","          --maximize-best-checkpoint-metric  \\\n","          --no-epoch-checkpoints \\\n","          --find-unused-parameters \\\n","          --distributed-world-size 1 \\\n","          --task sentence_prediction  \\\n","          --num-classes 3  \\\n","          --init-token 0  \\\n","          --separator-token 2   \\\n","          --max-positions 512  \\\n","          --shorten-method \"truncate\"  \\\n","          --arch roberta \\\n","          --dropout 0.1  \\\n","          --attention-dropout 0.1  \\\n","          --weight-decay 0.1  \\\n","          --criterion sentence_prediction  \\\n","          --classification-head-name 'anli' \\\n","          --optimizer adam  \\\n","          --adam-betas '(0.9, 0.98)'  \\\n","          --adam-eps 1e-06  \\\n","          --clip-norm 0.0  \\\n","          --lr-scheduler inverse_sqrt  \\\n","          --lr 1e-05 \\\n","          --fp16  \\\n","          --fp16-init-scale 4  \\\n","          --threshold-loss-scale 1  \\\n","          --fp16-scale-window 128  \\\n","          --batch-size 32  \\\n","          --required-batch-size-multiple 1  \\\n","          --max-tokens 4400 \\\n","          --update-freq 1  \\\n","          --max-update 123873 \\\n","          --max-epoch 5 \\\n","          --seed 100"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K4qRobnrUMNd","outputId":"300f5c1a-ea68-4686-dea1-a7bce73e4a1d","executionInfo":{"status":"ok","timestamp":1686497298527,"user_tz":-120,"elapsed":2597622,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-11 14:45:13.566431: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2023-06-11 14:45:14 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n","2023-06-11 14:45:19 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 100, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4400, 'batch_size': 32, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 123873, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/drive/MyDrive/NLP/checkpoints/R1/uda', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=100, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=1.0, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4400, batch_size=32, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4400, batch_size_valid=32, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta', max_epoch=5, max_update=123873, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/content/drive/MyDrive/NLP/checkpoints/R1/uda', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='/content/drive/MyDrive/NLP/anli/R1/orig/bin_uda', num_classes=3, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, d2v2_multi=False, classification_head_name='anli', regression_target=False, report_mcc=False, report_acc_and_f1=False, report_pearson_and_spearman=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.1, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, max_positions=512, dropout=0.1, attention_dropout=0.1, no_seed_provided=False, encoder_layers=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_attention_heads=12, activation_dropout=0.0, pooler_dropout=0.0, max_source_positions=512, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta'), 'task': {'_name': 'sentence_prediction', 'data': '/content/drive/MyDrive/NLP/anli/R1/orig/bin_uda', 'num_classes': 3, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 512, 'regression_target': False, 'classification_head_name': 'anli', 'seed': 100, 'd2v2_multi': False}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'anli', 'regression_target': False, 'report_mcc': False, 'report_acc_and_f1': False, 'report_pearson_and_spearman': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2023-06-11 14:45:19 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n","2023-06-11 14:45:20 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n","2023-06-11 14:45:24 | INFO | fairseq_cli.train | RobertaModel(\n","  (encoder): RobertaEncoder(\n","    (sentence_encoder): TransformerEncoder(\n","      (dropout_module): FairseqDropout()\n","      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","      (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (layers): ModuleList(\n","        (0-11): 12 x TransformerEncoderLayerBase(\n","          (self_attn): MultiheadAttention(\n","            (dropout_module): FairseqDropout()\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout_module): FairseqDropout()\n","          (activation_dropout_module): FairseqDropout()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (lm_head): RobertaLMHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (classification_heads): ModuleDict(\n","    (anli): RobertaClassificationHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","      (out_proj): Linear(in_features=768, out_features=3, bias=True)\n","    )\n","  )\n",")\n","2023-06-11 14:45:24 | INFO | fairseq_cli.train | task: SentencePredictionTask\n","2023-06-11 14:45:24 | INFO | fairseq_cli.train | model: RobertaModel\n","2023-06-11 14:45:24 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n","2023-06-11 14:45:24 | INFO | fairseq_cli.train | num. shared model params: 125,289,564 (num. trained: 125,289,564)\n","2023-06-11 14:45:24 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n","2023-06-11 14:45:24 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_uda/input0/valid\n","2023-06-11 14:45:25 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_uda/input1/valid\n","2023-06-11 14:45:25 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_uda/label/valid\n","2023-06-11 14:45:25 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 1000\n","2023-06-11 14:45:31 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n","2023-06-11 14:45:31 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-11 14:45:31 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n","2023-06-11 14:45:31 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-11 14:45:31 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2023-06-11 14:45:31 | INFO | fairseq_cli.train | max tokens per device = 4400 and max sentences per device = 32\n","2023-06-11 14:45:31 | INFO | fairseq.trainer | Preparing to load checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/uda/checkpoint_last.pt\n","2023-06-11 14:45:31 | INFO | fairseq.trainer | No existing checkpoint found /content/drive/MyDrive/NLP/checkpoints/R1/uda/checkpoint_last.pt\n","2023-06-11 14:45:31 | INFO | fairseq.trainer | loading train data for epoch 1\n","2023-06-11 14:45:32 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_uda/input0/train\n","2023-06-11 14:45:32 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_uda/input1/train\n","2023-06-11 14:45:33 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_uda/label/train\n","2023-06-11 14:45:33 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 50000\n","2023-06-11 14:45:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 14:45:33 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-11 14:45:33 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-11 14:45:33 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-11 14:45:33 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n","2023-06-11 14:45:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 14:45:33 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-11 14:45:33 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-11 14:45:33 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-11 14:45:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1823\n","epoch 001:   0% 0/1823 [00:00<?, ?it/s]2023-06-11 14:45:34 | INFO | fairseq.trainer | begin training epoch 1\n","2023-06-11 14:45:34 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 001:  79% 1438/1823 [06:17<01:40,  3.81it/s, loss=1.555, nll_loss=0.017, accuracy=40.5, wps=9292.4, ups=3.76, wpb=2469.2, bsz=27.6, num_updates=1400, lr=3.5e-06, gnorm=4.386, loss_scale=4096, train_wall=26, gb_free=10.6, wall=371]2023-06-11 14:51:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 001:  86% 1568/1823 [06:51<01:02,  4.06it/s, loss=1.538, nll_loss=0.017, accuracy=42.2, wps=9027.3, ups=3.73, wpb=2418.5, bsz=27.1, num_updates=1500, lr=3.75e-06, gnorm=4.503, loss_scale=4096, train_wall=26, gb_free=10.5, wall=397]2023-06-11 14:52:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 001:  99% 1809/1823 [07:55<00:03,  3.56it/s, loss=1.493, nll_loss=0.017, accuracy=46.2, wps=9083.4, ups=3.81, wpb=2385.1, bsz=26.7, num_updates=1800, lr=4.5e-06, gnorm=5.849, loss_scale=8192, train_wall=26, gb_free=11, wall=476]2023-06-11 14:53:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 001: 100% 1822/1823 [07:58<00:00,  3.59it/s, loss=1.493, nll_loss=0.017, accuracy=46.2, wps=9083.4, ups=3.81, wpb=2385.1, bsz=26.7, num_updates=1800, lr=4.5e-06, gnorm=5.849, loss_scale=8192, train_wall=26, gb_free=11, wall=476]2023-06-11 14:53:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 14:53:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   2% 1/40 [00:00<00:04,  8.81it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   8% 3/40 [00:00<00:03, 11.38it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  12% 5/40 [00:00<00:03, 11.36it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  18% 7/40 [00:00<00:02, 11.80it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  22% 9/40 [00:00<00:02, 12.06it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  28% 11/40 [00:00<00:02, 12.65it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  32% 13/40 [00:01<00:02, 11.92it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  38% 15/40 [00:01<00:02, 12.32it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  42% 17/40 [00:01<00:01, 12.26it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  48% 19/40 [00:01<00:01, 13.13it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  52% 21/40 [00:01<00:01, 12.57it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  57% 23/40 [00:01<00:01, 12.17it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  62% 25/40 [00:02<00:01, 12.39it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  68% 27/40 [00:02<00:01, 12.86it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  72% 29/40 [00:02<00:00, 13.43it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  78% 31/40 [00:02<00:00, 12.81it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  82% 33/40 [00:02<00:00, 12.82it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  88% 35/40 [00:02<00:00, 12.51it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  92% 37/40 [00:02<00:00, 13.00it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  98% 39/40 [00:03<00:00, 13.56it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 14:53:35 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.736 | nll_loss 0.018 | accuracy 32.9 | wps 31410 | wpb 2452.7 | bsz 25 | num_updates 1820\n","2023-06-11 14:53:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1820 updates\n","2023-06-11 14:53:35 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/uda/checkpoint_best.pt\n","2023-06-11 14:53:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/uda/checkpoint_best.pt\n","2023-06-11 14:54:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/uda/checkpoint_best.pt (epoch 1 @ 1820 updates, score 32.9) (writing took 26.755432517999907 seconds)\n","2023-06-11 14:54:02 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2023-06-11 14:54:02 | INFO | train | epoch 001 | loss 1.554 | nll_loss 0.017 | accuracy 41.8 | wps 8802.5 | ups 3.6 | wpb 2443.8 | bsz 27.4 | num_updates 1820 | lr 4.55e-06 | gnorm 4.893 | loss_scale 4096 | train_wall 468 | gb_free 10.5 | wall 512\n","2023-06-11 14:54:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 14:54:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1823\n","epoch 002:   0% 0/1823 [00:00<?, ?it/s]2023-06-11 14:54:02 | INFO | fairseq.trainer | begin training epoch 2\n","2023-06-11 14:54:02 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 002:   6% 117/1823 [00:36<07:25,  3.83it/s, loss=1.472, nll_loss=0.017, accuracy=47.5, wps=3964.3, ups=1.62, wpb=2442.7, bsz=27.5, num_updates=1900, lr=4.75e-06, gnorm=6.669, loss_scale=4096, train_wall=30, gb_free=10.5, wall=538]2023-06-11 14:54:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  17% 305/1823 [01:24<06:45,  3.75it/s, loss=1.422, nll_loss=0.016, accuracy=51.5, wps=9306.9, ups=3.92, wpb=2373.3, bsz=26.5, num_updates=2100, lr=5.25e-06, gnorm=8.693, loss_scale=8192, train_wall=25, gb_free=10.4, wall=590]2023-06-11 14:55:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  24% 435/1823 [01:58<06:22,  3.62it/s, loss=1.418, nll_loss=0.016, accuracy=53.2, wps=9314.7, ups=3.8, wpb=2451.6, bsz=27.4, num_updates=2200, lr=5.5e-06, gnorm=8.912, loss_scale=4096, train_wall=26, gb_free=10.5, wall=616]2023-06-11 14:56:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  32% 576/1823 [02:37<05:11,  4.01it/s, loss=1.408, nll_loss=0.016, accuracy=53, wps=8971.5, ups=3.66, wpb=2452.9, bsz=27.2, num_updates=2300, lr=5.75e-06, gnorm=9.928, loss_scale=4096, train_wall=27, gb_free=10.8, wall=644]2023-06-11 14:56:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  39% 708/1823 [03:11<04:37,  4.02it/s, loss=1.35, nll_loss=0.015, accuracy=56.2, wps=9075.6, ups=3.77, wpb=2408.5, bsz=27.1, num_updates=2500, lr=6.25e-06, gnorm=10.852, loss_scale=4096, train_wall=26, gb_free=10.8, wall=698]2023-06-11 14:57:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  46% 837/1823 [03:45<03:53,  4.23it/s, loss=1.382, nll_loss=0.016, accuracy=54.4, wps=9483.8, ups=3.82, wpb=2480.3, bsz=28.1, num_updates=2600, lr=6.5e-06, gnorm=10.393, loss_scale=4096, train_wall=26, gb_free=10.5, wall=724]2023-06-11 14:57:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  53% 968/1823 [04:19<04:19,  3.30it/s, loss=1.328, nll_loss=0.015, accuracy=57.1, wps=9478.9, ups=3.89, wpb=2434.1, bsz=27.2, num_updates=2700, lr=6.75e-06, gnorm=10.617, loss_scale=4096, train_wall=25, gb_free=11, wall=749]2023-06-11 14:58:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  61% 1106/1823 [04:56<03:07,  3.82it/s, loss=1.308, nll_loss=0.015, accuracy=58, wps=8593.1, ups=3.62, wpb=2375.9, bsz=26.4, num_updates=2900, lr=7.25e-06, gnorm=11.032, loss_scale=4096, train_wall=27, gb_free=9.8, wall=803]2023-06-11 14:58:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  64% 1166/1823 [05:11<02:34,  4.25it/s, loss=1.308, nll_loss=0.015, accuracy=58, wps=8593.1, ups=3.62, wpb=2375.9, bsz=26.4, num_updates=2900, lr=7.25e-06, gnorm=11.032, loss_scale=4096, train_wall=27, gb_free=9.8, wall=803]2023-06-11 14:59:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 002:  79% 1449/1823 [06:26<01:43,  3.61it/s, loss=1.246, nll_loss=0.014, accuracy=61.3, wps=9442.5, ups=3.83, wpb=2466.4, bsz=27.6, num_updates=3200, lr=8e-06, gnorm=11.037, loss_scale=4096, train_wall=25, gb_free=11.1, wall=882]2023-06-11 15:00:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  87% 1581/1823 [07:01<01:00,  4.00it/s, loss=1.28, nll_loss=0.014, accuracy=58.8, wps=9024.1, ups=3.68, wpb=2449.8, bsz=27.4, num_updates=3300, lr=8.25e-06, gnorm=10.731, loss_scale=4096, train_wall=27, gb_free=10.5, wall=909]2023-06-11 15:01:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  95% 1730/1823 [07:40<00:25,  3.64it/s, loss=1.262, nll_loss=0.014, accuracy=62, wps=9413.9, ups=3.84, wpb=2449, bsz=27.7, num_updates=3500, lr=8.75e-06, gnorm=11.492, loss_scale=4096, train_wall=25, gb_free=10.5, wall=962]2023-06-11 15:01:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002: 100% 1822/1823 [08:03<00:00,  3.90it/s, loss=1.203, nll_loss=0.014, accuracy=63, wps=9440.2, ups=3.8, wpb=2484.2, bsz=27.9, num_updates=3600, lr=9e-06, gnorm=9.907, loss_scale=4096, train_wall=26, gb_free=10.9, wall=988]2023-06-11 15:02:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 15:02:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   2% 1/40 [00:00<00:05,  7.12it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   5% 2/40 [00:00<00:05,  6.45it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  10% 4/40 [00:00<00:04,  8.81it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  15% 6/40 [00:00<00:03,  9.80it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  20% 8/40 [00:00<00:03, 10.51it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  25% 10/40 [00:00<00:02, 11.25it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  30% 12/40 [00:01<00:02, 11.59it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  35% 14/40 [00:01<00:02, 11.38it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  40% 16/40 [00:01<00:02, 11.62it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  45% 18/40 [00:01<00:01, 11.55it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  50% 20/40 [00:01<00:01, 12.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  55% 22/40 [00:01<00:01, 11.91it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  60% 24/40 [00:02<00:01, 11.80it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  65% 26/40 [00:02<00:01, 12.24it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  70% 28/40 [00:02<00:00, 12.80it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  75% 30/40 [00:02<00:00, 13.11it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  80% 32/40 [00:02<00:00, 12.96it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  85% 34/40 [00:02<00:00, 12.60it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  90% 36/40 [00:03<00:00, 12.60it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  95% 38/40 [00:03<00:00, 13.26it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset: 100% 40/40 [00:03<00:00, 13.43it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 15:02:10 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 2.191 | nll_loss 0.022 | accuracy 31.5 | wps 29761 | wpb 2452.7 | bsz 25 | num_updates 3631 | best_accuracy 32.9\n","2023-06-11 15:02:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3631 updates\n","2023-06-11 15:02:10 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/uda/checkpoint_last.pt\n","2023-06-11 15:02:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/uda/checkpoint_last.pt\n","2023-06-11 15:02:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/uda/checkpoint_last.pt (epoch 2 @ 3631 updates, score 31.5) (writing took 9.487591872999928 seconds)\n","2023-06-11 15:02:19 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2023-06-11 15:02:19 | INFO | train | epoch 002 | loss 1.334 | nll_loss 0.015 | accuracy 56.6 | wps 8907.6 | ups 3.65 | wpb 2443.4 | bsz 27.4 | num_updates 3631 | lr 9.0775e-06 | gnorm 10.031 | loss_scale 4096 | train_wall 473 | gb_free 10.6 | wall 1008\n","2023-06-11 15:02:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 15:02:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1823\n","epoch 003:   0% 0/1823 [00:00<?, ?it/s]2023-06-11 15:02:19 | INFO | fairseq.trainer | begin training epoch 3\n","2023-06-11 15:02:19 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 003:   2% 41/1823 [00:11<07:37,  3.90it/s]2023-06-11 15:02:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  10% 181/1823 [00:54<07:01,  3.90it/s, loss=1.128, nll_loss=0.013, accuracy=65.8, wps=8037.5, ups=3.24, wpb=2483.9, bsz=27.9, num_updates=3800, lr=9.5e-06, gnorm=11.328, loss_scale=8192, train_wall=30, gb_free=10.8, wall=1060]2023-06-11 15:03:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  10% 186/1823 [00:55<06:23,  4.27it/s, loss=1.128, nll_loss=0.013, accuracy=65.8, wps=8037.5, ups=3.24, wpb=2483.9, bsz=27.9, num_updates=3800, lr=9.5e-06, gnorm=11.328, loss_scale=8192, train_wall=30, gb_free=10.8, wall=1060]2023-06-11 15:03:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  20% 373/1823 [01:44<06:43,  3.59it/s, loss=1.142, nll_loss=0.013, accuracy=65.5, wps=9247, ups=3.79, wpb=2436.8, bsz=27.3, num_updates=4000, lr=1e-05, gnorm=10.399, loss_scale=4096, train_wall=26, gb_free=10.4, wall=1112]2023-06-11 15:04:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  35% 631/1823 [02:51<04:50,  4.11it/s, loss=1.128, nll_loss=0.013, accuracy=66.8, wps=9534.3, ups=3.84, wpb=2485.9, bsz=28.1, num_updates=4200, lr=9.759e-06, gnorm=10.177, loss_scale=4096, train_wall=25, gb_free=10.4, wall=1165]2023-06-11 15:05:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  42% 764/1823 [03:26<04:56,  3.57it/s, loss=1.096, nll_loss=0.012, accuracy=66.9, wps=9217.2, ups=3.73, wpb=2471.2, bsz=27.9, num_updates=4300, lr=9.64486e-06, gnorm=10.007, loss_scale=4096, train_wall=26, gb_free=10.6, wall=1192]2023-06-11 15:05:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  50% 905/1823 [04:04<04:22,  3.49it/s, loss=1.132, nll_loss=0.013, accuracy=64.6, wps=9341.8, ups=3.81, wpb=2450.5, bsz=27.6, num_updates=4500, lr=9.42809e-06, gnorm=9.889, loss_scale=4096, train_wall=26, gb_free=10.7, wall=1245]2023-06-11 15:06:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  57% 1034/1823 [04:40<04:40,  2.82it/s, loss=1.123, nll_loss=0.013, accuracy=66.8, wps=9271.3, ups=3.73, wpb=2483.1, bsz=27.8, num_updates=4600, lr=9.32505e-06, gnorm=9.215, loss_scale=4096, train_wall=26, gb_free=10.9, wall=1271]2023-06-11 15:07:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  64% 1168/1823 [05:15<03:23,  3.22it/s, loss=1.145, nll_loss=0.013, accuracy=64.5, wps=8568.9, ups=3.51, wpb=2438.6, bsz=27.2, num_updates=4700, lr=9.22531e-06, gnorm=9.751, loss_scale=4096, train_wall=28, gb_free=10.8, wall=1300]2023-06-11 15:07:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  71% 1297/1823 [05:49<02:15,  3.89it/s, loss=1.104, nll_loss=0.012, accuracy=67.1, wps=9312.3, ups=3.74, wpb=2490.8, bsz=27.9, num_updates=4900, lr=9.03508e-06, gnorm=8.988, loss_scale=4096, train_wall=26, gb_free=10.3, wall=1353]2023-06-11 15:08:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  78% 1429/1823 [06:24<01:57,  3.35it/s, loss=1.093, nll_loss=0.012, accuracy=67, wps=9164.9, ups=3.75, wpb=2443.9, bsz=27.4, num_updates=5000, lr=8.94427e-06, gnorm=9.315, loss_scale=4096, train_wall=26, gb_free=10.4, wall=1380]2023-06-11 15:08:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  86% 1561/1823 [06:59<01:06,  3.95it/s, loss=1.069, nll_loss=0.012, accuracy=67.9, wps=9061.4, ups=3.8, wpb=2386.6, bsz=26.7, num_updates=5100, lr=8.85615e-06, gnorm=9.811, loss_scale=4096, train_wall=26, gb_free=10.7, wall=1406]2023-06-11 15:09:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  95% 1739/1823 [07:46<00:20,  4.19it/s, loss=1.124, nll_loss=0.013, accuracy=66.9, wps=8840.6, ups=3.69, wpb=2392.6, bsz=26.8, num_updates=5300, lr=8.68744e-06, gnorm=9.47, loss_scale=4096, train_wall=26, gb_free=10.5, wall=1460]2023-06-11 15:10:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003: 100% 1822/1823 [08:07<00:00,  3.89it/s, loss=1.038, nll_loss=0.012, accuracy=69.6, wps=9296, ups=3.8, wpb=2446.1, bsz=27.4, num_updates=5400, lr=8.60663e-06, gnorm=8.593, loss_scale=4096, train_wall=26, gb_free=10.4, wall=1486]2023-06-11 15:10:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 15:10:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   2% 1/40 [00:00<00:05,  7.19it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   5% 2/40 [00:00<00:05,  7.36it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  10% 4/40 [00:00<00:03,  9.94it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  15% 6/40 [00:00<00:03, 10.73it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  20% 8/40 [00:00<00:02, 11.17it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  25% 10/40 [00:00<00:02, 11.90it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  30% 12/40 [00:01<00:02, 11.95it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  35% 14/40 [00:01<00:02, 11.76it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  40% 16/40 [00:01<00:02, 11.72it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  45% 18/40 [00:01<00:01, 11.56it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  50% 20/40 [00:01<00:01, 11.98it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  55% 22/40 [00:01<00:01, 12.06it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  60% 24/40 [00:02<00:01, 11.86it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  65% 26/40 [00:02<00:01, 12.65it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  70% 28/40 [00:02<00:00, 13.37it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  75% 30/40 [00:02<00:00, 13.39it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  80% 32/40 [00:02<00:00, 13.20it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  85% 34/40 [00:02<00:00, 12.68it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  90% 36/40 [00:03<00:00, 12.62it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  95% 38/40 [00:03<00:00, 13.39it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset: 100% 40/40 [00:03<00:00, 13.48it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 15:10:30 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 2.238 | nll_loss 0.023 | accuracy 30.8 | wps 30583.3 | wpb 2452.7 | bsz 25 | num_updates 5441 | best_accuracy 32.9\n","2023-06-11 15:10:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 5441 updates\n","2023-06-11 15:10:30 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/uda/checkpoint_last.pt\n","2023-06-11 15:10:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/uda/checkpoint_last.pt\n","2023-06-11 15:10:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/uda/checkpoint_last.pt (epoch 3 @ 5441 updates, score 30.8) (writing took 8.1813750299998 seconds)\n","2023-06-11 15:10:39 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2023-06-11 15:10:39 | INFO | train | epoch 003 | loss 1.116 | nll_loss 0.013 | accuracy 66.4 | wps 8850.9 | ups 3.62 | wpb 2442.9 | bsz 27.4 | num_updates 5441 | lr 8.57414e-06 | gnorm 9.972 | loss_scale 4096 | train_wall 476 | gb_free 10.7 | wall 1508\n","2023-06-11 15:10:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 15:10:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1823\n","epoch 004:   0% 0/1823 [00:00<?, ?it/s]2023-06-11 15:10:39 | INFO | fairseq.trainer | begin training epoch 4\n","2023-06-11 15:10:39 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 004:   3% 49/1823 [00:13<09:15,  3.19it/s]2023-06-11 15:10:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  10% 187/1823 [00:54<06:59,  3.90it/s, loss=1.001, nll_loss=0.011, accuracy=70.2, wps=8203.5, ups=3.25, wpb=2526.1, bsz=28.8, num_updates=5600, lr=8.45154e-06, gnorm=9.556, loss_scale=4096, train_wall=30, gb_free=10.4, wall=1555]2023-06-11 15:11:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  18% 325/1823 [01:30<07:41,  3.24it/s, loss=1.012, nll_loss=0.011, accuracy=69.8, wps=9028, ups=3.78, wpb=2390.8, bsz=26.9, num_updates=5700, lr=8.37708e-06, gnorm=8.93, loss_scale=4096, train_wall=26, gb_free=10.5, wall=1582]2023-06-11 15:12:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  25% 454/1823 [02:04<05:08,  4.43it/s, loss=1.018, nll_loss=0.011, accuracy=69.8, wps=9097.8, ups=3.84, wpb=2367.1, bsz=26.6, num_updates=5800, lr=8.30455e-06, gnorm=10.055, loss_scale=4096, train_wall=26, gb_free=10.7, wall=1608]2023-06-11 15:12:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  32% 592/1823 [02:40<05:05,  4.02it/s, loss=1.028, nll_loss=0.012, accuracy=68.9, wps=9276.6, ups=3.77, wpb=2459.6, bsz=27.7, num_updates=6000, lr=8.16497e-06, gnorm=9.437, loss_scale=4096, train_wall=26, gb_free=10.6, wall=1661]2023-06-11 15:13:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  40% 726/1823 [03:15<04:52,  3.75it/s, loss=0.986, nll_loss=0.011, accuracy=70.3, wps=9285.2, ups=3.79, wpb=2449.1, bsz=27.4, num_updates=6100, lr=8.09776e-06, gnorm=9.046, loss_scale=4096, train_wall=26, gb_free=10.4, wall=1687]2023-06-11 15:13:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  47% 857/1823 [03:50<04:12,  3.82it/s, loss=0.975, nll_loss=0.011, accuracy=71.8, wps=9036.4, ups=3.79, wpb=2385.8, bsz=26.7, num_updates=6200, lr=8.03219e-06, gnorm=9.488, loss_scale=4096, train_wall=26, gb_free=10.7, wall=1714]2023-06-11 15:14:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  55% 1004/1823 [04:29<03:49,  3.57it/s, loss=0.979, nll_loss=0.011, accuracy=70.7, wps=9068.9, ups=3.78, wpb=2399.6, bsz=27, num_updates=6400, lr=7.90569e-06, gnorm=9.429, loss_scale=4096, train_wall=26, gb_free=10.7, wall=1767]2023-06-11 15:15:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  63% 1140/1823 [05:04<03:04,  3.70it/s, loss=0.999, nll_loss=0.011, accuracy=69.5, wps=9338.7, ups=3.75, wpb=2492.8, bsz=28, num_updates=6500, lr=7.84465e-06, gnorm=9.465, loss_scale=4096, train_wall=26, gb_free=10.9, wall=1794]2023-06-11 15:15:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  70% 1278/1823 [05:40<02:20,  3.87it/s, loss=1.003, nll_loss=0.011, accuracy=69.9, wps=9433.1, ups=3.83, wpb=2460.5, bsz=27.8, num_updates=6700, lr=7.72667e-06, gnorm=9.347, loss_scale=4096, train_wall=25, gb_free=10.1, wall=1846]2023-06-11 15:16:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  78% 1424/1823 [06:19<01:54,  3.49it/s, loss=0.946, nll_loss=0.011, accuracy=72.2, wps=9306, ups=3.74, wpb=2487.4, bsz=28, num_updates=6800, lr=7.66965e-06, gnorm=8.517, loss_scale=4096, train_wall=26, gb_free=10.5, wall=1873]2023-06-11 15:16:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  88% 1598/1823 [07:05<01:00,  3.71it/s, loss=0.939, nll_loss=0.01, accuracy=72.4, wps=8889.3, ups=3.74, wpb=2378.5, bsz=26.5, num_updates=7000, lr=7.55929e-06, gnorm=8.923, loss_scale=8192, train_wall=26, gb_free=10.3, wall=1926]2023-06-11 15:17:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  95% 1731/1823 [07:40<00:25,  3.65it/s, loss=0.942, nll_loss=0.011, accuracy=72.2, wps=9246.1, ups=3.81, wpb=2428.7, bsz=27.3, num_updates=7100, lr=7.50587e-06, gnorm=8.668, loss_scale=4096, train_wall=26, gb_free=11.6, wall=1952]2023-06-11 15:18:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004: 100% 1822/1823 [08:04<00:00,  3.57it/s, loss=0.956, nll_loss=0.011, accuracy=72.4, wps=9112, ups=3.8, wpb=2399.2, bsz=26.9, num_updates=7200, lr=7.45356e-06, gnorm=8.882, loss_scale=4096, train_wall=26, gb_free=10.8, wall=1979]2023-06-11 15:18:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 15:18:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   2% 1/40 [00:00<00:04,  8.13it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   5% 2/40 [00:00<00:04,  8.98it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   8% 3/40 [00:00<00:04,  9.12it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  10% 4/40 [00:00<00:03,  9.36it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  15% 6/40 [00:00<00:03, 10.27it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  20% 8/40 [00:00<00:02, 11.03it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  25% 10/40 [00:00<00:02, 11.65it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  30% 12/40 [00:01<00:02, 12.05it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  35% 14/40 [00:01<00:02, 11.90it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  40% 16/40 [00:01<00:02, 11.79it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  45% 18/40 [00:01<00:01, 11.74it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  50% 20/40 [00:01<00:01, 12.17it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  55% 22/40 [00:01<00:01, 12.05it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  60% 24/40 [00:02<00:01, 11.69it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  65% 26/40 [00:02<00:01, 12.24it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  70% 28/40 [00:02<00:00, 12.78it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  75% 30/40 [00:02<00:00, 13.03it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  80% 32/40 [00:02<00:00, 13.05it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  85% 34/40 [00:02<00:00, 12.68it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  90% 36/40 [00:03<00:00, 12.76it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  95% 38/40 [00:03<00:00, 13.56it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset: 100% 40/40 [00:03<00:00, 13.72it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 15:18:46 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 2.492 | nll_loss 0.025 | accuracy 32.4 | wps 30332.5 | wpb 2452.7 | bsz 25 | num_updates 7251 | best_accuracy 32.9\n","2023-06-11 15:18:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 7251 updates\n","2023-06-11 15:18:46 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/uda/checkpoint_last.pt\n","2023-06-11 15:18:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/uda/checkpoint_last.pt\n","2023-06-11 15:18:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/uda/checkpoint_last.pt (epoch 4 @ 7251 updates, score 32.4) (writing took 11.605266534000293 seconds)\n","2023-06-11 15:18:58 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2023-06-11 15:18:58 | INFO | train | epoch 004 | loss 0.982 | nll_loss 0.011 | accuracy 70.8 | wps 8857.2 | ups 3.63 | wpb 2442.8 | bsz 27.4 | num_updates 7251 | lr 7.4273e-06 | gnorm 9.163 | loss_scale 4096 | train_wall 473 | gb_free 11.3 | wall 2007\n","2023-06-11 15:18:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 15:18:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1823\n","epoch 005:   0% 0/1823 [00:00<?, ?it/s]2023-06-11 15:18:58 | INFO | fairseq.trainer | begin training epoch 5\n","2023-06-11 15:18:58 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 005:   2% 42/1823 [00:11<08:30,  3.49it/s]2023-06-11 15:19:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  10% 174/1823 [00:52<07:49,  3.51it/s, loss=0.879, nll_loss=0.01, accuracy=74.5, wps=7973.4, ups=3.22, wpb=2475.6, bsz=27.8, num_updates=7400, lr=7.35215e-06, gnorm=9.717, loss_scale=4096, train_wall=30, gb_free=10.9, wall=2052]2023-06-11 15:19:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  19% 338/1823 [01:34<06:47,  3.64it/s, loss=0.876, nll_loss=0.01, accuracy=74.7, wps=9136.2, ups=3.72, wpb=2458.3, bsz=27.7, num_updates=7500, lr=7.30297e-06, gnorm=9.336, loss_scale=4096, train_wall=26, gb_free=11.5, wall=2079]2023-06-11 15:20:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  26% 473/1823 [02:10<06:19,  3.56it/s, loss=0.852, nll_loss=0.01, accuracy=75.3, wps=9482.5, ups=3.87, wpb=2452.7, bsz=27.5, num_updates=7700, lr=7.2075e-06, gnorm=9.255, loss_scale=4096, train_wall=25, gb_free=10.8, wall=2131]2023-06-11 15:21:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  34% 613/1823 [02:46<04:46,  4.22it/s, loss=0.89, nll_loss=0.01, accuracy=74, wps=8869.4, ups=3.71, wpb=2391.1, bsz=26.9, num_updates=7800, lr=7.16115e-06, gnorm=9.774, loss_scale=4096, train_wall=26, gb_free=9.6, wall=2158]2023-06-11 15:21:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  41% 752/1823 [03:23<04:35,  3.89it/s, loss=0.858, nll_loss=0.01, accuracy=75.7, wps=9261.1, ups=3.79, wpb=2441.3, bsz=27.4, num_updates=7900, lr=7.11568e-06, gnorm=9.679, loss_scale=4096, train_wall=26, gb_free=10.6, wall=2185]2023-06-11 15:22:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  48% 884/1823 [03:57<03:43,  4.20it/s, loss=0.855, nll_loss=0.01, accuracy=75.6, wps=9065.7, ups=3.76, wpb=2410.2, bsz=26.9, num_updates=8100, lr=7.02728e-06, gnorm=9.816, loss_scale=4096, train_wall=26, gb_free=10.8, wall=2238]2023-06-11 15:22:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  56% 1013/1823 [04:32<03:27,  3.90it/s, loss=0.858, nll_loss=0.01, accuracy=74.4, wps=9121.5, ups=3.74, wpb=2440.3, bsz=27.3, num_updates=8200, lr=6.9843e-06, gnorm=9.264, loss_scale=4096, train_wall=26, gb_free=10.6, wall=2264]2023-06-11 15:23:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  64% 1167/1823 [05:12<02:38,  4.13it/s, loss=0.844, nll_loss=0.009, accuracy=75.2, wps=9699.2, ups=3.85, wpb=2516.1, bsz=28.3, num_updates=8400, lr=6.90066e-06, gnorm=8.964, loss_scale=8192, train_wall=25, gb_free=10.7, wall=2317]2023-06-11 15:24:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  72% 1309/1823 [05:50<02:12,  3.87it/s, loss=0.85, nll_loss=0.009, accuracy=76, wps=9033, ups=3.73, wpb=2421.8, bsz=26.8, num_updates=8500, lr=6.85994e-06, gnorm=9.156, loss_scale=4096, train_wall=26, gb_free=10.8, wall=2344]2023-06-11 15:24:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  83% 1510/1823 [06:42<01:25,  3.68it/s, loss=0.88, nll_loss=0.01, accuracy=74.9, wps=9517.3, ups=3.82, wpb=2492.4, bsz=28, num_updates=8700, lr=6.78064e-06, gnorm=9.785, loss_scale=8192, train_wall=26, gb_free=10.8, wall=2396]2023-06-11 15:25:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  90% 1647/1823 [07:18<00:43,  4.02it/s, loss=0.811, nll_loss=0.009, accuracy=76.6, wps=9033.4, ups=3.75, wpb=2410.9, bsz=27.1, num_updates=8800, lr=6.742e-06, gnorm=9.972, loss_scale=4096, train_wall=26, gb_free=10.8, wall=2423]2023-06-11 15:26:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  98% 1788/1823 [07:55<00:09,  3.59it/s, loss=0.837, nll_loss=0.009, accuracy=76, wps=9467.4, ups=3.82, wpb=2475.7, bsz=27.9, num_updates=9000, lr=6.66667e-06, gnorm=9.488, loss_scale=4096, train_wall=26, gb_free=10.9, wall=2476]2023-06-11 15:26:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005: 100% 1822/1823 [08:04<00:00,  4.08it/s, loss=0.837, nll_loss=0.009, accuracy=76, wps=9467.4, ups=3.82, wpb=2475.7, bsz=27.9, num_updates=9000, lr=6.66667e-06, gnorm=9.488, loss_scale=4096, train_wall=26, gb_free=10.9, wall=2476]2023-06-11 15:27:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 15:27:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 005 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   2% 1/40 [00:00<00:04,  9.33it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   8% 3/40 [00:00<00:03, 11.84it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  12% 5/40 [00:00<00:02, 11.91it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  18% 7/40 [00:00<00:02, 12.46it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  22% 9/40 [00:00<00:02, 12.72it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  28% 11/40 [00:00<00:02, 13.37it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  32% 13/40 [00:01<00:02, 13.15it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  38% 15/40 [00:01<00:01, 13.25it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  42% 17/40 [00:01<00:01, 13.19it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  48% 19/40 [00:01<00:01, 13.93it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  52% 21/40 [00:01<00:01, 13.25it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  57% 23/40 [00:01<00:01, 12.72it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  62% 25/40 [00:01<00:01, 13.01it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  68% 27/40 [00:02<00:00, 13.26it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  72% 29/40 [00:02<00:00, 13.82it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  78% 31/40 [00:02<00:00, 13.13it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  82% 33/40 [00:02<00:00, 12.99it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  88% 35/40 [00:02<00:00, 12.68it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  92% 37/40 [00:02<00:00, 13.12it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  98% 39/40 [00:02<00:00, 13.85it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 15:27:05 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 2.842 | nll_loss 0.029 | accuracy 34.6 | wps 32639.1 | wpb 2452.7 | bsz 25 | num_updates 9061 | best_accuracy 34.6\n","2023-06-11 15:27:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 9061 updates\n","2023-06-11 15:27:05 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/uda/checkpoint_best.pt\n","2023-06-11 15:27:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/uda/checkpoint_best.pt\n","2023-06-11 15:28:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/uda/checkpoint_best.pt (epoch 5 @ 9061 updates, score 34.6) (writing took 55.43135974899997 seconds)\n","2023-06-11 15:28:01 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n","2023-06-11 15:28:01 | INFO | train | epoch 005 | loss 0.855 | nll_loss 0.01 | accuracy 75.2 | wps 8141.1 | ups 3.33 | wpb 2442.4 | bsz 27.4 | num_updates 9061 | lr 6.64419e-06 | gnorm 9.51 | loss_scale 4096 | train_wall 473 | gb_free 11 | wall 2550\n","2023-06-11 15:28:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 15:28:01 | INFO | fairseq_cli.train | done training in 2547.3 seconds\n"]}]},{"cell_type":"code","source":["from fairseq.models.roberta import RobertaModel\n","roberta = RobertaModel.from_pretrained(\n","    '/content/drive/MyDrive/NLP/checkpoints/R1/uda',\n","    checkpoint_file='checkpoint_best.pt',\n","    data_name_or_path='/content/drive/MyDrive/NLP/anli/R1/orig/bin_uda'\n",")\n","roberta.eval()"],"metadata":{"id":"ZcGviNqzVGAU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["genre = ['R1','R2','R3']\n","for g in genre:\n","  input0 = open('/content/drive/MyDrive/NLP/anli/'+g+'/orig/test.raw.input0', \"r\")\n","  input1 = open('/content/drive/MyDrive/NLP/anli/'+g+'/orig/test.raw.input1', \"r\")\n","  label = open('/content/drive/MyDrive/NLP/anli/'+g+'/orig/test.raw.label', \"r\")\n","\n","  accuracy = 0\n","  total = 0\n","  for (x1, x2, y) in zip(input0, input1, label):\n","    tokens = roberta.encode(x1, x2)\n","    idx = roberta.predict('anli', tokens).argmax().item()\n","    dictionary = roberta.task.label_dictionary\n","    pred = dictionary[idx + dictionary.nspecial]\n","    total = total + 1\n","    if  (pred == y.strip()) :\n","      accuracy = accuracy + 1\n","\n","\n","  print(g,\": \",accuracy)\n","  print(g,\": \",total)"],"metadata":{"id":"LOsGAXwOVKdx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686502508879,"user_tz":-120,"elapsed":1535122,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}},"outputId":"217f4c00-3731-4d8b-9d09-4e761bbea022"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["R1 :  287\n","R1 :  1000\n","R2 :  365\n","R2 :  1000\n","R3 :  404\n","R3 :  1200\n"]}]},{"cell_type":"markdown","source":["# SSMBA\n"],"metadata":{"id":"Ueci6F-Hy8jc"}},{"cell_type":"code","source":["import tensorflow as tf\n","with tf.device('/device:GPU:0'):\n","  ! MAX_EPOCH=10 \\\n","      LR=1e-05 \\\n","      BATCH_SIZE=32 \\\n","      CUDA_VISIBLE_DEVICES=0 fairseq-train /content/drive/MyDrive/NLP/anli/R1/orig/bin_ssmba \\\n","          --save-dir /content/drive/MyDrive/NLP/checkpoints/R1/ssmba \\\n","          --reset-optimizer  \\\n","          --reset-dataloader  \\\n","          --reset-meters  \\\n","          --best-checkpoint-metric accuracy  \\\n","          --maximize-best-checkpoint-metric  \\\n","          --no-epoch-checkpoints \\\n","          --find-unused-parameters \\\n","          --distributed-world-size 1 \\\n","          --task sentence_prediction  \\\n","          --num-classes 3  \\\n","          --init-token 0  \\\n","          --separator-token 2   \\\n","          --max-positions 512  \\\n","          --shorten-method \"truncate\"  \\\n","          --arch roberta \\\n","          --dropout 0.1  \\\n","          --attention-dropout 0.1  \\\n","          --weight-decay 0.1  \\\n","          --criterion sentence_prediction  \\\n","          --classification-head-name 'anli' \\\n","          --optimizer adam  \\\n","          --adam-betas '(0.9, 0.98)'  \\\n","          --adam-eps 1e-06  \\\n","          --clip-norm 0.0  \\\n","          --lr-scheduler inverse_sqrt  \\\n","          --lr 1e-05 \\\n","          --fp16  \\\n","          --fp16-init-scale 4  \\\n","          --threshold-loss-scale 1  \\\n","          --fp16-scale-window 128  \\\n","          --batch-size 32  \\\n","          --required-batch-size-multiple 1  \\\n","          --max-tokens 4400 \\\n","          --update-freq 1  \\\n","          --max-update 123873 \\\n","          --max-epoch 5 \\\n","          --seed 100"],"metadata":{"id":"WbTUM7n4UOt-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686500256096,"user_tz":-120,"elapsed":2919624,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}},"outputId":"10c5f205-e01f-4d60-d28f-8b93e674bc60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-11 15:29:03.384399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2023-06-11 15:29:07 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n","2023-06-11 15:29:11 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 100, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4400, 'batch_size': 32, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 123873, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/drive/MyDrive/NLP/checkpoints/R1/ssmba', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=100, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=1.0, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4400, batch_size=32, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4400, batch_size_valid=32, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta', max_epoch=5, max_update=123873, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/content/drive/MyDrive/NLP/checkpoints/R1/ssmba', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='/content/drive/MyDrive/NLP/anli/R1/orig/bin_ssmba', num_classes=3, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, d2v2_multi=False, classification_head_name='anli', regression_target=False, report_mcc=False, report_acc_and_f1=False, report_pearson_and_spearman=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.1, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, max_positions=512, dropout=0.1, attention_dropout=0.1, no_seed_provided=False, encoder_layers=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_attention_heads=12, activation_dropout=0.0, pooler_dropout=0.0, max_source_positions=512, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta'), 'task': {'_name': 'sentence_prediction', 'data': '/content/drive/MyDrive/NLP/anli/R1/orig/bin_ssmba', 'num_classes': 3, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 512, 'regression_target': False, 'classification_head_name': 'anli', 'seed': 100, 'd2v2_multi': False}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'anli', 'regression_target': False, 'report_mcc': False, 'report_acc_and_f1': False, 'report_pearson_and_spearman': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2023-06-11 15:29:11 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n","2023-06-11 15:29:12 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n","2023-06-11 15:29:15 | INFO | fairseq_cli.train | RobertaModel(\n","  (encoder): RobertaEncoder(\n","    (sentence_encoder): TransformerEncoder(\n","      (dropout_module): FairseqDropout()\n","      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","      (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (layers): ModuleList(\n","        (0-11): 12 x TransformerEncoderLayerBase(\n","          (self_attn): MultiheadAttention(\n","            (dropout_module): FairseqDropout()\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout_module): FairseqDropout()\n","          (activation_dropout_module): FairseqDropout()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (lm_head): RobertaLMHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (classification_heads): ModuleDict(\n","    (anli): RobertaClassificationHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","      (out_proj): Linear(in_features=768, out_features=3, bias=True)\n","    )\n","  )\n",")\n","2023-06-11 15:29:15 | INFO | fairseq_cli.train | task: SentencePredictionTask\n","2023-06-11 15:29:15 | INFO | fairseq_cli.train | model: RobertaModel\n","2023-06-11 15:29:15 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n","2023-06-11 15:29:15 | INFO | fairseq_cli.train | num. shared model params: 125,289,564 (num. trained: 125,289,564)\n","2023-06-11 15:29:15 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n","2023-06-11 15:29:15 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_ssmba/input0/valid\n","2023-06-11 15:29:16 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_ssmba/input1/valid\n","2023-06-11 15:29:16 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_ssmba/label/valid\n","2023-06-11 15:29:16 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 1000\n","2023-06-11 15:29:24 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n","2023-06-11 15:29:24 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-11 15:29:24 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n","2023-06-11 15:29:24 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-11 15:29:24 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2023-06-11 15:29:24 | INFO | fairseq_cli.train | max tokens per device = 4400 and max sentences per device = 32\n","2023-06-11 15:29:24 | INFO | fairseq.trainer | Preparing to load checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/ssmba/checkpoint_last.pt\n","2023-06-11 15:29:24 | INFO | fairseq.trainer | No existing checkpoint found /content/drive/MyDrive/NLP/checkpoints/R1/ssmba/checkpoint_last.pt\n","2023-06-11 15:29:24 | INFO | fairseq.trainer | loading train data for epoch 1\n","2023-06-11 15:29:25 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_ssmba/input0/train\n","2023-06-11 15:29:25 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_ssmba/input1/train\n","2023-06-11 15:29:26 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_ssmba/label/train\n","2023-06-11 15:29:26 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 50000\n","2023-06-11 15:29:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 15:29:26 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-11 15:29:26 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-11 15:29:26 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-11 15:29:26 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n","2023-06-11 15:29:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 15:29:26 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-11 15:29:26 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-11 15:29:26 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-11 15:29:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1953\n","epoch 001:   0% 0/1953 [00:00<?, ?it/s]2023-06-11 15:29:26 | INFO | fairseq.trainer | begin training epoch 1\n","2023-06-11 15:29:26 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 001:  76% 1478/1953 [06:45<02:11,  3.61it/s, loss=1.566, nll_loss=0.015, accuracy=40.2, wps=9661.4, ups=3.59, wpb=2694.1, bsz=25.9, num_updates=1400, lr=3.5e-06, gnorm=4.377, loss_scale=4096, train_wall=27, gb_free=10.8, wall=387]2023-06-11 15:36:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 001:  84% 1631/1953 [07:27<01:35,  3.36it/s, loss=1.545, nll_loss=0.015, accuracy=42, wps=9486.4, ups=3.62, wpb=2622.6, bsz=25.3, num_updates=1600, lr=4e-06, gnorm=4.272, loss_scale=4096, train_wall=27, gb_free=10.3, wall=442]2023-06-11 15:36:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 001:  91% 1773/1953 [08:06<00:52,  3.44it/s, loss=1.535, nll_loss=0.015, accuracy=43.5, wps=9148.7, ups=3.6, wpb=2544.4, bsz=24.4, num_updates=1700, lr=4.25e-06, gnorm=4.421, loss_scale=4096, train_wall=27, gb_free=10.5, wall=470]2023-06-11 15:37:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 001: 100% 1952/1953 [08:54<00:00,  4.12it/s, loss=1.538, nll_loss=0.015, accuracy=41.9, wps=9930.7, ups=3.68, wpb=2698.2, bsz=26.1, num_updates=1900, lr=4.75e-06, gnorm=4.827, loss_scale=8192, train_wall=27, gb_free=10.4, wall=524]2023-06-11 15:38:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 15:38:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   2% 1/40 [00:00<00:04,  8.23it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   8% 3/40 [00:00<00:03, 10.23it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  12% 5/40 [00:00<00:03, 10.56it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  18% 7/40 [00:00<00:02, 11.16it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  22% 9/40 [00:00<00:02, 11.47it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  28% 11/40 [00:00<00:02, 12.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  32% 13/40 [00:01<00:02, 12.13it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  38% 15/40 [00:01<00:02, 12.21it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  42% 17/40 [00:01<00:01, 12.06it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  48% 19/40 [00:01<00:01, 12.59it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  52% 21/40 [00:01<00:01, 11.92it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  57% 23/40 [00:01<00:01, 11.52it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  62% 25/40 [00:02<00:01, 11.96it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  68% 27/40 [00:02<00:01, 12.39it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  72% 29/40 [00:02<00:00, 12.97it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  78% 31/40 [00:02<00:00, 12.46it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  82% 33/40 [00:02<00:00, 12.58it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  88% 35/40 [00:02<00:00, 12.35it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  92% 37/40 [00:03<00:00, 12.95it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  98% 39/40 [00:03<00:00, 13.63it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 15:38:24 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.623 | nll_loss 0.017 | accuracy 31.8 | wps 30623.6 | wpb 2452.7 | bsz 25 | num_updates 1950\n","2023-06-11 15:38:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1950 updates\n","2023-06-11 15:38:24 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/ssmba/checkpoint_best.pt\n","2023-06-11 15:38:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/ssmba/checkpoint_best.pt\n","2023-06-11 15:38:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/ssmba/checkpoint_best.pt (epoch 1 @ 1950 updates, score 31.8) (writing took 33.23562380000021 seconds)\n","2023-06-11 15:38:57 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2023-06-11 15:38:57 | INFO | train | epoch 001 | loss 1.558 | nll_loss 0.015 | accuracy 41 | wps 9097.6 | ups 3.43 | wpb 2651.3 | bsz 25.6 | num_updates 1950 | lr 4.875e-06 | gnorm 4.786 | loss_scale 8192 | train_wall 523 | gb_free 10.5 | wall 574\n","2023-06-11 15:38:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 15:38:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1953\n","epoch 002:   0% 0/1953 [00:00<?, ?it/s]2023-06-11 15:38:58 | INFO | fairseq.trainer | begin training epoch 2\n","2023-06-11 15:38:58 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 002:   4% 74/1953 [00:21<08:20,  3.75it/s, loss=1.511, nll_loss=0.015, accuracy=45.6, wps=4139, ups=1.54, wpb=2696.4, bsz=26, num_updates=2000, lr=5e-06, gnorm=4.893, loss_scale=8192, train_wall=28, gb_free=10.3, wall=589]2023-06-11 15:39:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  13% 262/1953 [01:12<07:31,  3.75it/s, loss=1.47, nll_loss=0.014, accuracy=48.1, wps=9660.6, ups=3.66, wpb=2638, bsz=25.4, num_updates=2200, lr=5.5e-06, gnorm=7.317, loss_scale=8192, train_wall=27, gb_free=11.7, wall=644]2023-06-11 15:40:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  23% 440/1953 [02:02<06:53,  3.66it/s, loss=1.434, nll_loss=0.014, accuracy=49.8, wps=9147.2, ups=3.58, wpb=2551.8, bsz=24.4, num_updates=2300, lr=5.75e-06, gnorm=8.047, loss_scale=4096, train_wall=27, gb_free=10.4, wall=672]2023-06-11 15:41:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  30% 592/1953 [02:43<05:59,  3.78it/s, loss=1.409, nll_loss=0.014, accuracy=52.5, wps=9489.1, ups=3.62, wpb=2617.8, bsz=25.2, num_updates=2500, lr=6.25e-06, gnorm=7.572, loss_scale=4096, train_wall=27, gb_free=10.5, wall=727]2023-06-11 15:41:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  38% 747/1953 [03:25<04:27,  4.51it/s, loss=1.4, nll_loss=0.014, accuracy=54.1, wps=9594.9, ups=3.68, wpb=2604.2, bsz=25.3, num_updates=2600, lr=6.5e-06, gnorm=9.483, loss_scale=4096, train_wall=27, gb_free=10.5, wall=754]2023-06-11 15:42:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  45% 883/1953 [04:02<05:01,  3.54it/s, loss=1.378, nll_loss=0.013, accuracy=54.6, wps=9656.4, ups=3.62, wpb=2667.4, bsz=25.7, num_updates=2800, lr=7e-06, gnorm=9.928, loss_scale=4096, train_wall=27, gb_free=10.4, wall=809]2023-06-11 15:43:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  54% 1045/1953 [04:46<04:14,  3.57it/s, loss=1.34, nll_loss=0.013, accuracy=56.6, wps=10116.6, ups=3.68, wpb=2747.2, bsz=26.8, num_updates=2900, lr=7.25e-06, gnorm=9.448, loss_scale=4096, train_wall=27, gb_free=10.5, wall=836]2023-06-11 15:43:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  60% 1174/1953 [05:22<03:48,  3.41it/s, loss=1.317, nll_loss=0.013, accuracy=58, wps=10054, ups=3.71, wpb=2707, bsz=26.3, num_updates=3100, lr=7.75e-06, gnorm=9.686, loss_scale=4096, train_wall=26, gb_free=10.6, wall=891]2023-06-11 15:44:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  67% 1304/1953 [05:56<02:41,  4.01it/s, loss=1.289, nll_loss=0.012, accuracy=59, wps=9503, ups=3.6, wpb=2636.6, bsz=25.5, num_updates=3200, lr=8e-06, gnorm=10.652, loss_scale=4096, train_wall=27, gb_free=10.6, wall=919]2023-06-11 15:44:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  73% 1433/1953 [06:31<02:26,  3.54it/s, loss=1.28, nll_loss=0.012, accuracy=59.5, wps=10118, ups=3.72, wpb=2718.7, bsz=26.5, num_updates=3300, lr=8.25e-06, gnorm=11.018, loss_scale=4096, train_wall=26, gb_free=10.6, wall=946]2023-06-11 15:45:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  80% 1571/1953 [07:09<01:33,  4.09it/s, loss=1.292, nll_loss=0.012, accuracy=59.5, wps=9846, ups=3.64, wpb=2706.2, bsz=26.1, num_updates=3500, lr=8.75e-06, gnorm=9.986, loss_scale=4096, train_wall=27, gb_free=10.6, wall=1000]2023-06-11 15:46:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  87% 1700/1953 [07:44<01:03,  3.99it/s, loss=1.266, nll_loss=0.012, accuracy=60, wps=9226.5, ups=3.55, wpb=2600.8, bsz=24.9, num_updates=3600, lr=9e-06, gnorm=10.231, loss_scale=4096, train_wall=28, gb_free=10.3, wall=1029]2023-06-11 15:46:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  94% 1838/1953 [08:22<00:31,  3.65it/s, loss=1.265, nll_loss=0.012, accuracy=60.2, wps=9977.8, ups=3.68, wpb=2710.4, bsz=26.3, num_updates=3700, lr=9.25e-06, gnorm=10.73, loss_scale=4096, train_wall=27, gb_free=11.4, wall=1056]2023-06-11 15:47:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002: 100% 1952/1953 [08:53<00:00,  3.34it/s, loss=1.243, nll_loss=0.012, accuracy=61.4, wps=10089.9, ups=3.69, wpb=2737.9, bsz=26.6, num_updates=3800, lr=9.5e-06, gnorm=9.997, loss_scale=4096, train_wall=26, gb_free=11, wall=1083]2023-06-11 15:47:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 15:47:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   2% 1/40 [00:00<00:05,  7.23it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   5% 2/40 [00:00<00:05,  7.37it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  10% 4/40 [00:00<00:03,  9.63it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  15% 6/40 [00:00<00:03, 10.37it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  20% 8/40 [00:00<00:02, 11.10it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  25% 10/40 [00:00<00:02, 11.64it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  30% 12/40 [00:01<00:02, 12.17it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  35% 14/40 [00:01<00:02, 12.11it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  40% 16/40 [00:01<00:02, 11.88it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  45% 18/40 [00:01<00:01, 11.84it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  50% 20/40 [00:01<00:01, 12.17it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  55% 22/40 [00:01<00:01, 12.25it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  60% 24/40 [00:02<00:01, 11.85it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  65% 26/40 [00:02<00:01, 12.17it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  70% 28/40 [00:02<00:00, 12.69it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  75% 30/40 [00:02<00:00, 12.99it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  80% 32/40 [00:02<00:00, 12.97it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  85% 34/40 [00:02<00:00, 12.68it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  90% 36/40 [00:03<00:00, 12.87it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  95% 38/40 [00:03<00:00, 13.55it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset: 100% 40/40 [00:03<00:00, 13.81it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 15:47:54 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 1.846 | nll_loss 0.019 | accuracy 32.4 | wps 30491.6 | wpb 2452.7 | bsz 25 | num_updates 3890 | best_accuracy 32.4\n","2023-06-11 15:47:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3890 updates\n","2023-06-11 15:47:54 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/ssmba/checkpoint_best.pt\n","2023-06-11 15:48:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/ssmba/checkpoint_best.pt\n","2023-06-11 15:48:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/ssmba/checkpoint_best.pt (epoch 2 @ 3890 updates, score 32.4) (writing took 50.61739136400047 seconds)\n","2023-06-11 15:48:45 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2023-06-11 15:48:45 | INFO | train | epoch 002 | loss 1.352 | nll_loss 0.013 | accuracy 55.5 | wps 8748.2 | ups 3.3 | wpb 2649.1 | bsz 25.6 | num_updates 3890 | lr 9.725e-06 | gnorm 9.28 | loss_scale 4096 | train_wall 522 | gb_free 10.6 | wall 1161\n","2023-06-11 15:48:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 15:48:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1953\n","epoch 003:   0% 0/1953 [00:00<?, ?it/s]2023-06-11 15:48:45 | INFO | fairseq.trainer | begin training epoch 3\n","2023-06-11 15:48:45 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 003:   1% 14/1953 [00:04<09:41,  3.33it/s, loss=1.304, nll_loss=0.013, accuracy=59.6, wps=3263.3, ups=1.22, wpb=2670, bsz=25.8, num_updates=3900, lr=9.75e-06, gnorm=10.125, loss_scale=4096, train_wall=27, gb_free=10.5, wall=1165]2023-06-11 15:48:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:   4% 85/1953 [00:26<09:05,  3.43it/s, loss=1.304, nll_loss=0.013, accuracy=59.6, wps=3263.3, ups=1.22, wpb=2670, bsz=25.8, num_updates=3900, lr=9.75e-06, gnorm=10.125, loss_scale=4096, train_wall=27, gb_free=10.5, wall=1165]2023-06-11 15:49:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  18% 356/1953 [01:42<07:16,  3.66it/s, loss=1.194, nll_loss=0.012, accuracy=63.1, wps=9977.2, ups=3.69, wpb=2702.6, bsz=26.1, num_updates=4200, lr=9.759e-06, gnorm=10.463, loss_scale=4096, train_wall=27, gb_free=10.6, wall=1252]2023-06-11 15:50:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  25% 485/1953 [02:16<06:24,  3.82it/s, loss=1.172, nll_loss=0.011, accuracy=63.7, wps=9686.8, ups=3.73, wpb=2593.9, bsz=25.1, num_updates=4300, lr=9.64486e-06, gnorm=10.699, loss_scale=4096, train_wall=26, gb_free=11.1, wall=1278]2023-06-11 15:51:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  29% 572/1953 [02:41<06:44,  3.41it/s, loss=1.177, nll_loss=0.011, accuracy=62.9, wps=9815.3, ups=3.64, wpb=2696.7, bsz=26.2, num_updates=4400, lr=9.53463e-06, gnorm=10.522, loss_scale=4096, train_wall=27, gb_free=10.5, wall=1306]2023-06-11 15:51:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  42% 829/1953 [03:52<05:18,  3.53it/s, loss=1.166, nll_loss=0.011, accuracy=63.7, wps=9546.3, ups=3.59, wpb=2660.2, bsz=25.7, num_updates=4700, lr=9.22531e-06, gnorm=9.294, loss_scale=4096, train_wall=27, gb_free=10.3, wall=1390]2023-06-11 15:52:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  49% 960/1953 [04:28<05:07,  3.23it/s, loss=1.175, nll_loss=0.011, accuracy=64.7, wps=9773.3, ups=3.65, wpb=2680.6, bsz=25.8, num_updates=4800, lr=9.12871e-06, gnorm=10.035, loss_scale=4096, train_wall=27, gb_free=11.2, wall=1417]2023-06-11 15:53:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  56% 1091/1953 [05:04<03:37,  3.97it/s, loss=1.173, nll_loss=0.011, accuracy=63.3, wps=9778.4, ups=3.64, wpb=2689.3, bsz=26, num_updates=4900, lr=9.03508e-06, gnorm=10.245, loss_scale=4096, train_wall=27, gb_free=10.5, wall=1445]2023-06-11 15:53:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  64% 1248/1953 [05:47<03:15,  3.60it/s, loss=1.142, nll_loss=0.011, accuracy=65.3, wps=9560.4, ups=3.65, wpb=2616.2, bsz=25.2, num_updates=5100, lr=8.85615e-06, gnorm=10.322, loss_scale=4096, train_wall=27, gb_free=10.6, wall=1500]2023-06-11 15:54:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  71% 1377/1953 [06:22<02:45,  3.48it/s, loss=1.109, nll_loss=0.011, accuracy=66.4, wps=9594.6, ups=3.61, wpb=2656.9, bsz=25.7, num_updates=5200, lr=8.77058e-06, gnorm=9.365, loss_scale=4096, train_wall=27, gb_free=10.3, wall=1528]2023-06-11 15:55:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  73% 1434/1953 [06:37<02:09,  4.00it/s, loss=1.133, nll_loss=0.011, accuracy=65.3, wps=9891.4, ups=3.66, wpb=2699.7, bsz=26.2, num_updates=5300, lr=8.68744e-06, gnorm=9.439, loss_scale=4096, train_wall=27, gb_free=10.8, wall=1555]2023-06-11 15:55:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  87% 1696/1953 [07:48<01:01,  4.15it/s, loss=1.111, nll_loss=0.011, accuracy=66.6, wps=9691.7, ups=3.67, wpb=2641.6, bsz=25.5, num_updates=5500, lr=8.52803e-06, gnorm=9.671, loss_scale=4096, train_wall=27, gb_free=10.5, wall=1610]2023-06-11 15:56:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  95% 1848/1953 [08:29<00:30,  3.40it/s, loss=1.062, nll_loss=0.01, accuracy=67.6, wps=10139.5, ups=3.7, wpb=2744.1, bsz=26.6, num_updates=5700, lr=8.37708e-06, gnorm=10.061, loss_scale=4096, train_wall=26, gb_free=10.4, wall=1664]2023-06-11 15:57:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003: 100% 1952/1953 [08:57<00:00,  4.03it/s, loss=1.12, nll_loss=0.011, accuracy=66.6, wps=9548, ups=3.63, wpb=2632.2, bsz=25.4, num_updates=5800, lr=8.30455e-06, gnorm=9.359, loss_scale=4096, train_wall=27, gb_free=10.6, wall=1692]2023-06-11 15:57:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 15:57:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   2% 1/40 [00:00<00:04,  9.04it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   8% 3/40 [00:00<00:03, 11.79it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  12% 5/40 [00:00<00:02, 12.07it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  18% 7/40 [00:00<00:02, 12.51it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  22% 9/40 [00:00<00:02, 12.73it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  28% 11/40 [00:00<00:02, 13.33it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  32% 13/40 [00:01<00:02, 12.80it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  38% 15/40 [00:01<00:01, 12.52it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  42% 17/40 [00:01<00:01, 12.40it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  48% 19/40 [00:01<00:01, 13.12it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  52% 21/40 [00:01<00:01, 12.08it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  57% 23/40 [00:01<00:01, 11.65it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  62% 25/40 [00:02<00:01, 12.04it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  68% 27/40 [00:02<00:01, 12.60it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  72% 29/40 [00:02<00:00, 13.43it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  78% 31/40 [00:02<00:00, 12.79it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  82% 33/40 [00:02<00:00, 12.80it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  88% 35/40 [00:02<00:00, 12.41it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  92% 37/40 [00:02<00:00, 12.89it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  98% 39/40 [00:03<00:00, 13.72it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 15:57:46 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 2.061 | nll_loss 0.021 | accuracy 33 | wps 31619.4 | wpb 2452.7 | bsz 25 | num_updates 5830 | best_accuracy 33\n","2023-06-11 15:57:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 5830 updates\n","2023-06-11 15:57:46 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/ssmba/checkpoint_best.pt\n","2023-06-11 15:57:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/ssmba/checkpoint_best.pt\n","2023-06-11 15:58:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/ssmba/checkpoint_best.pt (epoch 3 @ 5830 updates, score 33.0) (writing took 44.55664654500015 seconds)\n","2023-06-11 15:58:31 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2023-06-11 15:58:31 | INFO | train | epoch 003 | loss 1.152 | nll_loss 0.011 | accuracy 64.7 | wps 8775.6 | ups 3.31 | wpb 2649.8 | bsz 25.6 | num_updates 5830 | lr 8.28315e-06 | gnorm 10.085 | loss_scale 4096 | train_wall 526 | gb_free 10.4 | wall 1747\n","2023-06-11 15:58:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 15:58:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1953\n","epoch 004:   0% 0/1953 [00:00<?, ?it/s]2023-06-11 15:58:31 | INFO | fairseq.trainer | begin training epoch 4\n","2023-06-11 15:58:31 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 004:   1% 26/1953 [00:09<09:08,  3.52it/s]2023-06-11 15:58:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:   4% 85/1953 [00:26<08:18,  3.75it/s, loss=1.043, nll_loss=0.01, accuracy=67.9, wps=3434.2, ups=1.26, wpb=2716.2, bsz=26.4, num_updates=5900, lr=8.23387e-06, gnorm=9.441, loss_scale=4096, train_wall=30, gb_free=10.5, wall=1771]2023-06-11 15:58:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  18% 342/1953 [01:40<08:06,  3.31it/s, loss=1.012, nll_loss=0.01, accuracy=70.7, wps=9740.6, ups=3.63, wpb=2685.3, bsz=25.9, num_updates=6100, lr=8.09776e-06, gnorm=9.274, loss_scale=4096, train_wall=27, gb_free=10.3, wall=1829]2023-06-11 16:00:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  25% 493/1953 [02:21<07:05,  3.43it/s, loss=1.043, nll_loss=0.01, accuracy=69.1, wps=9592.6, ups=3.64, wpb=2635.2, bsz=25.4, num_updates=6300, lr=7.96819e-06, gnorm=10.507, loss_scale=8192, train_wall=27, gb_free=10.5, wall=1884]2023-06-11 16:00:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  28% 544/1953 [02:36<06:19,  3.71it/s, loss=1.043, nll_loss=0.01, accuracy=69.1, wps=9592.6, ups=3.64, wpb=2635.2, bsz=25.4, num_updates=6300, lr=7.96819e-06, gnorm=10.507, loss_scale=8192, train_wall=27, gb_free=10.5, wall=1884]2023-06-11 16:01:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  41% 803/1953 [03:47<05:10,  3.71it/s, loss=1.021, nll_loss=0.01, accuracy=69.5, wps=9597.7, ups=3.65, wpb=2626.7, bsz=25.3, num_updates=6600, lr=7.78499e-06, gnorm=8.897, loss_scale=4096, train_wall=27, gb_free=10.5, wall=1967]2023-06-11 16:02:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  50% 982/1953 [04:36<04:28,  3.61it/s, loss=0.993, nll_loss=0.01, accuracy=69.8, wps=9621.3, ups=3.63, wpb=2647.7, bsz=25.4, num_updates=6800, lr=7.66965e-06, gnorm=9.562, loss_scale=8192, train_wall=27, gb_free=10.5, wall=2022]2023-06-11 16:03:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  57% 1114/1953 [05:11<04:02,  3.46it/s, loss=1.06, nll_loss=0.01, accuracy=67.8, wps=9759.2, ups=3.65, wpb=2672.7, bsz=25.8, num_updates=6900, lr=7.61387e-06, gnorm=9.878, loss_scale=4096, train_wall=27, gb_free=10.4, wall=2049]2023-06-11 16:03:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  64% 1248/1953 [05:48<03:20,  3.51it/s, loss=0.998, nll_loss=0.01, accuracy=70.5, wps=9750.5, ups=3.64, wpb=2678.7, bsz=26.1, num_updates=7000, lr=7.55929e-06, gnorm=9.691, loss_scale=4096, train_wall=27, gb_free=10.5, wall=2077]2023-06-11 16:04:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  71% 1393/1953 [06:27<02:37,  3.56it/s, loss=1.019, nll_loss=0.01, accuracy=69.3, wps=9557.2, ups=3.71, wpb=2578.5, bsz=24.9, num_updates=7200, lr=7.45356e-06, gnorm=9.754, loss_scale=8192, train_wall=26, gb_free=10.5, wall=2131]2023-06-11 16:04:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  78% 1523/1953 [07:02<01:55,  3.71it/s, loss=0.965, nll_loss=0.009, accuracy=72.2, wps=9424, ups=3.66, wpb=2571.9, bsz=24.8, num_updates=7300, lr=7.40233e-06, gnorm=10.344, loss_scale=4096, train_wall=27, gb_free=10.6, wall=2158]2023-06-11 16:05:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  85% 1659/1953 [07:40<01:18,  3.76it/s, loss=0.972, nll_loss=0.009, accuracy=71.2, wps=9502.7, ups=3.65, wpb=2606.2, bsz=25, num_updates=7400, lr=7.35215e-06, gnorm=10.071, loss_scale=4096, train_wall=27, gb_free=10.3, wall=2186]2023-06-11 16:06:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  92% 1788/1953 [08:15<00:46,  3.52it/s, loss=0.99, nll_loss=0.01, accuracy=70.6, wps=9864.6, ups=3.72, wpb=2648.9, bsz=25.5, num_updates=7600, lr=7.25476e-06, gnorm=9.735, loss_scale=4096, train_wall=26, gb_free=10.4, wall=2241]2023-06-11 16:06:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  98% 1922/1953 [08:51<00:07,  3.91it/s, loss=0.96, nll_loss=0.009, accuracy=71.5, wps=10132.4, ups=3.67, wpb=2758.1, bsz=26.8, num_updates=7700, lr=7.2075e-06, gnorm=9.208, loss_scale=4096, train_wall=27, gb_free=10.2, wall=2268]2023-06-11 16:07:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004: 100% 1952/1953 [08:59<00:00,  3.55it/s, loss=0.96, nll_loss=0.009, accuracy=71.5, wps=10132.4, ups=3.67, wpb=2758.1, bsz=26.8, num_updates=7700, lr=7.2075e-06, gnorm=9.208, loss_scale=4096, train_wall=27, gb_free=10.2, wall=2268]2023-06-11 16:07:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 16:07:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   2% 1/40 [00:00<00:04,  9.11it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   8% 3/40 [00:00<00:03, 11.98it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  12% 5/40 [00:00<00:02, 12.07it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  18% 7/40 [00:00<00:02, 12.51it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  22% 9/40 [00:00<00:02, 12.52it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  28% 11/40 [00:00<00:02, 12.78it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  32% 13/40 [00:01<00:02, 12.69it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  38% 15/40 [00:01<00:01, 12.62it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  42% 17/40 [00:01<00:01, 12.42it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  48% 19/40 [00:01<00:01, 12.99it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  52% 21/40 [00:01<00:01, 12.12it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  57% 23/40 [00:01<00:01, 11.71it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  62% 25/40 [00:02<00:01, 12.04it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  68% 27/40 [00:02<00:01, 12.63it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  72% 29/40 [00:02<00:00, 13.44it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  78% 31/40 [00:02<00:00, 12.71it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  82% 33/40 [00:02<00:00, 12.73it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  88% 35/40 [00:02<00:00, 12.48it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  92% 37/40 [00:02<00:00, 13.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  98% 39/40 [00:03<00:00, 13.74it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 16:07:33 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 2.286 | nll_loss 0.023 | accuracy 35.3 | wps 31575.3 | wpb 2452.7 | bsz 25 | num_updates 7769 | best_accuracy 35.3\n","2023-06-11 16:07:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 7769 updates\n","2023-06-11 16:07:33 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/ssmba/checkpoint_best.pt\n","2023-06-11 16:07:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/ssmba/checkpoint_best.pt\n","2023-06-11 16:08:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/ssmba/checkpoint_best.pt (epoch 4 @ 7769 updates, score 35.3) (writing took 33.489241215999755 seconds)\n","2023-06-11 16:08:07 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2023-06-11 16:08:07 | INFO | train | epoch 004 | loss 1 | nll_loss 0.01 | accuracy 70.3 | wps 8920.7 | ups 3.37 | wpb 2650.7 | bsz 25.6 | num_updates 7769 | lr 7.17542e-06 | gnorm 9.887 | loss_scale 4096 | train_wall 527 | gb_free 10.8 | wall 2323\n","2023-06-11 16:08:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 16:08:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1953\n","epoch 005:   0% 0/1953 [00:00<?, ?it/s]2023-06-11 16:08:07 | INFO | fairseq.trainer | begin training epoch 5\n","2023-06-11 16:08:07 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 005:   5% 98/1953 [00:31<09:01,  3.43it/s, loss=0.933, nll_loss=0.009, accuracy=72.6, wps=4161.4, ups=1.55, wpb=2686.8, bsz=25.8, num_updates=7800, lr=7.16115e-06, gnorm=9.668, loss_scale=4096, train_wall=27, gb_free=10.6, wall=2332]2023-06-11 16:08:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  12% 229/1953 [01:08<07:06,  4.04it/s, loss=0.865, nll_loss=0.008, accuracy=74.9, wps=8187.6, ups=3.17, wpb=2586.8, bsz=24.9, num_updates=7900, lr=7.11568e-06, gnorm=10.878, loss_scale=4096, train_wall=31, gb_free=11.5, wall=2364]2023-06-11 16:09:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  19% 364/1953 [01:44<07:07,  3.72it/s, loss=0.889, nll_loss=0.009, accuracy=74.4, wps=10172.7, ups=3.76, wpb=2705.2, bsz=26.3, num_updates=8100, lr=7.02728e-06, gnorm=10.239, loss_scale=4096, train_wall=26, gb_free=10.6, wall=2419]2023-06-11 16:09:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  25% 494/1953 [02:21<07:07,  3.41it/s, loss=0.888, nll_loss=0.009, accuracy=74.4, wps=9166.4, ups=3.55, wpb=2584.8, bsz=24.8, num_updates=8200, lr=6.9843e-06, gnorm=10.737, loss_scale=4096, train_wall=28, gb_free=10.4, wall=2447]2023-06-11 16:10:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  32% 627/1953 [02:57<06:12,  3.56it/s, loss=0.888, nll_loss=0.009, accuracy=74, wps=9407.6, ups=3.53, wpb=2663.2, bsz=25.6, num_updates=8300, lr=6.9421e-06, gnorm=11.277, loss_scale=4096, train_wall=28, gb_free=10.4, wall=2475]2023-06-11 16:11:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  39% 768/1953 [03:35<05:26,  3.63it/s, loss=0.857, nll_loss=0.008, accuracy=75.3, wps=9940.3, ups=3.77, wpb=2638.4, bsz=25.5, num_updates=8500, lr=6.85994e-06, gnorm=10.366, loss_scale=4096, train_wall=26, gb_free=10.6, wall=2530]2023-06-11 16:11:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  46% 903/1953 [04:10<04:41,  3.74it/s, loss=0.886, nll_loss=0.009, accuracy=73.9, wps=9751, ups=3.69, wpb=2645.9, bsz=25.5, num_updates=8600, lr=6.81994e-06, gnorm=11.086, loss_scale=4096, train_wall=27, gb_free=10.5, wall=2557]2023-06-11 16:12:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  53% 1036/1953 [04:46<04:20,  3.52it/s, loss=0.869, nll_loss=0.008, accuracy=74.1, wps=10211.5, ups=3.84, wpb=2660.3, bsz=25.8, num_updates=8700, lr=6.78064e-06, gnorm=10.084, loss_scale=4096, train_wall=26, gb_free=10.4, wall=2583]2023-06-11 16:12:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  60% 1172/1953 [05:23<03:57,  3.29it/s, loss=0.856, nll_loss=0.008, accuracy=75.4, wps=9924.4, ups=3.63, wpb=2733.6, bsz=26.5, num_updates=8900, lr=6.70402e-06, gnorm=10.45, loss_scale=4096, train_wall=27, gb_free=10.5, wall=2638]2023-06-11 16:13:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  67% 1301/1953 [05:58<02:53,  3.75it/s, loss=0.831, nll_loss=0.008, accuracy=75.8, wps=9425.6, ups=3.64, wpb=2589.9, bsz=24.9, num_updates=9000, lr=6.66667e-06, gnorm=11.204, loss_scale=4096, train_wall=27, gb_free=10.5, wall=2665]2023-06-11 16:14:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  74% 1444/1953 [06:38<02:23,  3.54it/s, loss=0.875, nll_loss=0.008, accuracy=73.8, wps=9515.9, ups=3.64, wpb=2615.6, bsz=25.3, num_updates=9200, lr=6.5938e-06, gnorm=11.22, loss_scale=8192, train_wall=27, gb_free=10.3, wall=2720]2023-06-11 16:14:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  81% 1576/1953 [07:14<01:46,  3.53it/s, loss=0.838, nll_loss=0.008, accuracy=76.5, wps=9453.9, ups=3.55, wpb=2665, bsz=25.7, num_updates=9300, lr=6.55826e-06, gnorm=11.089, loss_scale=4096, train_wall=28, gb_free=10.6, wall=2749]2023-06-11 16:15:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  88% 1711/1953 [07:52<01:06,  3.61it/s, loss=0.868, nll_loss=0.008, accuracy=75, wps=9468, ups=3.63, wpb=2605.4, bsz=25.1, num_updates=9400, lr=6.52328e-06, gnorm=11.264, loss_scale=4096, train_wall=27, gb_free=10.6, wall=2776]2023-06-11 16:15:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  95% 1851/1953 [08:30<00:25,  4.01it/s, loss=0.824, nll_loss=0.008, accuracy=75.8, wps=9605.5, ups=3.59, wpb=2672.3, bsz=25.8, num_updates=9600, lr=6.45497e-06, gnorm=10.684, loss_scale=8192, train_wall=27, gb_free=10.6, wall=2832]2023-06-11 16:16:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005: 100% 1952/1953 [08:57<00:00,  4.15it/s, loss=0.836, nll_loss=0.008, accuracy=76.5, wps=10139.8, ups=3.71, wpb=2736.6, bsz=26.5, num_updates=9700, lr=6.42161e-06, gnorm=10.69, loss_scale=4096, train_wall=26, gb_free=10.6, wall=2859]2023-06-11 16:17:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-11 16:17:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 005 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   2% 1/40 [00:00<00:04,  9.69it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   8% 3/40 [00:00<00:03, 11.76it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  12% 5/40 [00:00<00:02, 12.09it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  18% 7/40 [00:00<00:02, 12.50it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  22% 9/40 [00:00<00:02, 12.74it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  28% 11/40 [00:00<00:02, 13.15it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  32% 13/40 [00:01<00:02, 13.14it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  38% 15/40 [00:01<00:01, 13.23it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  42% 17/40 [00:01<00:01, 13.12it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  48% 19/40 [00:01<00:01, 13.90it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  52% 21/40 [00:01<00:01, 13.17it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  57% 23/40 [00:01<00:01, 12.56it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  62% 25/40 [00:01<00:01, 13.15it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  68% 27/40 [00:02<00:00, 13.65it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  72% 29/40 [00:02<00:00, 14.28it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  78% 31/40 [00:02<00:00, 13.46it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  82% 33/40 [00:02<00:00, 13.31it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  88% 35/40 [00:02<00:00, 12.88it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  92% 37/40 [00:02<00:00, 13.45it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  98% 39/40 [00:02<00:00, 14.17it/s]\u001b[A\n","                                                                        \u001b[A2023-06-11 16:17:08 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 2.649 | nll_loss 0.027 | accuracy 34.8 | wps 32952.8 | wpb 2452.7 | bsz 25 | num_updates 9708 | best_accuracy 35.3\n","2023-06-11 16:17:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 9708 updates\n","2023-06-11 16:17:08 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/ssmba/checkpoint_last.pt\n","2023-06-11 16:17:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/ssmba/checkpoint_last.pt\n","2023-06-11 16:17:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/ssmba/checkpoint_last.pt (epoch 5 @ 9708 updates, score 34.8) (writing took 9.632458211000085 seconds)\n","2023-06-11 16:17:17 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n","2023-06-11 16:17:17 | INFO | train | epoch 005 | loss 0.865 | nll_loss 0.008 | accuracy 74.9 | wps 9332.5 | ups 3.52 | wpb 2649.6 | bsz 25.6 | num_updates 9708 | lr 6.41897e-06 | gnorm 10.815 | loss_scale 4096 | train_wall 526 | gb_free 11.6 | wall 2874\n","2023-06-11 16:17:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-11 16:17:17 | INFO | fairseq_cli.train | done training in 2871.2 seconds\n"]}]},{"cell_type":"code","source":["from fairseq.models.roberta import RobertaModel\n","roberta = RobertaModel.from_pretrained(\n","    '/content/drive/MyDrive/NLP/checkpoints/R1/ssmba',\n","    checkpoint_file='checkpoint_best.pt',\n","    data_name_or_path='/content/drive/MyDrive/NLP/anli/R1/orig/bin_ssmba'\n",")\n","roberta.eval()"],"metadata":{"id":"KCNz5msTVI4v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["genre = ['R1','R2','R3']\n","for g in genre:\n","  input0 = open('/content/drive/MyDrive/NLP/anli/'+g+'/orig/test.raw.input0', \"r\")\n","  input1 = open('/content/drive/MyDrive/NLP/anli/'+g+'/orig/test.raw.input1', \"r\")\n","  label = open('/content/drive/MyDrive/NLP/anli/'+g+'/orig/test.raw.label', \"r\")\n","\n","  accuracy = 0\n","  total = 0\n","  for (x1, x2, y) in zip(input0, input1, label):\n","    tokens = roberta.encode(x1, x2)\n","    idx = roberta.predict('anli', tokens).argmax().item()\n","    dictionary = roberta.task.label_dictionary\n","    pred = dictionary[idx + dictionary.nspecial]\n","    total = total + 1\n","    if  (pred == y.strip()) :\n","      accuracy = accuracy + 1\n","\n","\n","  print(g,\": \",accuracy)\n","  print(g,\": \",total)"],"metadata":{"id":"kzFYIkioVJzM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686504206854,"user_tz":-120,"elapsed":1526099,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}},"outputId":"e59bca76-dc45-4f16-89f1-853c4742dad5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["R1 :  331\n","R1 :  1000\n","R2 :  360\n","R2 :  1000\n","R3 :  401\n","R3 :  1200\n"]}]},{"cell_type":"markdown","source":["# EDA_extended"],"metadata":{"id":"DYUSIfVlkkGw"}},{"cell_type":"code","source":["import tensorflow as tf\n","with tf.device('/device:GPU:0'):\n","  ! MAX_EPOCH=10 \\\n","      LR=1e-05 \\\n","      BATCH_SIZE=32 \\\n","      CUDA_VISIBLE_DEVICES=0 fairseq-train /content/drive/MyDrive/NLP/anli/R1/orig/bin_eda_extended \\\n","          --save-dir /content/drive/MyDrive/NLP/checkpoints/R1/eda_extended \\\n","          --reset-optimizer  \\\n","          --reset-dataloader  \\\n","          --reset-meters  \\\n","          --best-checkpoint-metric accuracy  \\\n","          --maximize-best-checkpoint-metric  \\\n","          --no-epoch-checkpoints \\\n","          --find-unused-parameters \\\n","          --distributed-world-size 1 \\\n","          --task sentence_prediction  \\\n","          --num-classes 3  \\\n","          --init-token 0  \\\n","          --separator-token 2   \\\n","          --max-positions 512  \\\n","          --shorten-method \"truncate\"  \\\n","          --arch roberta \\\n","          --dropout 0.1  \\\n","          --attention-dropout 0.1  \\\n","          --weight-decay 0.1  \\\n","          --criterion sentence_prediction  \\\n","          --classification-head-name 'anli' \\\n","          --optimizer adam  \\\n","          --adam-betas '(0.9, 0.98)'  \\\n","          --adam-eps 1e-06  \\\n","          --clip-norm 0.0  \\\n","          --lr-scheduler inverse_sqrt  \\\n","          --lr 1e-05 \\\n","          --fp16  \\\n","          --fp16-init-scale 4  \\\n","          --threshold-loss-scale 1  \\\n","          --fp16-scale-window 128  \\\n","          --batch-size 32  \\\n","          --required-batch-size-multiple 1  \\\n","          --max-tokens 4400 \\\n","          --update-freq 1  \\\n","          --max-update 123873 \\\n","          --max-epoch 5 \\\n","          --seed 100"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1BBp4cY1kjGj","executionInfo":{"status":"ok","timestamp":1686827242667,"user_tz":-120,"elapsed":2562546,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}},"outputId":"2faf0c5e-dc0e-4aaf-a6c4-7b4df81461bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-15 10:24:46.745112: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2023-06-15 10:24:48 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n","2023-06-15 10:24:51 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 100, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4400, 'batch_size': 32, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 123873, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/drive/MyDrive/NLP/checkpoints/R1/eda_extended', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=100, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=1.0, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4400, batch_size=32, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4400, batch_size_valid=32, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta', max_epoch=5, max_update=123873, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/content/drive/MyDrive/NLP/checkpoints/R1/eda_extended', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='/content/drive/MyDrive/NLP/anli/R1/orig/bin_eda_extended', num_classes=3, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, d2v2_multi=False, classification_head_name='anli', regression_target=False, report_mcc=False, report_acc_and_f1=False, report_pearson_and_spearman=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.1, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, max_positions=512, dropout=0.1, attention_dropout=0.1, no_seed_provided=False, encoder_layers=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_attention_heads=12, activation_dropout=0.0, pooler_dropout=0.0, max_source_positions=512, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta'), 'task': {'_name': 'sentence_prediction', 'data': '/content/drive/MyDrive/NLP/anli/R1/orig/bin_eda_extended', 'num_classes': 3, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 512, 'regression_target': False, 'classification_head_name': 'anli', 'seed': 100, 'd2v2_multi': False}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'anli', 'regression_target': False, 'report_mcc': False, 'report_acc_and_f1': False, 'report_pearson_and_spearman': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2023-06-15 10:24:53 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n","2023-06-15 10:24:54 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n","2023-06-15 10:24:57 | INFO | fairseq_cli.train | RobertaModel(\n","  (encoder): RobertaEncoder(\n","    (sentence_encoder): TransformerEncoder(\n","      (dropout_module): FairseqDropout()\n","      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","      (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (layers): ModuleList(\n","        (0-11): 12 x TransformerEncoderLayerBase(\n","          (self_attn): MultiheadAttention(\n","            (dropout_module): FairseqDropout()\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout_module): FairseqDropout()\n","          (activation_dropout_module): FairseqDropout()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (lm_head): RobertaLMHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (classification_heads): ModuleDict(\n","    (anli): RobertaClassificationHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","      (out_proj): Linear(in_features=768, out_features=3, bias=True)\n","    )\n","  )\n",")\n","2023-06-15 10:24:57 | INFO | fairseq_cli.train | task: SentencePredictionTask\n","2023-06-15 10:24:57 | INFO | fairseq_cli.train | model: RobertaModel\n","2023-06-15 10:24:57 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n","2023-06-15 10:24:57 | INFO | fairseq_cli.train | num. shared model params: 125,289,564 (num. trained: 125,289,564)\n","2023-06-15 10:24:57 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n","2023-06-15 10:24:59 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_eda_extended/input0/valid\n","2023-06-15 10:25:01 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_eda_extended/input1/valid\n","2023-06-15 10:25:03 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_eda_extended/label/valid\n","2023-06-15 10:25:03 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 1000\n","2023-06-15 10:25:08 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n","2023-06-15 10:25:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-15 10:25:08 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n","2023-06-15 10:25:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-15 10:25:08 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2023-06-15 10:25:08 | INFO | fairseq_cli.train | max tokens per device = 4400 and max sentences per device = 32\n","2023-06-15 10:25:08 | INFO | fairseq.trainer | Preparing to load checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/eda_extended/checkpoint_last.pt\n","2023-06-15 10:25:08 | INFO | fairseq.trainer | No existing checkpoint found /content/drive/MyDrive/NLP/checkpoints/R1/eda_extended/checkpoint_last.pt\n","2023-06-15 10:25:08 | INFO | fairseq.trainer | loading train data for epoch 1\n","2023-06-15 10:25:11 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_eda_extended/input0/train\n","2023-06-15 10:25:13 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_eda_extended/input1/train\n","2023-06-15 10:25:15 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/anli/R1/orig/bin_eda_extended/label/train\n","2023-06-15 10:25:15 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 50000\n","2023-06-15 10:25:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-15 10:25:15 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-15 10:25:15 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-15 10:25:15 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-15 10:25:15 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n","2023-06-15 10:25:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-15 10:25:15 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-15 10:25:15 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-15 10:25:15 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-15 10:25:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1765\n","epoch 001:   0% 0/1765 [00:00<?, ?it/s]2023-06-15 10:25:16 | INFO | fairseq.trainer | begin training epoch 1\n","2023-06-15 10:25:16 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 001:  80% 1417/1765 [06:11<01:33,  3.71it/s, loss=1.549, nll_loss=0.017, accuracy=42.1, wps=9800.4, ups=3.79, wpb=2585.6, bsz=27.9, num_updates=1400, lr=3.5e-06, gnorm=4.034, loss_scale=4096, train_wall=26, gb_free=10.5, wall=374]2023-06-15 10:31:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 001:  95% 1674/1765 [07:18<00:21,  4.19it/s, loss=1.541, nll_loss=0.017, accuracy=42, wps=10118.7, ups=3.83, wpb=2639.3, bsz=28.7, num_updates=1600, lr=4e-06, gnorm=4.291, loss_scale=8192, train_wall=25, gb_free=10.7, wall=428]2023-06-15 10:32:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8192.0\n","epoch 001:  95% 1679/1765 [07:19<00:19,  4.31it/s, loss=1.541, nll_loss=0.017, accuracy=42, wps=10118.7, ups=3.83, wpb=2639.3, bsz=28.7, num_updates=1600, lr=4e-06, gnorm=4.291, loss_scale=8192, train_wall=25, gb_free=10.7, wall=428]2023-06-15 10:32:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 001: 100% 1764/1765 [07:41<00:00,  3.62it/s, loss=1.522, nll_loss=0.016, accuracy=45, wps=10077.1, ups=3.83, wpb=2629.4, bsz=28.5, num_updates=1700, lr=4.25e-06, gnorm=4.276, loss_scale=4096, train_wall=26, gb_free=10.4, wall=454]2023-06-15 10:32:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-15 10:32:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   2% 1/40 [00:00<00:05,  7.61it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   5% 2/40 [00:00<00:04,  8.34it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  10% 4/40 [00:00<00:03,  9.92it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  15% 6/40 [00:00<00:03, 11.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  20% 8/40 [00:00<00:02, 11.88it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  25% 10/40 [00:00<00:02, 12.76it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  30% 12/40 [00:01<00:02, 12.93it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  35% 14/40 [00:01<00:02, 12.87it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  40% 16/40 [00:01<00:01, 12.86it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  45% 18/40 [00:01<00:01, 12.81it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  50% 20/40 [00:01<00:01, 13.26it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  55% 22/40 [00:01<00:01, 13.26it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  60% 24/40 [00:01<00:01, 13.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  65% 26/40 [00:02<00:01, 13.59it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  70% 28/40 [00:02<00:00, 13.95it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  75% 30/40 [00:02<00:00, 13.96it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  80% 32/40 [00:02<00:00, 13.67it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  85% 34/40 [00:02<00:00, 13.14it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  90% 36/40 [00:02<00:00, 13.13it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  95% 38/40 [00:02<00:00, 13.92it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset: 100% 40/40 [00:03<00:00, 14.01it/s]\u001b[A\n","                                                                        \u001b[A2023-06-15 10:33:01 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.637 | nll_loss 0.017 | accuracy 33.1 | wps 32459.9 | wpb 2452.7 | bsz 25 | num_updates 1762\n","2023-06-15 10:33:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1762 updates\n","2023-06-15 10:33:01 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda_extended/checkpoint_best.pt\n","2023-06-15 10:33:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda_extended/checkpoint_best.pt\n","2023-06-15 10:33:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/eda_extended/checkpoint_best.pt (epoch 1 @ 1762 updates, score 33.1) (writing took 26.48835780699983 seconds)\n","2023-06-15 10:33:27 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2023-06-15 10:33:27 | INFO | train | epoch 001 | loss 1.557 | nll_loss 0.017 | accuracy 41.4 | wps 9422.7 | ups 3.61 | wpb 2613 | bsz 28.3 | num_updates 1762 | lr 4.405e-06 | gnorm 4.628 | loss_scale 4096 | train_wall 452 | gb_free 10.8 | wall 499\n","2023-06-15 10:33:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-15 10:33:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1765\n","epoch 002:   0% 0/1765 [00:00<?, ?it/s]2023-06-15 10:33:27 | INFO | fairseq.trainer | begin training epoch 2\n","2023-06-15 10:33:27 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 002:   4% 68/1765 [00:19<08:07,  3.48it/s, loss=1.502, nll_loss=0.016, accuracy=47, wps=4767.9, ups=1.78, wpb=2674.8, bsz=29.1, num_updates=1800, lr=4.5e-06, gnorm=5.431, loss_scale=4096, train_wall=26, gb_free=10.5, wall=510]2023-06-15 10:33:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  17% 293/1765 [01:19<06:14,  3.94it/s, loss=1.45, nll_loss=0.016, accuracy=49.3, wps=9898.9, ups=3.8, wpb=2607.1, bsz=28.3, num_updates=2000, lr=5e-06, gnorm=7.841, loss_scale=8192, train_wall=26, gb_free=10.8, wall=565]2023-06-15 10:34:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  25% 441/1765 [01:58<05:01,  4.38it/s, loss=1.396, nll_loss=0.015, accuracy=53.2, wps=10035.4, ups=3.85, wpb=2607.4, bsz=28.3, num_updates=2200, lr=5.5e-06, gnorm=9.127, loss_scale=8192, train_wall=25, gb_free=10.5, wall=618]2023-06-15 10:35:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  33% 585/1765 [02:36<05:56,  3.31it/s, loss=1.4, nll_loss=0.015, accuracy=52, wps=10200.3, ups=3.83, wpb=2665.9, bsz=29.1, num_updates=2300, lr=5.75e-06, gnorm=9.459, loss_scale=4096, train_wall=25, gb_free=10.8, wall=644]2023-06-15 10:36:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  39% 697/1765 [03:05<04:44,  3.76it/s, loss=1.373, nll_loss=0.015, accuracy=55.5, wps=9842.3, ups=3.82, wpb=2575.4, bsz=27.8, num_updates=2400, lr=6e-06, gnorm=9.281, loss_scale=4096, train_wall=26, gb_free=10.8, wall=670]2023-06-15 10:36:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 002:  54% 957/1765 [04:12<03:30,  3.83it/s, loss=1.343, nll_loss=0.014, accuracy=56.1, wps=9933.9, ups=3.8, wpb=2611.2, bsz=28.1, num_updates=2700, lr=6.75e-06, gnorm=9.282, loss_scale=4096, train_wall=26, gb_free=10.1, wall=749]2023-06-15 10:37:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  61% 1074/1765 [04:43<02:58,  3.88it/s, loss=1.359, nll_loss=0.015, accuracy=55.7, wps=10126.1, ups=3.81, wpb=2654.8, bsz=28.8, num_updates=2800, lr=7e-06, gnorm=10.253, loss_scale=4096, train_wall=26, gb_free=10.8, wall=775]2023-06-15 10:38:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 002:  76% 1336/1765 [05:51<01:49,  3.92it/s, loss=1.344, nll_loss=0.015, accuracy=56.3, wps=9599.6, ups=3.76, wpb=2555.5, bsz=27.6, num_updates=3000, lr=7.5e-06, gnorm=10.219, loss_scale=4096, train_wall=26, gb_free=10.8, wall=828]2023-06-15 10:39:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  84% 1475/1765 [06:28<01:25,  3.39it/s, loss=1.325, nll_loss=0.014, accuracy=57.4, wps=9884.6, ups=3.8, wpb=2598.4, bsz=28.1, num_updates=3200, lr=8e-06, gnorm=10.155, loss_scale=4096, train_wall=26, gb_free=10.5, wall=880]2023-06-15 10:39:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  92% 1617/1765 [07:07<00:38,  3.87it/s, loss=1.32, nll_loss=0.014, accuracy=57.1, wps=9427.7, ups=3.66, wpb=2574.5, bsz=27.8, num_updates=3300, lr=8.25e-06, gnorm=9.283, loss_scale=4096, train_wall=27, gb_free=10.5, wall=908]2023-06-15 10:40:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  99% 1747/1765 [07:40<00:04,  3.96it/s, loss=1.292, nll_loss=0.014, accuracy=59, wps=9549.8, ups=3.71, wpb=2577.3, bsz=27.9, num_updates=3400, lr=8.5e-06, gnorm=10.214, loss_scale=4096, train_wall=26, gb_free=10.5, wall=935]2023-06-15 10:41:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002: 100% 1764/1765 [07:44<00:00,  3.63it/s, loss=1.242, nll_loss=0.014, accuracy=61.1, wps=10478.7, ups=3.93, wpb=2667.2, bsz=29.2, num_updates=3500, lr=8.75e-06, gnorm=9.351, loss_scale=4096, train_wall=25, gb_free=10.4, wall=960]2023-06-15 10:41:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-15 10:41:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   2% 1/40 [00:00<00:04,  8.90it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   5% 2/40 [00:00<00:05,  7.09it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  10% 4/40 [00:00<00:03,  9.74it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  15% 6/40 [00:00<00:03, 10.51it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  20% 8/40 [00:00<00:02, 11.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  25% 10/40 [00:00<00:02, 11.64it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  30% 12/40 [00:01<00:02, 12.05it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  35% 14/40 [00:01<00:02, 11.77it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  40% 16/40 [00:01<00:02, 11.92it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  45% 18/40 [00:01<00:01, 11.87it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  50% 20/40 [00:01<00:01, 12.27it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  55% 22/40 [00:01<00:01, 12.31it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  60% 24/40 [00:02<00:01, 12.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  65% 26/40 [00:02<00:01, 12.34it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  70% 28/40 [00:02<00:00, 12.82it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  75% 30/40 [00:02<00:00, 13.05it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  80% 32/40 [00:02<00:00, 13.05it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  85% 34/40 [00:02<00:00, 12.48it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  90% 36/40 [00:03<00:00, 12.59it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  95% 38/40 [00:03<00:00, 13.28it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset: 100% 40/40 [00:03<00:00, 13.45it/s]\u001b[A\n","                                                                        \u001b[A2023-06-15 10:41:16 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 2.041 | nll_loss 0.021 | accuracy 33.5 | wps 30214.9 | wpb 2452.7 | bsz 25 | num_updates 3516 | best_accuracy 33.5\n","2023-06-15 10:41:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3516 updates\n","2023-06-15 10:41:16 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda_extended/checkpoint_best.pt\n","2023-06-15 10:41:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda_extended/checkpoint_best.pt\n","2023-06-15 10:41:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/eda_extended/checkpoint_best.pt (epoch 2 @ 3516 updates, score 33.5) (writing took 40.83290986499992 seconds)\n","2023-06-15 10:41:56 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2023-06-15 10:41:56 | INFO | train | epoch 002 | loss 1.363 | nll_loss 0.015 | accuracy 54.8 | wps 9001.2 | ups 3.45 | wpb 2612.2 | bsz 28.3 | num_updates 3516 | lr 8.79e-06 | gnorm 9.263 | loss_scale 4096 | train_wall 454 | gb_free 10.5 | wall 1008\n","2023-06-15 10:41:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-15 10:41:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1765\n","epoch 003:   0% 0/1765 [00:00<?, ?it/s]2023-06-15 10:41:57 | INFO | fairseq.trainer | begin training epoch 3\n","2023-06-15 10:41:57 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 003:   1% 18/1765 [00:05<08:45,  3.32it/s]2023-06-15 10:42:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  16% 291/1765 [01:19<06:55,  3.55it/s, loss=1.209, nll_loss=0.013, accuracy=63.8, wps=9429.3, ups=3.62, wpb=2601.7, bsz=28.1, num_updates=3800, lr=9.5e-06, gnorm=9.688, loss_scale=8192, train_wall=27, gb_free=10.6, wall=1087]2023-06-15 10:43:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  24% 423/1765 [01:56<07:03,  3.17it/s, loss=1.184, nll_loss=0.013, accuracy=64.4, wps=9362.9, ups=3.55, wpb=2637.6, bsz=28.5, num_updates=3900, lr=9.75e-06, gnorm=9.763, loss_scale=4096, train_wall=27, gb_free=10.4, wall=1115]2023-06-15 10:43:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  28% 501/1765 [02:16<05:32,  3.80it/s, loss=1.176, nll_loss=0.013, accuracy=63.5, wps=9849.7, ups=3.72, wpb=2647.9, bsz=28.7, num_updates=4000, lr=1e-05, gnorm=10.357, loss_scale=4096, train_wall=26, gb_free=10.2, wall=1142]2023-06-15 10:44:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  43% 763/1765 [03:25<03:52,  4.32it/s, loss=1.197, nll_loss=0.013, accuracy=63.7, wps=10315.9, ups=3.89, wpb=2653.2, bsz=28.5, num_updates=4200, lr=9.759e-06, gnorm=9.039, loss_scale=4096, train_wall=25, gb_free=10.7, wall=1195]2023-06-15 10:45:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  47% 828/1765 [03:42<04:29,  3.47it/s, loss=1.168, nll_loss=0.013, accuracy=64.4, wps=10109.3, ups=3.81, wpb=2654.2, bsz=28.8, num_updates=4300, lr=9.64486e-06, gnorm=9.64, loss_scale=4096, train_wall=26, gb_free=10.5, wall=1221]2023-06-15 10:45:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  63% 1114/1765 [04:57<02:41,  4.04it/s, loss=1.161, nll_loss=0.013, accuracy=65.8, wps=9961.7, ups=3.8, wpb=2623.1, bsz=28.3, num_updates=4600, lr=9.32505e-06, gnorm=8.735, loss_scale=8192, train_wall=26, gb_free=10.8, wall=1299]2023-06-15 10:46:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  70% 1244/1765 [05:30<02:08,  4.05it/s, loss=1.133, nll_loss=0.012, accuracy=65.1, wps=10056.7, ups=3.8, wpb=2647.6, bsz=28.8, num_updates=4700, lr=9.22531e-06, gnorm=10.126, loss_scale=4096, train_wall=26, gb_free=10.7, wall=1326]2023-06-15 10:47:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  78% 1377/1765 [06:05<01:44,  3.73it/s, loss=1.143, nll_loss=0.012, accuracy=65.3, wps=9958.6, ups=3.86, wpb=2578.3, bsz=28.1, num_updates=4800, lr=9.12871e-06, gnorm=9.064, loss_scale=4096, train_wall=25, gb_free=10.4, wall=1352]2023-06-15 10:48:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  83% 1471/1765 [06:29<01:19,  3.69it/s, loss=1.127, nll_loss=0.012, accuracy=66.9, wps=9837.6, ups=3.77, wpb=2606.5, bsz=28.3, num_updates=4900, lr=9.03508e-06, gnorm=9.052, loss_scale=4096, train_wall=26, gb_free=10.6, wall=1378]2023-06-15 10:48:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  99% 1739/1765 [07:40<00:06,  3.74it/s, loss=1.153, nll_loss=0.013, accuracy=65.1, wps=10207, ups=3.84, wpb=2656.6, bsz=28.9, num_updates=5200, lr=8.77058e-06, gnorm=9.535, loss_scale=4096, train_wall=25, gb_free=10.4, wall=1457]2023-06-15 10:49:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003: 100% 1764/1765 [07:46<00:00,  4.06it/s, loss=1.153, nll_loss=0.013, accuracy=65.1, wps=10207, ups=3.84, wpb=2656.6, bsz=28.9, num_updates=5200, lr=8.77058e-06, gnorm=9.535, loss_scale=4096, train_wall=25, gb_free=10.4, wall=1457]2023-06-15 10:49:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-15 10:49:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   2% 1/40 [00:00<00:04,  9.50it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   8% 3/40 [00:00<00:03, 12.08it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  12% 5/40 [00:00<00:02, 12.24it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  18% 7/40 [00:00<00:02, 12.62it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  22% 9/40 [00:00<00:02, 12.47it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  28% 11/40 [00:00<00:02, 12.98it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  32% 13/40 [00:01<00:02, 12.77it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  38% 15/40 [00:01<00:01, 12.63it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  42% 17/40 [00:01<00:01, 12.15it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  48% 19/40 [00:01<00:01, 12.82it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  52% 21/40 [00:01<00:01, 12.31it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  57% 23/40 [00:01<00:01, 11.77it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  62% 25/40 [00:02<00:01, 12.11it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  68% 27/40 [00:02<00:01, 12.69it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  72% 29/40 [00:02<00:00, 13.29it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  78% 31/40 [00:02<00:00, 12.68it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  82% 33/40 [00:02<00:00, 12.71it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  88% 35/40 [00:02<00:00, 12.41it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  92% 37/40 [00:02<00:00, 12.91it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  98% 39/40 [00:03<00:00, 13.67it/s]\u001b[A\n","                                                                        \u001b[A2023-06-15 10:49:47 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 2.181 | nll_loss 0.022 | accuracy 33.6 | wps 31543.8 | wpb 2452.7 | bsz 25 | num_updates 5270 | best_accuracy 33.6\n","2023-06-15 10:49:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 5270 updates\n","2023-06-15 10:49:47 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda_extended/checkpoint_best.pt\n","2023-06-15 10:49:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda_extended/checkpoint_best.pt\n","2023-06-15 10:50:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/eda_extended/checkpoint_best.pt (epoch 3 @ 5270 updates, score 33.6) (writing took 39.809641934999945 seconds)\n","2023-06-15 10:50:26 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2023-06-15 10:50:26 | INFO | train | epoch 003 | loss 1.163 | nll_loss 0.013 | accuracy 64.6 | wps 8987.1 | ups 3.44 | wpb 2612.4 | bsz 28.3 | num_updates 5270 | lr 8.71214e-06 | gnorm 9.607 | loss_scale 4096 | train_wall 456 | gb_free 10.5 | wall 1518\n","2023-06-15 10:50:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-15 10:50:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1765\n","epoch 004:   0% 0/1765 [00:00<?, ?it/s]2023-06-15 10:50:26 | INFO | fairseq.trainer | begin training epoch 4\n","2023-06-15 10:50:26 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 004:   6% 104/1765 [00:29<07:44,  3.58it/s, loss=1.066, nll_loss=0.012, accuracy=68.2, wps=3729.3, ups=1.42, wpb=2629.7, bsz=28.4, num_updates=5300, lr=8.68744e-06, gnorm=9.048, loss_scale=4096, train_wall=27, gb_free=10.3, wall=1527]2023-06-15 10:50:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  14% 253/1765 [01:10<07:21,  3.43it/s, loss=1.03, nll_loss=0.011, accuracy=68.1, wps=9531.5, ups=3.6, wpb=2649.9, bsz=28.8, num_updates=5500, lr=8.52803e-06, gnorm=9.119, loss_scale=4096, train_wall=27, gb_free=10.4, wall=1583]2023-06-15 10:51:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  22% 390/1765 [01:46<05:04,  4.52it/s, loss=1.05, nll_loss=0.011, accuracy=67.9, wps=9709.6, ups=3.75, wpb=2590, bsz=28, num_updates=5600, lr=8.45154e-06, gnorm=9.415, loss_scale=4096, train_wall=26, gb_free=10.3, wall=1609]2023-06-15 10:52:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  31% 542/1765 [02:26<05:27,  3.73it/s, loss=1.048, nll_loss=0.011, accuracy=68.5, wps=9850.3, ups=3.8, wpb=2591.5, bsz=28.1, num_updates=5800, lr=8.30455e-06, gnorm=8.598, loss_scale=8192, train_wall=26, gb_free=10.3, wall=1662]2023-06-15 10:52:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  38% 675/1765 [03:01<04:43,  3.85it/s, loss=1.041, nll_loss=0.011, accuracy=69, wps=9677.8, ups=3.71, wpb=2610.1, bsz=28.1, num_updates=5900, lr=8.23387e-06, gnorm=9.565, loss_scale=4096, train_wall=26, gb_free=10.6, wall=1689]2023-06-15 10:53:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  45% 796/1765 [03:33<04:23,  3.67it/s, loss=1.047, nll_loss=0.011, accuracy=68.5, wps=9992.6, ups=3.76, wpb=2657.7, bsz=29, num_updates=6000, lr=8.16497e-06, gnorm=8.778, loss_scale=4096, train_wall=26, gb_free=10.9, wall=1716]2023-06-15 10:54:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  61% 1075/1765 [04:46<03:13,  3.57it/s, loss=1.023, nll_loss=0.011, accuracy=69.8, wps=9907.9, ups=3.82, wpb=2595.8, bsz=27.9, num_updates=6300, lr=7.96819e-06, gnorm=8.95, loss_scale=4096, train_wall=26, gb_free=10.7, wall=1794]2023-06-15 10:55:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  69% 1214/1765 [05:22<02:26,  3.76it/s, loss=1.025, nll_loss=0.011, accuracy=68.8, wps=9768.4, ups=3.73, wpb=2616.2, bsz=28.4, num_updates=6400, lr=7.90569e-06, gnorm=8.656, loss_scale=4096, train_wall=26, gb_free=11.4, wall=1821]2023-06-15 10:55:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  76% 1345/1765 [05:56<01:40,  4.19it/s, loss=1, nll_loss=0.011, accuracy=70.3, wps=9903.5, ups=3.78, wpb=2618, bsz=28.4, num_updates=6600, lr=7.78499e-06, gnorm=9.412, loss_scale=4096, train_wall=26, gb_free=10.6, wall=1873]2023-06-15 10:56:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  84% 1478/1765 [06:31<01:19,  3.59it/s, loss=1.028, nll_loss=0.011, accuracy=68.9, wps=10169, ups=3.83, wpb=2652.1, bsz=28.9, num_updates=6700, lr=7.72667e-06, gnorm=8.958, loss_scale=4096, train_wall=26, gb_free=10.6, wall=1899]2023-06-15 10:56:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  92% 1616/1765 [07:07<00:36,  4.13it/s, loss=0.994, nll_loss=0.011, accuracy=70.9, wps=9762.9, ups=3.78, wpb=2580.2, bsz=27.8, num_updates=6800, lr=7.66965e-06, gnorm=8.467, loss_scale=4096, train_wall=26, gb_free=10.5, wall=1926]2023-06-15 10:57:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:  99% 1753/1765 [07:44<00:03,  3.98it/s, loss=1.008, nll_loss=0.011, accuracy=69.6, wps=9762.6, ups=3.71, wpb=2632.8, bsz=28.6, num_updates=7000, lr=7.55929e-06, gnorm=8.77, loss_scale=4096, train_wall=26, gb_free=11.2, wall=1979]2023-06-15 10:58:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004: 100% 1764/1765 [07:46<00:00,  3.86it/s, loss=1.008, nll_loss=0.011, accuracy=69.6, wps=9762.6, ups=3.71, wpb=2632.8, bsz=28.6, num_updates=7000, lr=7.55929e-06, gnorm=8.77, loss_scale=4096, train_wall=26, gb_free=11.2, wall=1979]2023-06-15 10:58:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-15 10:58:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   2% 1/40 [00:00<00:03,  9.87it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   8% 3/40 [00:00<00:02, 12.43it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  12% 5/40 [00:00<00:02, 12.19it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  18% 7/40 [00:00<00:02, 12.52it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  22% 9/40 [00:00<00:02, 12.67it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  28% 11/40 [00:00<00:02, 13.32it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  32% 13/40 [00:01<00:02, 13.28it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  38% 15/40 [00:01<00:01, 13.31it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  42% 17/40 [00:01<00:01, 13.22it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  48% 19/40 [00:01<00:01, 14.00it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  52% 21/40 [00:01<00:01, 13.33it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  57% 23/40 [00:01<00:01, 12.71it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  62% 25/40 [00:01<00:01, 13.26it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  68% 27/40 [00:02<00:00, 13.84it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  72% 29/40 [00:02<00:00, 14.47it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  78% 31/40 [00:02<00:00, 13.48it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  82% 33/40 [00:02<00:00, 13.24it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  88% 35/40 [00:02<00:00, 12.85it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  92% 37/40 [00:02<00:00, 13.41it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  98% 39/40 [00:02<00:00, 14.17it/s]\u001b[A\n","                                                                        \u001b[A2023-06-15 10:58:16 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 2.252 | nll_loss 0.023 | accuracy 35.5 | wps 33183.1 | wpb 2452.7 | bsz 25 | num_updates 7023 | best_accuracy 35.5\n","2023-06-15 10:58:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 7023 updates\n","2023-06-15 10:58:16 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda_extended/checkpoint_best.pt\n","2023-06-15 10:58:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda_extended/checkpoint_best.pt\n","2023-06-15 10:59:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/eda_extended/checkpoint_best.pt (epoch 4 @ 7023 updates, score 35.5) (writing took 52.18069170300032 seconds)\n","2023-06-15 10:59:09 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2023-06-15 10:59:09 | INFO | train | epoch 004 | loss 1.028 | nll_loss 0.011 | accuracy 69.3 | wps 8765.7 | ups 3.36 | wpb 2612 | bsz 28.3 | num_updates 7023 | lr 7.5469e-06 | gnorm 9.089 | loss_scale 4096 | train_wall 457 | gb_free 10.8 | wall 2041\n","2023-06-15 10:59:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-15 10:59:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1765\n","epoch 005:   0% 0/1765 [00:00<?, ?it/s]2023-06-15 10:59:09 | INFO | fairseq.trainer | begin training epoch 5\n","2023-06-15 10:59:09 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 005:   7% 122/1765 [00:33<07:01,  3.90it/s, loss=0.967, nll_loss=0.01, accuracy=71.1, wps=3196.6, ups=1.22, wpb=2630, bsz=28.4, num_updates=7100, lr=7.50587e-06, gnorm=9.012, loss_scale=4096, train_wall=26, gb_free=10.7, wall=2062]2023-06-15 10:59:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  16% 278/1765 [01:16<06:36,  3.75it/s, loss=0.93, nll_loss=0.01, accuracy=73.4, wps=9464.5, ups=3.62, wpb=2612.3, bsz=28.2, num_updates=7300, lr=7.40233e-06, gnorm=9.076, loss_scale=8192, train_wall=27, gb_free=10.4, wall=2117]2023-06-15 11:00:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  23% 410/1765 [01:52<05:59,  3.77it/s, loss=0.944, nll_loss=0.01, accuracy=71.8, wps=9647.4, ups=3.64, wpb=2652.1, bsz=28.8, num_updates=7400, lr=7.35215e-06, gnorm=9.599, loss_scale=4096, train_wall=27, gb_free=10.4, wall=2144]2023-06-15 11:01:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  31% 541/1765 [02:26<05:57,  3.42it/s, loss=0.935, nll_loss=0.01, accuracy=73, wps=9771.6, ups=3.72, wpb=2629.9, bsz=28.6, num_updates=7500, lr=7.30297e-06, gnorm=8.632, loss_scale=4096, train_wall=26, gb_free=10.6, wall=2171]2023-06-15 11:01:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  38% 674/1765 [03:01<05:06,  3.56it/s, loss=0.945, nll_loss=0.01, accuracy=71.7, wps=9925.6, ups=3.74, wpb=2653.5, bsz=28.8, num_updates=7600, lr=7.25476e-06, gnorm=9.343, loss_scale=4096, train_wall=26, gb_free=10.8, wall=2198]2023-06-15 11:02:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  46% 807/1765 [03:36<04:18,  3.71it/s, loss=0.902, nll_loss=0.01, accuracy=74.5, wps=9653.3, ups=3.8, wpb=2537.6, bsz=27.5, num_updates=7800, lr=7.16115e-06, gnorm=9.276, loss_scale=4096, train_wall=26, gb_free=10.5, wall=2250]2023-06-15 11:02:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  54% 949/1765 [04:12<03:54,  3.48it/s, loss=0.919, nll_loss=0.01, accuracy=73.7, wps=10310.3, ups=3.88, wpb=2656.7, bsz=29.1, num_updates=7900, lr=7.11568e-06, gnorm=8.953, loss_scale=4096, train_wall=25, gb_free=11.6, wall=2276]2023-06-15 11:03:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  58% 1020/1765 [04:31<03:13,  3.86it/s, loss=0.92, nll_loss=0.01, accuracy=73.3, wps=9703.5, ups=3.7, wpb=2619.2, bsz=28.2, num_updates=8000, lr=7.07107e-06, gnorm=9.891, loss_scale=4096, train_wall=26, gb_free=10.4, wall=2303]2023-06-15 11:03:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  73% 1291/1765 [05:42<01:53,  4.19it/s, loss=0.876, nll_loss=0.01, accuracy=74.1, wps=10182.9, ups=3.91, wpb=2602.7, bsz=28.3, num_updates=8300, lr=6.9421e-06, gnorm=9.75, loss_scale=8192, train_wall=25, gb_free=10.6, wall=2381]2023-06-15 11:04:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  81% 1426/1765 [06:17<01:28,  3.85it/s, loss=0.902, nll_loss=0.01, accuracy=73.2, wps=10251.6, ups=3.87, wpb=2648.1, bsz=28.6, num_updates=8400, lr=6.90066e-06, gnorm=9.518, loss_scale=4096, train_wall=25, gb_free=11.1, wall=2407]2023-06-15 11:05:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  90% 1584/1765 [06:58<00:43,  4.20it/s, loss=0.883, nll_loss=0.01, accuracy=74.3, wps=9994.2, ups=3.76, wpb=2658, bsz=28.9, num_updates=8500, lr=6.85994e-06, gnorm=8.957, loss_scale=4096, train_wall=26, gb_free=10.4, wall=2434]2023-06-15 11:06:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  98% 1733/1765 [07:38<00:07,  4.41it/s, loss=0.896, nll_loss=0.01, accuracy=73.2, wps=9662.9, ups=3.73, wpb=2591.4, bsz=28, num_updates=8700, lr=6.78064e-06, gnorm=10.161, loss_scale=4096, train_wall=26, gb_free=10.6, wall=2487]2023-06-15 11:06:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005: 100% 1764/1765 [07:46<00:00,  3.90it/s, loss=0.896, nll_loss=0.01, accuracy=73.2, wps=9662.9, ups=3.73, wpb=2591.4, bsz=28, num_updates=8700, lr=6.78064e-06, gnorm=10.161, loss_scale=4096, train_wall=26, gb_free=10.6, wall=2487]2023-06-15 11:06:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-15 11:06:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 005 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   2% 1/40 [00:00<00:04,  9.09it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   8% 3/40 [00:00<00:03, 12.19it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  12% 5/40 [00:00<00:02, 12.25it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  18% 7/40 [00:00<00:02, 12.77it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  22% 9/40 [00:00<00:02, 12.84it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  28% 11/40 [00:00<00:02, 13.45it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  32% 13/40 [00:01<00:02, 13.27it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  38% 15/40 [00:01<00:01, 13.28it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  42% 17/40 [00:01<00:01, 13.08it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  48% 19/40 [00:01<00:01, 13.96it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  52% 21/40 [00:01<00:01, 13.30it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  57% 23/40 [00:01<00:01, 12.73it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  62% 25/40 [00:01<00:01, 13.30it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  68% 27/40 [00:02<00:00, 13.94it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  72% 29/40 [00:02<00:00, 14.56it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  78% 31/40 [00:02<00:00, 13.52it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  82% 33/40 [00:02<00:00, 13.33it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  88% 35/40 [00:02<00:00, 12.92it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  92% 37/40 [00:02<00:00, 13.46it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  98% 39/40 [00:02<00:00, 14.27it/s]\u001b[A\n","                                                                        \u001b[A2023-06-15 11:06:58 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 2.3 | nll_loss 0.023 | accuracy 34.3 | wps 33315.8 | wpb 2452.7 | bsz 25 | num_updates 8776 | best_accuracy 35.5\n","2023-06-15 11:06:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 8776 updates\n","2023-06-15 11:06:58 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda_extended/checkpoint_last.pt\n","2023-06-15 11:07:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/R1/eda_extended/checkpoint_last.pt\n","2023-06-15 11:07:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/R1/eda_extended/checkpoint_last.pt (epoch 5 @ 8776 updates, score 34.3) (writing took 8.231436390999988 seconds)\n","2023-06-15 11:07:06 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n","2023-06-15 11:07:06 | INFO | train | epoch 005 | loss 0.92 | nll_loss 0.01 | accuracy 73 | wps 9585.2 | ups 3.67 | wpb 2612.5 | bsz 28.3 | num_updates 8776 | lr 6.75121e-06 | gnorm 9.415 | loss_scale 4096 | train_wall 456 | gb_free 10.6 | wall 2518\n","2023-06-15 11:07:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-15 11:07:06 | INFO | fairseq_cli.train | done training in 2510.7 seconds\n"]}]},{"cell_type":"code","source":["from fairseq.models.roberta import RobertaModel\n","roberta = RobertaModel.from_pretrained(\n","    '/content/drive/MyDrive/NLP/checkpoints/R1/eda_extended',\n","    checkpoint_file='checkpoint_best.pt',\n","    data_name_or_path='/content/drive/MyDrive/NLP/anli/R1/orig/bin_eda_extended'\n",")\n","roberta.eval()"],"metadata":{"id":"fLXzJInN1Wn8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["genre = ['R1','R2','R3']\n","for g in genre:\n","  input0 = open('/content/drive/MyDrive/NLP/anli/'+g+'/orig/test.raw.input0', \"r\")\n","  input1 = open('/content/drive/MyDrive/NLP/anli/'+g+'/orig/test.raw.input1', \"r\")\n","  label = open('/content/drive/MyDrive/NLP/anli/'+g+'/orig/test.raw.label', \"r\")\n","\n","  accuracy = 0\n","  total = 0\n","  for (x1, x2, y) in zip(input0, input1, label):\n","    tokens = roberta.encode(x1, x2)\n","    idx = roberta.predict('anli', tokens).argmax().item()\n","    dictionary = roberta.task.label_dictionary\n","    pred = dictionary[idx + dictionary.nspecial]\n","    total = total + 1\n","    if  (pred == y.strip()) :\n","      accuracy = accuracy + 1\n","\n","\n","  print(g,\": \",accuracy)\n","  print(g,\": \",total)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u0sUyt3N1XJS","executionInfo":{"status":"ok","timestamp":1686830387078,"user_tz":-120,"elapsed":1311312,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}},"outputId":"0ce9de67-c498-465b-a58f-2672fe5870ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["R1 :  340\n","R1 :  1000\n","R2 :  358\n","R2 :  1000\n","R3 :  422\n","R3 :  1200\n"]}]}]}