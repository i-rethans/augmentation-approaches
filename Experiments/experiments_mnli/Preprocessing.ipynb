{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO5e3NDZYLWqW5r5/NTaflV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Following is needed for connecting to google drive and setting up fairseq to for tokenization and binarization.\n"],"metadata":{"id":"LOYx5MdO4rpO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ECB0HlLA-h6"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["! pip install tensorboardX\n","! pip install tensorrt"],"metadata":{"id":"ajM3DiMkB-44"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install cython -U\n","!git clone https://github.com/pytorch/fairseq.git\n","%cd fairseq\n","!pip install --quiet --editable .\n","!pip install --quiet sentencepiece"],"metadata":{"id":"uvrEpD7lB_dl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! mkdir -p gpt2_bpe\n","! wget -O gpt2_bpe/encoder.json https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n","! wget -O gpt2_bpe/vocab.bpe https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\n","! wget -O gpt2_bpe/dict.txt https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt"],"metadata":{"id":"Jyw57kZNBe95"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Truncation\n","\n","**Run only once!!!** (r2 added for failsafe)"],"metadata":{"id":"dhDxC7nz5BJG"}},{"cell_type":"code","source":["import numpy as np\n","import random\n","f1 = open('/content/drive/MyDrive/NLP/mnli/fiction/orig/train.raw.input0', 'r')\n","f2 = open('/content/drive/MyDrive/NLP/mnli/fiction/orig/train.raw.input1', 'r')\n","f3 = open('/content/drive/MyDrive/NLP/mnli/fiction/orig/train.raw.label', 'r')\n","\n","o1 = open('/content/drive/MyDrive/NLP/mnli/fiction/orig/trainr2.raw.input0', 'w')\n","o2 = open('/content/drive/MyDrive/NLP/mnli/fiction/orig/trainr2.raw.input1', 'w')\n","o3 = open('/content/drive/MyDrive/NLP/mnli/fiction/orig/trainr2.raw.label', 'w')\n","\n","\n","lines1 = f1.read().splitlines()\n","lines2 = f2.read().splitlines()\n","lines3 = f3.read().splitlines()\n","\n","arr = random.sample(range(1, len(lines1)), 10000)\n","\n","for i in arr:\n","    o1.write(lines1[i].rstrip()+\"\\n\")\n","    o2.write(lines2[i].rstrip()+\"\\n\")\n","    o3.write(lines3[i].rstrip()+\"\\n\")\n","\n","o1.close()\n","o2.close()\n","o3.close()"],"metadata":{"id":"KeuF5xnq5F9y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# No augmentation\n","\n","Target: \\bin\n"],"metadata":{"id":"7BizGw9zBvWB"}},{"cell_type":"code","source":["!for GENRE in fiction; do \\\n","    for SPLIT in trainr valid; do \\\n","      for TYPE in input0 input1; do \\\n","        python /content/fairseq/examples/roberta/multiprocessing_bpe_encoder.py \\\n","            --encoder-json gpt2_bpe/encoder.json \\\n","            --vocab-bpe gpt2_bpe/vocab.bpe \\\n","            --inputs /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/${SPLIT}.raw.${TYPE} \\\n","            --outputs /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/${SPLIT}.bpe.${TYPE} \\\n","            --keep-empty \\\n","            --workers 60; \\\n","      done \\\n","    done \\\n","  done\n"],"metadata":{"id":"LPu6T8lXBrUD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!for GENRE in fiction; do \\\n","    for TYPE in input0 input1; do \\\n","      fairseq-preprocess \\\n","      --only-source \\\n","      --srcdict gpt2_bpe/dict.txt \\\n","      --trainpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/trainr.bpe.${TYPE} \\\n","      --validpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/valid.bpe.${TYPE} \\\n","      --testpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/test.bpe.${TYPE} \\\n","      --destdir /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/bin/${TYPE} \\\n","      --workers 60;\\\n","    done \\\n","  done"],"metadata":{"id":"aLD9ve8iBfsp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!for GENRE in fiction; do \\\n","    for TYPE in label; do \\\n","      fairseq-preprocess \\\n","      --only-source \\\n","      --trainpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/trainr.raw.${TYPE} \\\n","      --validpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/valid.raw.${TYPE} \\\n","      --testpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/test.raw.${TYPE} \\\n","      --destdir /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/bin/${TYPE} \\\n","      --workers 60;\\\n","    done \\\n","  done"],"metadata":{"id":"Sx5SyO7sCtBm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# EDA\n","\n","Target: \\bin_eda"],"metadata":{"id":"18NWeP-8CvXO"}},{"cell_type":"code","source":["import nltk; nltk.download('wordnet')"],"metadata":{"id":"4UaWUOOoC6Ws","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686210719443,"user_tz":-120,"elapsed":2915,"user":{"displayName":"Louise Leibbrandt","userId":"11762724927321883770"}},"outputId":"b7462515-afb2-4fa1-dfd1-0c04363829e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["import subprocess\n","\n","def execute_shell_command(command):\n","    process = subprocess.Popen(command, shell=True)\n","    process.wait()\n","\n","command = \"\"\"\n","for GENRE in fiction; do \\\n","    for TYPE in input0 input1; do \\\n","        paste -d \"\\t\" /content/drive/MyDrive/NLP/mnli/$GENRE/orig/trainr.raw.label /content/drive/MyDrive/NLP/mnli/$GENRE/orig/trainr.raw.$TYPE > /content/drive/MyDrive/NLP/mnli/$GENRE/orig/eda/train.combined.label.$TYPE; \\\n","    done \\\n","done\n","\"\"\"\n","\n","execute_shell_command(command)"],"metadata":{"id":"mZ3tNUmiC80o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import subprocess\n","\n","def execute_shell_command(command):\n","    process = subprocess.Popen(command, shell=True)\n","    process.wait()\n","\n","genres = ['fiction']\n","types = ['input0', 'input1']\n","\n","for genre in genres:\n","    for type_ in types:\n","        command = f\"python /content/drive/MyDrive/NLP/eda_nlp-master/code/augment.py \\\n","                    --input /content/drive/MyDrive/NLP/mnli/{genre}/orig/eda/train.combined.label.{type_} \\\n","                    --num_aug=4\"\n","        execute_shell_command(command)"],"metadata":{"id":"Ow_3eQreC_o0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","with tf.device('/device:GPU:0'):\n","  !for GENRE in fiction; do \\\n","    for SPLIT in train; do \\\n","      for TYPE in input0 input1; do \\\n","        python /content/fairseq/examples/roberta/multiprocessing_bpe_encoder.py \\\n","            --encoder-json gpt2_bpe/encoder.json \\\n","            --vocab-bpe gpt2_bpe/vocab.bpe \\\n","            --inputs /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/eda/eda_${SPLIT}.combined.label.${TYPE} \\\n","            --outputs /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/eda/eda_${SPLIT}.combined.label.bpe.${TYPE} \\\n","            --keep-empty \\\n","            --workers 60; \\\n","      done \\\n","    done \\\n","  done"],"metadata":{"id":"F_7PkPZdDK-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","with tf.device('/device:GPU:0'):\n","  !for GENRE in fiction; do \\\n","    for SPLIT in valid test; do \\\n","      for TYPE in input0 input1; do \\\n","        python /content/fairseq/examples/roberta/multiprocessing_bpe_encoder.py \\\n","            --encoder-json gpt2_bpe/encoder.json \\\n","            --vocab-bpe gpt2_bpe/vocab.bpe \\\n","            --inputs /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/${SPLIT}.raw.${TYPE} \\\n","            --outputs /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/${SPLIT}.bpe.${TYPE} \\\n","            --keep-empty \\\n","            --workers 60; \\\n","      done \\\n","    done \\\n","  done"],"metadata":{"id":"9bmgvuFmDSbV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with tf.device('/device:GPU:0'):\n","    !for GENRE in fiction; do \\\n","      for TYPE in input0 input1; do \\\n","        fairseq-preprocess \\\n","        --only-source \\\n","        --srcdict gpt2_bpe/dict.txt \\\n","        --trainpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/eda/eda_train.combined.label.bpe.${TYPE}  \\\n","        --validpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/valid.bpe.${TYPE} \\\n","        --testpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/test.bpe.${TYPE} \\\n","        --destdir /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/bin_eda2/${TYPE} \\\n","        --workers 60;\\\n","      done \\\n","    done\n"],"metadata":{"id":"t63j2h-ODWe4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  # for the two input scentences\n","with tf.device('/device:GPU:0'):\n","    !for GENRE in fiction; do \\\n","      for TYPE in label; do \\\n","        fairseq-preprocess \\\n","        --only-source \\\n","        --trainpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/eda/eda_label_train.combined.${TYPE}.input0  \\\n","        --validpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/valid.raw.${TYPE} \\\n","        --testpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/test.raw.${TYPE} \\\n","        --destdir /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/bin_eda2/${TYPE} \\\n","        --workers 60;\\\n","      done \\\n","    done"],"metadata":{"id":"aAUcC0xyDuJy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# UDA\n","\n","Target: \\bin_uda"],"metadata":{"id":"GBlDpfNIf9uf"}},{"cell_type":"code","source":["! pip install transformers\n","! pip install sentencepiece"],"metadata":{"id":"SDUYKgTK6K25"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import MarianMTModel, MarianTokenizer\n","\n","torch.cuda.empty_cache()\n","\n","en_fr_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n","en_fr_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n","\n","fr_en_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")\n","fr_en_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")"],"metadata":{"id":"-CrtLT2L6Lgx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Code for translating input 0\n","input = open('/content/drive/MyDrive/NLP/mnli/fiction/orig/trainr2.raw.input0', 'r')\n","src_text = input.read().splitlines()\n","\n","output = open('/content/drive/MyDrive/NLP/mnli/fiction/orig/translate/trans_train.raw.input0', 'w')\n","\n","for line in src_text:\n","  translated_tokens = en_fr_model.generate(**en_fr_tokenizer(line, return_tensors=\"pt\", padding=True), num_return_sequences = 2, top_k = 10, temperature = 2.0)\n","  en_fr = [en_fr_tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n","\n","  translated_tokens2 = fr_en_model.generate(**fr_en_tokenizer(en_fr, return_tensors=\"pt\", padding=True), num_return_sequences = 2, top_k = 10, temperature = 2.0)\n","  fr_en = [fr_en_tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens2]\n","  output.write(line.rstrip()+\"\\n\")\n","\n","  for r in fr_en:\n","    output.write(r.rstrip()+\"\\n\")\n","    output.flush()\n","\n","input.close()\n","output.close()"],"metadata":{"id":"ZC5mHQAi6Nod"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Code for translating input 1\n","input = open('/content/drive/MyDrive/NLP/mnli/fiction/orig/trainr2.raw.input1', 'r')\n","src_text = input.read().splitlines()\n","\n","output = open('/content/drive/MyDrive/NLP/mnli/fiction/orig/translate/trans_train.raw.input1', 'w')\n","\n","for line in src_text:\n","  translated_tokens = en_fr_model.generate(**en_fr_tokenizer(line, return_tensors=\"pt\", padding=True), num_return_sequences = 2, top_k = 10, temperature = 2.0)\n","  en_fr = [en_fr_tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n","\n","  translated_tokens2 = fr_en_model.generate(**fr_en_tokenizer(en_fr, return_tensors=\"pt\", padding=True), num_return_sequences = 2, top_k = 10, temperature = 2.0)\n","  fr_en = [fr_en_tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens2]\n","  output.write(line.rstrip()+\"\\n\")\n","\n","  for r in fr_en:\n","    output.write(r.rstrip()+\"\\n\")\n","    output.flush()\n","\n","input.close()\n","output.close()"],"metadata":{"id":"37S8c3xL6vUL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create label file\n","f = open('/content/drive/MyDrive/NLP/mnli/fiction/orig/trainr.raw.label', 'r')\n","o = open('/content/drive/MyDrive/NLP/mnli/fiction/orig/translate/trans_train.raw.label', 'w')\n","\n","lines = f.read().splitlines()\n","print(len(lines))\n","for i in lines:\n","  for j in range (5):\n","    o.write(i.strip()+\"\\n\")\n","\n","\n","o.close()"],"metadata":{"id":"NrK31ODej2Dm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","with tf.device('/device:GPU:0'):\n","  !for GENRE in fiction; do \\\n","    for SPLIT in train; do \\\n","      for TYPE in input0 input1; do \\\n","        python /content/fairseq/examples/roberta/multiprocessing_bpe_encoder.py \\\n","            --encoder-json gpt2_bpe/encoder.json \\\n","            --vocab-bpe gpt2_bpe/vocab.bpe \\\n","            --inputs /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/translate/trans_${SPLIT}.raw.${TYPE} \\\n","            --outputs /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/translate/trans_${SPLIT}.raw.bpe.${TYPE} \\\n","            --keep-empty \\\n","            --workers 60; \\\n","      done \\\n","    done \\\n","  done"],"metadata":{"id":"yPx69RfmmAEN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  # for the two input scentences\n","with tf.device('/device:GPU:0'):\n","    !for GENRE in fiction; do \\\n","      for TYPE in input0 input1; do \\\n","        fairseq-preprocess \\\n","        --only-source \\\n","        --srcdict gpt2_bpe/dict.txt \\\n","        --trainpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/translate/trans_train.raw.bpe.${TYPE}  \\\n","        --validpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/valid.bpe.${TYPE} \\\n","        --testpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/test.bpe.${TYPE} \\\n","        --destdir /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/bin_uda/${TYPE} \\\n","        --workers 60;\\\n","      done \\\n","    done"],"metadata":{"id":"aYo-yPnvmOkE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  # for the labels\n","with tf.device('/device:GPU:0'):\n","    !for GENRE in fiction; do \\\n","      for TYPE in label; do \\\n","        fairseq-preprocess \\\n","        --only-source \\\n","        --trainpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/translate/trans_train.raw.${TYPE}  \\\n","        --validpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/valid.raw.${TYPE} \\\n","        --testpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/test.raw.${TYPE} \\\n","        --destdir /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/bin_uda/${TYPE} \\\n","        --workers 60;\\\n","      done \\\n","    done"],"metadata":{"id":"4qqoPPo30SOE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# SSMBA\n","\n","Target: \\bin_ssmba"],"metadata":{"id":"cxYFt-U_f-n3"}},{"cell_type":"code","source":["! git clone https://github.com/nng555/ssmba.git /content/drive/MyDrive/ssmba\n","! pip install -r /content/drive/MyDrive/ssmba/requirements.txt"],"metadata":{"id":"Su6kWEwL7UVt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import subprocess\n","import tensorflow as tf\n","\n","def execute_shell_command(command):\n","    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)\n","    stdout, stderr = process.communicate()\n","    return process.returncode, stdout, stderr\n","\n","with tf.device('/device:GPU:0'):\n","    for genre in ['fiction']:\n","        for split in ['trainr']:\n","            for type_ in ['input0 input1']:\n","                command = [\n","                    'python',\n","                    '/content/drive/MyDrive/ssmba/ssmba.py',\n","                    '--model', 'bert-base-uncased',\n","                    '--in-file', f'/content/drive/MyDrive/{genre}/orig/{split}.raw.{type_}',\n","                    '--label-file', f'/content/drive/MyDrive/{genre}/orig/{split}.raw.label',\n","                    '--output-prefix', f'ssmba_out',\n","                    '--noise-prob', '0.05',\n","                    '--num-samples', '5',\n","                ]\n","                returncode, stdout, stderr = execute_shell_command(command)\n","                if returncode != 0:\n","                    print(f\"Error running the command for {genre}/{split}/{type_}:\")\n","                    print(stderr)\n","                else:\n","                    print(f\"Command executed successfully for {genre}/{split}/{type_}:\")\n","                    print(stdout)"],"metadata":{"id":"sAbTIwhY7dz_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f = open('/content/drive/MyDrive/NLP/mnli/fiction/orig/ssmba/ssmba_out', 'r')\n","o1 = open('/content/drive/MyDrive/NLP/mnli/fiction/orig/ssmba/ssmba_train.raw.input0', 'w')\n","o2 = open('/content/drive/MyDrive/NLP/mnli/fiction/orig/ssmba/ssmba_train.raw.input1', 'w')\n","\n","lines = [tuple(s.strip().split('\\t')) for s in f.readlines()]\n","\n","for i in range (len(lines)):\n","  o1.write(lines[i][0].strip()+\"\\n\")\n","  o2.write(lines[i][1].strip()+\"\\n\")\n","\n","\n","o1.close()\n","o2.close()"],"metadata":{"id":"rZnE3kKpgDtK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","with tf.device('/device:GPU:0'):\n","  !for GENRE in fiction; do \\\n","    for SPLIT in train; do \\\n","      for TYPE in input0 input1; do \\\n","        python /content/fairseq/examples/roberta/multiprocessing_bpe_encoder.py \\\n","            --encoder-json gpt2_bpe/encoder.json \\\n","            --vocab-bpe gpt2_bpe/vocab.bpe \\\n","            --inputs /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/ssmba/ssmba_${SPLIT}.raw.${TYPE} \\\n","            --outputs /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/ssmba/ssmba_${SPLIT}.raw.bpe.${TYPE} \\\n","            --keep-empty \\\n","            --workers 60; \\\n","      done \\\n","    done \\\n","  done"],"metadata":{"id":"q_yw7aK-gMZG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  # for the two input scentences\n","with tf.device('/device:GPU:0'):\n","    !for GENRE in fiction; do \\\n","      for TYPE in input0 input1; do \\\n","        fairseq-preprocess \\\n","        --only-source \\\n","        --srcdict gpt2_bpe/dict.txt \\\n","        --trainpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/ssmba/ssmba_train.raw.bpe.${TYPE}  \\\n","        --validpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/valid.bpe.${TYPE} \\\n","        --testpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/test.bpe.${TYPE} \\\n","        --destdir /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/bin_ssmba2/${TYPE} \\\n","        --workers 60;\\\n","      done \\\n","    done"],"metadata":{"id":"uuIv4L4dgnNL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  # for the labels\n","with tf.device('/device:GPU:0'):\n","    !for GENRE in fiction; do \\\n","      for TYPE in label; do \\\n","        fairseq-preprocess \\\n","        --only-source \\\n","        --trainpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/ssmba/ssmba_out2.${TYPE}  \\\n","        --validpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/valid.raw.${TYPE} \\\n","        --testpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/test.raw.${TYPE} \\\n","        --destdir /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/bin_ssmba2/${TYPE} \\\n","        --workers 60;\\\n","      done \\\n","    done"],"metadata":{"id":"2EegsLVbhBQy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# EDA_Extended"],"metadata":{"id":"8ljM6NXMhXpC"}},{"cell_type":"code","source":["import nltk; nltk.download('wordnet')"],"metadata":{"id":"IsG2GIqwhXDP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install python-utils"],"metadata":{"id":"r1jP0-1zlwhQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# First Experiment\n","import subprocess\n","\n","def execute_shell_command(command):\n","    process = subprocess.Popen(command, shell=True)\n","    process.wait()\n","\n","command = \"\"\"\n","for GENRE in fiction; do \\\n","    for TYPE in input0 input1; do \\\n","        paste -d \"\\t\" /content/drive/MyDrive/NLP/mnli/$GENRE/orig/trainr.raw.label /content/drive/MyDrive/NLP/mnli/$GENRE/orig/trainr.raw.$TYPE > /content/drive/MyDrive/NLP/mnli/$GENRE/orig/eda_extended/train.combined.label.$TYPE; \\\n","    done \\\n","done\n","\"\"\"\n","\n","execute_shell_command(command)"],"metadata":{"id":"PZksOza5hdzX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# First Experiment\n","!for GENRE in fiction; do \\\n","  for TYPE in input0 input1; do \\\n","    python /content/drive/MyDrive/NLP/eda_extended/augment.py \\\n","      --input /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/eda_extended/train.combined.label.${TYPE} \\\n","      --num_aug=4 \\\n","      --alpha_sr=0.1 \\\n","      --alpha_ri=0.1 \\\n","      --alpha_rs=0.1 \\\n","      --alpha_rd=0.1; \\\n","    done \\\n","  done"],"metadata":{"id":"ZeyGkPmIh6aI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# First Experiment\n","import tensorflow as tf\n","with tf.device('/device:GPU:0'):\n","  !for GENRE in fiction; do \\\n","    for SPLIT in train; do \\\n","      for TYPE in input0 input1; do \\\n","        python /content/fairseq/examples/roberta/multiprocessing_bpe_encoder.py \\\n","            --encoder-json gpt2_bpe/encoder.json \\\n","            --vocab-bpe gpt2_bpe/vocab.bpe \\\n","            --inputs /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/eda_extended/eda_extended_${SPLIT}.combined.label.${TYPE} \\\n","            --outputs /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/eda_extended/eda_extended_${SPLIT}.combined.label.bpe.${TYPE} \\\n","            --keep-empty \\\n","            --workers 60; \\\n","      done \\\n","    done \\\n","  done"],"metadata":{"id":"o9TpCPKH4Zkm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with tf.device('/device:GPU:0'):\n","    !for GENRE in fiction; do \\\n","      for TYPE in input0 input1; do \\\n","        fairseq-preprocess \\\n","        --only-source \\\n","        --srcdict gpt2_bpe/dict.txt \\\n","        --trainpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/eda_extended/eda_extended_train.combined.label.bpe.${TYPE}  \\\n","        --validpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/valid.bpe.${TYPE} \\\n","        --testpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/test.bpe.${TYPE} \\\n","        --destdir /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/bin_eda_extended/${TYPE} \\\n","        --workers 60;\\\n","      done \\\n","    done\n"],"metadata":{"id":"A9i-7Fcy4rYj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  # for the two input scentences\n","with tf.device('/device:GPU:0'):\n","    !for GENRE in fiction; do \\\n","      for TYPE in label; do \\\n","        fairseq-preprocess \\\n","        --only-source \\\n","        --trainpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/eda_extended/eda_label_extended_train.combined.${TYPE}.input0  \\\n","        --validpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/valid.raw.${TYPE} \\\n","        --testpref /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/test.raw.${TYPE} \\\n","        --destdir /content/drive/MyDrive/NLP/mnli/${GENRE}/orig/bin_eda_extended/${TYPE} \\\n","        --workers 60;\\\n","      done \\\n","    done"],"metadata":{"id":"w-fHbwjf4tez"},"execution_count":null,"outputs":[]}]}