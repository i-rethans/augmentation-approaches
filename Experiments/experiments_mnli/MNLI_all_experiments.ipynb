{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1Xwvtd4J1Ux0"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_hsKVWC1xJO"},"outputs":[],"source":["! pip install tensorboardX\n","! pip install tensorrt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a0W7l7Ch12DN"},"outputs":[],"source":["!pip install cython -U\n","!git clone https://github.com/pytorch/fairseq.git\n","%cd fairseq\n","!pip install --quiet --editable .\n","!pip install --quiet sentencepiece"]},{"cell_type":"markdown","source":["# Baseline\n","\n","Result:"],"metadata":{"id":"5XT3qol9iQxe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FrEEP2aJ2XiB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686306581018,"user_tz":-120,"elapsed":559527,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}},"outputId":"1511b509-2577-46ea-f568-cfbc7f32bfb2"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-09 10:20:35.944380: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2023-06-09 10:20:37 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n","2023-06-09 10:20:44 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 100, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4400, 'batch_size': 32, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 123873, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/drive/MyDrive/NLP/checkpoints/fiction/baseline', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=100, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=1.0, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4400, batch_size=32, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4400, batch_size_valid=32, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta', max_epoch=5, max_update=123873, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/content/drive/MyDrive/NLP/checkpoints/fiction/baseline', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='/content/drive/MyDrive/NLP/mnli/fiction/orig/bin', num_classes=3, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, d2v2_multi=False, classification_head_name='mnli', regression_target=False, report_mcc=False, report_acc_and_f1=False, report_pearson_and_spearman=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.1, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, max_positions=512, dropout=0.1, attention_dropout=0.1, no_seed_provided=False, encoder_layers=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_attention_heads=12, activation_dropout=0.0, pooler_dropout=0.0, max_source_positions=512, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta'), 'task': {'_name': 'sentence_prediction', 'data': '/content/drive/MyDrive/NLP/mnli/fiction/orig/bin', 'num_classes': 3, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 512, 'regression_target': False, 'classification_head_name': 'mnli', 'seed': 100, 'd2v2_multi': False}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'mnli', 'regression_target': False, 'report_mcc': False, 'report_acc_and_f1': False, 'report_pearson_and_spearman': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2023-06-09 10:20:44 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n","2023-06-09 10:20:45 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n","2023-06-09 10:20:48 | INFO | fairseq_cli.train | RobertaModel(\n","  (encoder): RobertaEncoder(\n","    (sentence_encoder): TransformerEncoder(\n","      (dropout_module): FairseqDropout()\n","      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","      (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (layers): ModuleList(\n","        (0-11): 12 x TransformerEncoderLayerBase(\n","          (self_attn): MultiheadAttention(\n","            (dropout_module): FairseqDropout()\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout_module): FairseqDropout()\n","          (activation_dropout_module): FairseqDropout()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (lm_head): RobertaLMHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (classification_heads): ModuleDict(\n","    (mnli): RobertaClassificationHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","      (out_proj): Linear(in_features=768, out_features=3, bias=True)\n","    )\n","  )\n",")\n","2023-06-09 10:20:48 | INFO | fairseq_cli.train | task: SentencePredictionTask\n","2023-06-09 10:20:48 | INFO | fairseq_cli.train | model: RobertaModel\n","2023-06-09 10:20:48 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n","2023-06-09 10:20:48 | INFO | fairseq_cli.train | num. shared model params: 125,289,564 (num. trained: 125,289,564)\n","2023-06-09 10:20:48 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n","2023-06-09 10:20:49 | INFO | fairseq.data.data_utils | loaded 1,973 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin/input0/valid\n","2023-06-09 10:20:50 | INFO | fairseq.data.data_utils | loaded 1,973 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin/input1/valid\n","2023-06-09 10:20:51 | INFO | fairseq.data.data_utils | loaded 1,973 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin/label/valid\n","2023-06-09 10:20:51 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 1973\n","2023-06-09 10:20:57 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n","2023-06-09 10:20:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-09 10:20:57 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n","2023-06-09 10:20:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-09 10:20:57 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2023-06-09 10:20:57 | INFO | fairseq_cli.train | max tokens per device = 4400 and max sentences per device = 32\n","2023-06-09 10:20:57 | INFO | fairseq.trainer | Preparing to load checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/baseline/checkpoint_last.pt\n","2023-06-09 10:20:57 | INFO | fairseq.trainer | No existing checkpoint found /content/drive/MyDrive/NLP/checkpoints/fiction/baseline/checkpoint_last.pt\n","2023-06-09 10:20:57 | INFO | fairseq.trainer | loading train data for epoch 1\n","2023-06-09 10:20:58 | INFO | fairseq.data.data_utils | loaded 10,000 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin/input0/train\n","2023-06-09 10:20:58 | INFO | fairseq.data.data_utils | loaded 10,000 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin/input1/train\n","2023-06-09 10:20:59 | INFO | fairseq.data.data_utils | loaded 10,000 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin/label/train\n","2023-06-09 10:20:59 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 10000\n","2023-06-09 10:20:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 10:20:59 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-09 10:20:59 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-09 10:20:59 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-09 10:20:59 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n","2023-06-09 10:20:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 10:20:59 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-09 10:20:59 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-09 10:20:59 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-09 10:21:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 313\n","epoch 001:   0% 0/313 [00:00<?, ?it/s]2023-06-09 10:21:00 | INFO | fairseq.trainer | begin training epoch 1\n","2023-06-09 10:21:00 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 001: 100% 312/313 [00:51<00:00,  5.18it/s, loss=1.593, nll_loss=0.053, accuracy=33.6, wps=6110.4, ups=6.41, wpb=953, bsz=32, num_updates=300, lr=7.5e-07, gnorm=4.808, loss_scale=16, train_wall=15, gb_free=11.9, wall=52]2023-06-09 10:21:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 10:21:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   3% 2/62 [00:00<00:04, 14.26it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   6% 4/62 [00:00<00:03, 17.05it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  10% 6/62 [00:00<00:03, 15.72it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  13% 8/62 [00:00<00:03, 15.57it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  16% 10/62 [00:00<00:03, 16.61it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  21% 13/62 [00:00<00:02, 18.73it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  26% 16/62 [00:00<00:02, 19.50it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  29% 18/62 [00:00<00:02, 19.53it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  34% 21/62 [00:01<00:01, 20.65it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  39% 24/62 [00:01<00:01, 21.73it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  44% 27/62 [00:01<00:01, 21.33it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  48% 30/62 [00:01<00:01, 21.44it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  53% 33/62 [00:01<00:01, 21.56it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  58% 36/62 [00:01<00:01, 21.19it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  63% 39/62 [00:01<00:01, 21.09it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  68% 42/62 [00:02<00:00, 21.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  73% 45/62 [00:02<00:00, 21.20it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  77% 48/62 [00:02<00:00, 21.71it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  82% 51/62 [00:02<00:00, 23.44it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  89% 55/62 [00:02<00:00, 26.19it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  94% 58/62 [00:02<00:00, 26.46it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  98% 61/62 [00:02<00:00, 25.81it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 10:21:55 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.579 | nll_loss 0.054 | accuracy 34.3 | wps 20342.6 | wpb 931.1 | bsz 31.8 | num_updates 313\n","2023-06-09 10:21:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 313 updates\n","2023-06-09 10:21:55 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/baseline/checkpoint_best.pt\n","2023-06-09 10:22:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/baseline/checkpoint_best.pt\n","2023-06-09 10:22:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/baseline/checkpoint_best.pt (epoch 1 @ 313 updates, score 34.3) (writing took 29.28112387100009 seconds)\n","2023-06-09 10:22:24 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2023-06-09 10:22:24 | INFO | train | epoch 001 | loss 1.595 | nll_loss 0.053 | accuracy 33.4 | wps 3698.5 | ups 3.88 | wpb 954.4 | bsz 31.9 | num_updates 313 | lr 7.825e-07 | gnorm 4.918 | loss_scale 16 | train_wall 50 | gb_free 11.6 | wall 87\n","2023-06-09 10:22:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 10:22:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 313\n","epoch 002:   0% 0/313 [00:00<?, ?it/s]2023-06-09 10:22:24 | INFO | fairseq.trainer | begin training epoch 2\n","2023-06-09 10:22:24 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 002: 100% 312/313 [00:55<00:00,  6.77it/s, loss=1.591, nll_loss=0.053, accuracy=34.9, wps=5984.3, ups=6.3, wpb=949.8, bsz=31.9, num_updates=600, lr=1.5e-06, gnorm=4.507, loss_scale=64, train_wall=15, gb_free=11.8, wall=138]2023-06-09 10:23:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 10:23:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 17.33it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   8% 5/62 [00:00<00:02, 20.48it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  13% 8/62 [00:00<00:02, 21.77it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  18% 11/62 [00:00<00:02, 22.41it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  23% 14/62 [00:00<00:02, 23.73it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  27% 17/62 [00:00<00:01, 24.40it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  32% 20/62 [00:00<00:01, 25.25it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  37% 23/62 [00:00<00:01, 25.96it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  42% 26/62 [00:01<00:01, 26.96it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  47% 29/62 [00:01<00:01, 25.68it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  52% 32/62 [00:01<00:01, 26.18it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  56% 35/62 [00:01<00:01, 26.28it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  61% 38/62 [00:01<00:00, 26.20it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  66% 41/62 [00:01<00:00, 25.89it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  71% 44/62 [00:01<00:00, 26.25it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  76% 47/62 [00:01<00:00, 27.17it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  81% 50/62 [00:01<00:00, 27.47it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  87% 54/62 [00:02<00:00, 30.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  94% 58/62 [00:02<00:00, 29.25it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  98% 61/62 [00:02<00:00, 28.42it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 10:23:22 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 1.581 | nll_loss 0.054 | accuracy 36.5 | wps 24609.4 | wpb 931.1 | bsz 31.8 | num_updates 626 | best_accuracy 36.5\n","2023-06-09 10:23:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 626 updates\n","2023-06-09 10:23:22 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/baseline/checkpoint_best.pt\n","2023-06-09 10:23:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/baseline/checkpoint_best.pt\n","2023-06-09 10:24:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/baseline/checkpoint_best.pt (epoch 2 @ 626 updates, score 36.5) (writing took 72.27001611399987 seconds)\n","2023-06-09 10:24:34 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2023-06-09 10:24:34 | INFO | train | epoch 002 | loss 1.589 | nll_loss 0.053 | accuracy 35.3 | wps 2294 | ups 2.4 | wpb 954.4 | bsz 31.9 | num_updates 626 | lr 1.565e-06 | gnorm 4.727 | loss_scale 64 | train_wall 53 | gb_free 11.1 | wall 217\n","2023-06-09 10:24:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 10:24:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 313\n","epoch 003:   0% 0/313 [00:00<?, ?it/s]2023-06-09 10:24:34 | INFO | fairseq.trainer | begin training epoch 3\n","2023-06-09 10:24:34 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 003: 100% 312/313 [01:01<00:00,  6.62it/s, loss=1.583, nll_loss=0.053, accuracy=36.3, wps=4603.2, ups=4.85, wpb=948.9, bsz=31.9, num_updates=900, lr=2.25e-06, gnorm=4.155, loss_scale=512, train_wall=20, gb_free=11.7, wall=272]2023-06-09 10:25:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 10:25:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 18.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   8% 5/62 [00:00<00:02, 20.63it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  13% 8/62 [00:00<00:02, 22.17it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  18% 11/62 [00:00<00:02, 23.11it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  23% 14/62 [00:00<00:01, 24.31it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  27% 17/62 [00:00<00:01, 24.76it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  32% 20/62 [00:00<00:01, 25.65it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  37% 23/62 [00:01<00:01, 20.60it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  42% 26/62 [00:01<00:01, 21.72it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  47% 29/62 [00:01<00:01, 22.19it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  52% 32/62 [00:01<00:01, 23.77it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  56% 35/62 [00:01<00:01, 24.59it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  61% 38/62 [00:01<00:00, 25.05it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  66% 41/62 [00:01<00:00, 24.24it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  71% 44/62 [00:01<00:00, 22.27it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  76% 47/62 [00:02<00:00, 19.04it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  82% 51/62 [00:02<00:00, 22.21it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  89% 55/62 [00:02<00:00, 25.03it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  94% 58/62 [00:02<00:00, 25.68it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  98% 61/62 [00:02<00:00, 26.12it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 10:25:39 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 1.589 | nll_loss 0.054 | accuracy 35.5 | wps 22123.7 | wpb 931.1 | bsz 31.8 | num_updates 939 | best_accuracy 36.5\n","2023-06-09 10:25:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 939 updates\n","2023-06-09 10:25:39 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/baseline/checkpoint_last.pt\n","2023-06-09 10:25:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/baseline/checkpoint_last.pt\n","2023-06-09 10:25:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/baseline/checkpoint_last.pt (epoch 3 @ 939 updates, score 35.5) (writing took 12.499202232000016 seconds)\n","2023-06-09 10:25:51 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2023-06-09 10:25:51 | INFO | train | epoch 003 | loss 1.582 | nll_loss 0.053 | accuracy 36.3 | wps 3871.3 | ups 4.06 | wpb 954.4 | bsz 31.9 | num_updates 939 | lr 2.3475e-06 | gnorm 4.372 | loss_scale 512 | train_wall 59 | gb_free 11.8 | wall 294\n","2023-06-09 10:25:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 10:25:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 313\n","epoch 004:   0% 0/313 [00:00<?, ?it/s]2023-06-09 10:25:51 | INFO | fairseq.trainer | begin training epoch 4\n","2023-06-09 10:25:51 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 004: 100% 312/313 [01:03<00:00,  6.64it/s, loss=1.575, nll_loss=0.053, accuracy=37.5, wps=5341.6, ups=5.61, wpb=952.3, bsz=32, num_updates=1200, lr=3e-06, gnorm=4.05, loss_scale=2048, train_wall=17, gb_free=11.9, wall=348]2023-06-09 10:26:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 10:26:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 16.98it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   8% 5/62 [00:00<00:02, 19.71it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  13% 8/62 [00:00<00:02, 21.81it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  18% 11/62 [00:00<00:02, 22.60it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  23% 14/62 [00:00<00:01, 24.30it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  27% 17/62 [00:00<00:01, 25.30it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  32% 20/62 [00:00<00:01, 25.94it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  37% 23/62 [00:00<00:01, 26.10it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  42% 26/62 [00:01<00:01, 26.73it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  47% 29/62 [00:01<00:01, 26.16it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  52% 32/62 [00:01<00:01, 26.70it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  56% 35/62 [00:01<00:01, 26.73it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  61% 38/62 [00:01<00:00, 26.66it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  66% 41/62 [00:01<00:00, 26.04it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  71% 44/62 [00:01<00:00, 26.57it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  77% 48/62 [00:01<00:00, 27.59it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  84% 52/62 [00:01<00:00, 29.40it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  90% 56/62 [00:02<00:00, 29.87it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  95% 59/62 [00:02<00:00, 29.77it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset: 100% 62/62 [00:02<00:00, 29.68it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 10:26:57 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 1.57 | nll_loss 0.054 | accuracy 37.3 | wps 24953.4 | wpb 931.1 | bsz 31.8 | num_updates 1252 | best_accuracy 37.3\n","2023-06-09 10:26:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1252 updates\n","2023-06-09 10:26:57 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/baseline/checkpoint_best.pt\n","2023-06-09 10:27:05 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/baseline/checkpoint_best.pt\n","2023-06-09 10:27:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/baseline/checkpoint_best.pt (epoch 4 @ 1252 updates, score 37.3) (writing took 49.612413469999865 seconds)\n","2023-06-09 10:27:46 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2023-06-09 10:27:46 | INFO | train | epoch 004 | loss 1.579 | nll_loss 0.053 | accuracy 36.5 | wps 2593.7 | ups 2.72 | wpb 954.4 | bsz 31.9 | num_updates 1252 | lr 3.13e-06 | gnorm 4.178 | loss_scale 2048 | train_wall 61 | gb_free 11.6 | wall 409\n","2023-06-09 10:27:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 10:27:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 313\n","epoch 005:   0% 0/313 [00:00<?, ?it/s]2023-06-09 10:27:46 | INFO | fairseq.trainer | begin training epoch 5\n","2023-06-09 10:27:46 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 005:  74% 233/313 [00:46<00:12,  6.60it/s, loss=1.571, nll_loss=0.053, accuracy=38.3, wps=5182.3, ups=5.47, wpb=946.6, bsz=32, num_updates=1400, lr=3.5e-06, gnorm=4.087, loss_scale=4096, train_wall=18, gb_free=11.9, wall=439]2023-06-09 10:28:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005: 100% 312/313 [00:59<00:00,  5.93it/s, loss=1.568, nll_loss=0.052, accuracy=38.8, wps=5049.9, ups=5.25, wpb=961.8, bsz=31.9, num_updates=1500, lr=3.75e-06, gnorm=3.929, loss_scale=4096, train_wall=18, gb_free=11.8, wall=458]2023-06-09 10:28:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 10:28:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 005 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 19.17it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   6% 4/62 [00:00<00:02, 19.52it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  11% 7/62 [00:00<00:02, 21.71it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  16% 10/62 [00:00<00:02, 22.39it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  23% 14/62 [00:00<00:02, 22.76it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  27% 17/62 [00:00<00:02, 21.08it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  32% 20/62 [00:00<00:01, 22.85it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  39% 24/62 [00:01<00:01, 25.27it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  44% 27/62 [00:01<00:01, 25.39it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  48% 30/62 [00:01<00:01, 25.90it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  53% 33/62 [00:01<00:01, 26.19it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  58% 36/62 [00:01<00:01, 25.96it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  63% 39/62 [00:01<00:00, 25.70it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  68% 42/62 [00:01<00:00, 26.04it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  73% 45/62 [00:01<00:00, 26.60it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  77% 48/62 [00:01<00:00, 27.20it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  84% 52/62 [00:02<00:00, 29.21it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  90% 56/62 [00:02<00:00, 30.34it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  97% 60/62 [00:02<00:00, 28.98it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 10:28:48 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 1.566 | nll_loss 0.054 | accuracy 38.6 | wps 24293.7 | wpb 931.1 | bsz 31.8 | num_updates 1564 | best_accuracy 38.6\n","2023-06-09 10:28:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1564 updates\n","2023-06-09 10:28:48 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/baseline/checkpoint_best.pt\n","2023-06-09 10:28:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/baseline/checkpoint_best.pt\n","2023-06-09 10:29:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/baseline/checkpoint_best.pt (epoch 5 @ 1564 updates, score 38.6) (writing took 34.448884575999955 seconds)\n","2023-06-09 10:29:23 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n","2023-06-09 10:29:23 | INFO | train | epoch 005 | loss 1.568 | nll_loss 0.053 | accuracy 38.8 | wps 3096.5 | ups 3.25 | wpb 954.1 | bsz 32 | num_updates 1564 | lr 3.91e-06 | gnorm 4.004 | loss_scale 4096 | train_wall 57 | gb_free 11.9 | wall 505\n","2023-06-09 10:29:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 10:29:23 | INFO | fairseq_cli.train | done training in 502.7 seconds\n"]}],"source":["import tensorflow as tf\n","with tf.device('/device:GPU:0'):\n","    ! CUDA_VISIBLE_DEVICES=0 fairseq-train /content/drive/MyDrive/NLP/mnli/fiction/orig/bin \\\n","          --save-dir /content/drive/MyDrive/NLP/checkpoints/fiction/baseline \\\n","          --reset-optimizer  \\\n","          --reset-dataloader  \\\n","          --reset-meters  \\\n","          --best-checkpoint-metric accuracy  \\\n","          --maximize-best-checkpoint-metric  \\\n","          --no-epoch-checkpoints \\\n","          --find-unused-parameters \\\n","          --distributed-world-size 1 \\\n","          --task sentence_prediction  \\\n","          --num-classes 3  \\\n","          --init-token 0  \\\n","          --separator-token 2   \\\n","          --max-positions 512  \\\n","          --shorten-method \"truncate\"  \\\n","          --arch roberta \\\n","          --dropout 0.1  \\\n","          --attention-dropout 0.1  \\\n","          --weight-decay 0.1  \\\n","          --criterion sentence_prediction  \\\n","          --classification-head-name 'mnli' \\\n","          --optimizer adam  \\\n","          --adam-betas '(0.9, 0.98)'  \\\n","          --adam-eps 1e-06  \\\n","          --clip-norm 0.0  \\\n","          --lr-scheduler inverse_sqrt  \\\n","          --lr 1e-05 \\\n","          --fp16  \\\n","          --fp16-init-scale 4  \\\n","          --threshold-loss-scale 1  \\\n","          --fp16-scale-window 128  \\\n","          --batch-size 32  \\\n","          --required-batch-size-multiple 1  \\\n","          --max-tokens 4400 \\\n","          --update-freq 1  \\\n","          --max-update 123873 \\\n","          --max-epoch 5 \\\n","          --seed 100"]},{"cell_type":"code","source":["from fairseq.models.roberta import RobertaModel\n","roberta = RobertaModel.from_pretrained(\n","    '/content/drive/MyDrive/NLP/checkpoints/fiction/baseline',\n","    checkpoint_file='checkpoint_best.pt',\n","    data_name_or_path='/content/drive/MyDrive/NLP/mnli/fiction/orig/bin'\n",")\n","roberta.eval()"],"metadata":{"id":"jhot-cBksB6O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["genre = ['slate','fiction','telephone','travel','government']\n","for g in genre:\n","  input0 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input0', \"r\")\n","  input1 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input1', \"r\")\n","  label = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.label', \"r\")\n","\n","  accuracy = 0\n","  total = 0\n","  for (x1, x2, y) in zip(input0, input1, label):\n","    tokens = roberta.encode(x1, x2)\n","    idx = roberta.predict('mnli', tokens).argmax().item()\n","    dictionary = roberta.task.label_dictionary\n","    pred = dictionary[idx + dictionary.nspecial]\n","    total = total + 1\n","    if  (pred == y.strip()) :\n","      accuracy = accuracy + 1\n","\n","\n","  print(g,\": \",accuracy)\n","  print(g,\": \",total)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QZErFEQ7iyOO","executionInfo":{"status":"ok","timestamp":1686313716184,"user_tz":-120,"elapsed":1473443,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}},"outputId":"2b8180ba-e45c-4c9e-a4ae-1947c8e1e3ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["slate :  710\n","slate :  2000\n","fiction :  734\n","fiction :  2000\n","telephone :  688\n","telephone :  2000\n","travel :  704\n","travel :  2000\n","government :  741\n","government :  2000\n"]}]},{"cell_type":"code","source":["genre = ['verbatim','facetoface','oup','nineeleven','letters']\n","for g in genre:\n","  input0 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input0', \"r\")\n","  input1 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input1', \"r\")\n","  label = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.label', \"r\")\n","\n","  accuracy = 0\n","  total = 0\n","  for (x1, x2, y) in zip(input0, input1, label):\n","    tokens = roberta.encode(x1, x2)\n","    idx = roberta.predict('mnli', tokens).argmax().item()\n","    dictionary = roberta.task.label_dictionary\n","    pred = dictionary[idx + dictionary.nspecial]\n","    total = total + 1\n","    if  (pred == y.strip()) :\n","      accuracy = accuracy + 1\n","\n","  print(g,\": \",accuracy)\n","  print(g,\": \",total)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VuTObXNYp-h0","executionInfo":{"status":"ok","timestamp":1686491246892,"user_tz":-120,"elapsed":932505,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}},"outputId":"52c17e9f-0d5c-46b0-ac28-75647d0b6e23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["verbatim :  359\n","verbatim :  1000\n","facetoface :  369\n","facetoface :  1000\n","oup :  360\n","oup :  1000\n","nineeleven :  365\n","nineeleven :  1000\n","letters :  367\n","letters :  1000\n"]}]},{"cell_type":"markdown","source":["# EDA"],"metadata":{"id":"NtMgDNa8iy5G"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Sk4mGhs24wK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686308138827,"user_tz":-120,"elapsed":398006,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}},"outputId":"1f3bd701-5850-43cf-fef7-c7468e5449f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-09 10:31:17.244614: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2023-06-09 10:31:20 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n","2023-06-09 10:31:24 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 100, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4400, 'batch_size': 32, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 123873, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/drive/MyDrive/NLP/checkpoints/fiction/eda', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=100, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=1.0, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4400, batch_size=32, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4400, batch_size_valid=32, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta', max_epoch=5, max_update=123873, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/content/drive/MyDrive/NLP/checkpoints/fiction/eda', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='/content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda', num_classes=3, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, d2v2_multi=False, classification_head_name='mnli', regression_target=False, report_mcc=False, report_acc_and_f1=False, report_pearson_and_spearman=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.1, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, max_positions=512, dropout=0.1, attention_dropout=0.1, no_seed_provided=False, encoder_layers=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_attention_heads=12, activation_dropout=0.0, pooler_dropout=0.0, max_source_positions=512, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta'), 'task': {'_name': 'sentence_prediction', 'data': '/content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda', 'num_classes': 3, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 512, 'regression_target': False, 'classification_head_name': 'mnli', 'seed': 100, 'd2v2_multi': False}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'mnli', 'regression_target': False, 'report_mcc': False, 'report_acc_and_f1': False, 'report_pearson_and_spearman': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2023-06-09 10:31:25 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n","2023-06-09 10:31:25 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n","2023-06-09 10:31:29 | INFO | fairseq_cli.train | RobertaModel(\n","  (encoder): RobertaEncoder(\n","    (sentence_encoder): TransformerEncoder(\n","      (dropout_module): FairseqDropout()\n","      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","      (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (layers): ModuleList(\n","        (0-11): 12 x TransformerEncoderLayerBase(\n","          (self_attn): MultiheadAttention(\n","            (dropout_module): FairseqDropout()\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout_module): FairseqDropout()\n","          (activation_dropout_module): FairseqDropout()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (lm_head): RobertaLMHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (classification_heads): ModuleDict(\n","    (mnli): RobertaClassificationHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","      (out_proj): Linear(in_features=768, out_features=3, bias=True)\n","    )\n","  )\n",")\n","2023-06-09 10:31:29 | INFO | fairseq_cli.train | task: SentencePredictionTask\n","2023-06-09 10:31:29 | INFO | fairseq_cli.train | model: RobertaModel\n","2023-06-09 10:31:29 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n","2023-06-09 10:31:29 | INFO | fairseq_cli.train | num. shared model params: 125,289,564 (num. trained: 125,289,564)\n","2023-06-09 10:31:29 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n","2023-06-09 10:31:29 | INFO | fairseq.data.data_utils | loaded 1,973 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda/input0/valid\n","2023-06-09 10:31:30 | INFO | fairseq.data.data_utils | loaded 1,973 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda/input1/valid\n","2023-06-09 10:31:31 | INFO | fairseq.data.data_utils | loaded 1,973 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda/label/valid\n","2023-06-09 10:31:31 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 1973\n","2023-06-09 10:31:39 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n","2023-06-09 10:31:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-09 10:31:39 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n","2023-06-09 10:31:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-09 10:31:39 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2023-06-09 10:31:39 | INFO | fairseq_cli.train | max tokens per device = 4400 and max sentences per device = 32\n","2023-06-09 10:31:39 | INFO | fairseq.trainer | Preparing to load checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/eda/checkpoint_last.pt\n","2023-06-09 10:31:39 | INFO | fairseq.trainer | No existing checkpoint found /content/drive/MyDrive/NLP/checkpoints/fiction/eda/checkpoint_last.pt\n","2023-06-09 10:31:39 | INFO | fairseq.trainer | loading train data for epoch 1\n","2023-06-09 10:31:40 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda/input0/train\n","2023-06-09 10:31:40 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda/input1/train\n","2023-06-09 10:31:41 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda/label/train\n","2023-06-09 10:31:41 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 50000\n","2023-06-09 10:31:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 10:31:41 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-09 10:31:41 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-09 10:31:41 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-09 10:31:41 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n","2023-06-09 10:31:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 10:31:41 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-09 10:31:41 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-09 10:31:41 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-09 10:31:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1564\n","epoch 001:   0% 0/1564 [00:00<?, ?it/s]2023-06-09 10:31:42 | INFO | fairseq.trainer | begin training epoch 1\n","2023-06-09 10:31:42 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 001:  95% 1485/1564 [03:45<00:10,  7.40it/s, loss=1.571, nll_loss=0.058, accuracy=37.3, wps=5856.6, ups=6.78, wpb=863.2, bsz=32, num_updates=1400, lr=3.5e-06, gnorm=4.007, loss_scale=4096, train_wall=14, gb_free=11.7, wall=217]2023-06-09 10:35:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 001: 100% 1563/1564 [03:57<00:00,  7.97it/s, loss=1.564, nll_loss=0.058, accuracy=39, wps=5817.6, ups=6.73, wpb=864.7, bsz=32, num_updates=1500, lr=3.75e-06, gnorm=3.914, loss_scale=4096, train_wall=14, gb_free=11.9, wall=232]2023-06-09 10:35:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 10:35:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 17.65it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   8% 5/62 [00:00<00:03, 18.97it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  13% 8/62 [00:00<00:02, 21.33it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  18% 11/62 [00:00<00:02, 22.35it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  23% 14/62 [00:00<00:02, 23.72it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  27% 17/62 [00:00<00:01, 24.38it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  32% 20/62 [00:00<00:01, 25.04it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  37% 23/62 [00:00<00:01, 25.80it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  42% 26/62 [00:01<00:01, 26.21it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  47% 29/62 [00:01<00:01, 25.54it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  52% 32/62 [00:01<00:01, 26.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  56% 35/62 [00:01<00:01, 26.29it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  61% 38/62 [00:01<00:00, 26.46it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  66% 41/62 [00:01<00:00, 26.23it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  71% 44/62 [00:01<00:00, 26.48it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  77% 48/62 [00:01<00:00, 27.53it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  84% 52/62 [00:02<00:00, 29.13it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  90% 56/62 [00:02<00:00, 30.12it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  97% 60/62 [00:02<00:00, 28.91it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 10:35:42 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.581 | nll_loss 0.054 | accuracy 37.1 | wps 24715.9 | wpb 931.1 | bsz 31.8 | num_updates 1563\n","2023-06-09 10:35:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1563 updates\n","2023-06-09 10:35:42 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda/checkpoint_best.pt\n","2023-06-09 10:35:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda/checkpoint_best.pt\n","2023-06-09 10:36:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/eda/checkpoint_best.pt (epoch 1 @ 1563 updates, score 37.1) (writing took 28.04176137200011 seconds)\n","2023-06-09 10:36:10 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2023-06-09 10:36:10 | INFO | train | epoch 001 | loss 1.582 | nll_loss 0.058 | accuracy 36.2 | wps 5124.1 | ups 5.91 | wpb 866.7 | bsz 32 | num_updates 1563 | lr 3.9075e-06 | gnorm 4.458 | loss_scale 4096 | train_wall 229 | gb_free 11.8 | wall 271\n","2023-06-09 10:36:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 10:36:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1564\n","epoch 002:   0% 0/1564 [00:00<?, ?it/s]2023-06-09 10:36:10 | INFO | fairseq.trainer | begin training epoch 2\n","2023-06-09 10:36:10 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 002:   3% 52/1564 [00:09<04:37,  5.44it/s, loss=1.558, nll_loss=0.058, accuracy=39.6, wps=1841.8, ups=2.16, wpb=854.4, bsz=32, num_updates=1600, lr=4e-06, gnorm=3.909, loss_scale=4096, train_wall=15, gb_free=11.9, wall=278]2023-06-09 10:36:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  14% 213/1564 [00:38<03:43,  6.04it/s, loss=1.557, nll_loss=0.057, accuracy=39.9, wps=4281, ups=4.91, wpb=872.5, bsz=32, num_updates=1700, lr=4.25e-06, gnorm=4.364, loss_scale=4096, train_wall=19, gb_free=11.9, wall=298]2023-06-09 10:36:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  24% 383/1564 [01:03<02:49,  6.98it/s, loss=1.553, nll_loss=0.057, accuracy=42.3, wps=5712.7, ups=6.55, wpb=871.8, bsz=31.9, num_updates=1900, lr=4.75e-06, gnorm=5.007, loss_scale=4096, train_wall=15, gb_free=11.8, wall=329]2023-06-09 10:37:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  33% 515/1564 [01:23<02:34,  6.77it/s, loss=1.535, nll_loss=0.057, accuracy=43.1, wps=5735.7, ups=6.67, wpb=860, bsz=32, num_updates=2000, lr=5e-06, gnorm=5.1, loss_scale=4096, train_wall=14, gb_free=11.8, wall=344]2023-06-09 10:37:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  42% 651/1564 [01:43<02:12,  6.89it/s, loss=1.533, nll_loss=0.057, accuracy=43, wps=5798, ups=6.73, wpb=861, bsz=31.9, num_updates=2200, lr=5.5e-06, gnorm=4.92, loss_scale=4096, train_wall=14, gb_free=11.7, wall=374]2023-06-09 10:37:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  51% 803/1564 [02:06<01:56,  6.55it/s, loss=1.527, nll_loss=0.056, accuracy=44.1, wps=5819.2, ups=6.71, wpb=867.4, bsz=32, num_updates=2300, lr=5.75e-06, gnorm=5.714, loss_scale=4096, train_wall=14, gb_free=11.6, wall=389]2023-06-09 10:38:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  61% 950/1564 [02:28<01:32,  6.67it/s, loss=1.519, nll_loss=0.056, accuracy=44.8, wps=5738.1, ups=6.64, wpb=864.7, bsz=31.9, num_updates=2500, lr=6.25e-06, gnorm=5.253, loss_scale=8192, train_wall=15, gb_free=11.8, wall=419]2023-06-09 10:38:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  69% 1084/1564 [02:49<01:03,  7.53it/s, loss=1.508, nll_loss=0.056, accuracy=45.3, wps=5623.4, ups=6.5, wpb=865.7, bsz=32, num_updates=2600, lr=6.5e-06, gnorm=5.951, loss_scale=4096, train_wall=15, gb_free=11.5, wall=435]2023-06-09 10:38:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  78% 1217/1564 [03:08<00:49,  6.97it/s, loss=1.494, nll_loss=0.056, accuracy=46, wps=5773.9, ups=6.71, wpb=860, bsz=32, num_updates=2700, lr=6.75e-06, gnorm=5.641, loss_scale=4096, train_wall=14, gb_free=11.4, wall=449]2023-06-09 10:39:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  86% 1350/1564 [03:29<00:36,  5.82it/s, loss=1.5, nll_loss=0.055, accuracy=45.7, wps=5826.3, ups=6.69, wpb=870.4, bsz=32, num_updates=2900, lr=7.25e-06, gnorm=5.611, loss_scale=8192, train_wall=14, gb_free=11.8, wall=480]2023-06-09 10:39:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  95% 1481/1564 [03:48<00:11,  7.16it/s, loss=1.491, nll_loss=0.054, accuracy=46.4, wps=5722.3, ups=6.52, wpb=877.6, bsz=32, num_updates=3000, lr=7.5e-06, gnorm=6.012, loss_scale=4096, train_wall=15, gb_free=11.9, wall=495]2023-06-09 10:39:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002: 100% 1563/1564 [04:00<00:00,  6.38it/s, loss=1.5, nll_loss=0.055, accuracy=46.7, wps=5911.2, ups=6.78, wpb=872, bsz=32, num_updates=3100, lr=7.75e-06, gnorm=5.996, loss_scale=4096, train_wall=14, gb_free=11.9, wall=510]2023-06-09 10:40:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 10:40:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 19.27it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   6% 4/62 [00:00<00:03, 17.62it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  11% 7/62 [00:00<00:02, 20.21it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  16% 10/62 [00:00<00:02, 21.38it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  21% 13/62 [00:00<00:02, 23.96it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  26% 16/62 [00:00<00:01, 24.64it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  31% 19/62 [00:00<00:01, 24.68it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  35% 22/62 [00:00<00:01, 25.47it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  42% 26/62 [00:01<00:01, 26.96it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  47% 29/62 [00:01<00:01, 25.94it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  52% 32/62 [00:01<00:01, 26.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  56% 35/62 [00:01<00:01, 26.34it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  61% 38/62 [00:01<00:00, 26.46it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  66% 41/62 [00:01<00:00, 26.07it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  71% 44/62 [00:01<00:00, 26.43it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  77% 48/62 [00:01<00:00, 27.51it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  84% 52/62 [00:02<00:00, 29.32it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  90% 56/62 [00:02<00:00, 30.23it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  97% 60/62 [00:02<00:00, 28.63it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 10:40:13 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 1.589 | nll_loss 0.054 | accuracy 40.4 | wps 24640.6 | wpb 931.1 | bsz 31.8 | num_updates 3116 | best_accuracy 40.4\n","2023-06-09 10:40:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3116 updates\n","2023-06-09 10:40:13 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda/checkpoint_best.pt\n","2023-06-09 10:40:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda/checkpoint_best.pt\n","2023-06-09 10:41:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/eda/checkpoint_best.pt (epoch 2 @ 3116 updates, score 40.4) (writing took 57.5552736730001 seconds)\n","2023-06-09 10:41:11 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2023-06-09 10:41:11 | INFO | train | epoch 002 | loss 1.522 | nll_loss 0.056 | accuracy 44.1 | wps 4474.9 | ups 5.16 | wpb 866.6 | bsz 32 | num_updates 3116 | lr 7.79e-06 | gnorm 5.321 | loss_scale 4096 | train_wall 232 | gb_free 11.9 | wall 572\n","2023-06-09 10:41:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 10:41:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1564\n","epoch 003:   0% 0/1564 [00:00<?, ?it/s]2023-06-09 10:41:11 | INFO | fairseq.trainer | begin training epoch 3\n","2023-06-09 10:41:11 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 003:   3% 40/1564 [00:07<05:32,  4.59it/s]2023-06-09 10:41:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  15% 232/1564 [00:44<03:16,  6.80it/s, loss=1.407, nll_loss=0.052, accuracy=52.2, wps=4496.7, ups=5.19, wpb=866.7, bsz=31.9, num_updates=3300, lr=8.25e-06, gnorm=6.927, loss_scale=4096, train_wall=18, gb_free=11.9, wall=609]2023-06-09 10:41:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  26% 399/1564 [01:09<03:00,  6.47it/s, loss=1.396, nll_loss=0.052, accuracy=54.1, wps=5712, ups=6.62, wpb=863, bsz=32, num_updates=3500, lr=8.75e-06, gnorm=7.081, loss_scale=4096, train_wall=15, gb_free=11.9, wall=640]2023-06-09 10:42:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  42% 656/1564 [01:47<02:19,  6.51it/s, loss=1.395, nll_loss=0.052, accuracy=53, wps=5739.3, ups=6.72, wpb=854.7, bsz=32, num_updates=3700, lr=9.25e-06, gnorm=7.788, loss_scale=4096, train_wall=14, gb_free=11.5, wall=670]2023-06-09 10:42:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  43% 668/1564 [01:49<02:02,  7.34it/s, loss=1.395, nll_loss=0.052, accuracy=53, wps=5739.3, ups=6.72, wpb=854.7, bsz=32, num_updates=3700, lr=9.25e-06, gnorm=7.788, loss_scale=4096, train_wall=14, gb_free=11.5, wall=670]2023-06-09 10:43:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  51% 797/1564 [02:09<02:03,  6.20it/s, loss=1.358, nll_loss=0.05, accuracy=55.3, wps=5600.9, ups=6.48, wpb=864, bsz=31.9, num_updates=3900, lr=9.75e-06, gnorm=8.026, loss_scale=2048, train_wall=15, gb_free=11.8, wall=700]2023-06-09 10:43:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  60% 946/1564 [02:31<01:37,  6.35it/s, loss=1.348, nll_loss=0.05, accuracy=55.9, wps=5785, ups=6.74, wpb=858.2, bsz=31.8, num_updates=4000, lr=1e-05, gnorm=8.665, loss_scale=2048, train_wall=14, gb_free=11.8, wall=715]2023-06-09 10:43:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  73% 1136/1564 [02:59<01:02,  6.88it/s, loss=1.35, nll_loss=0.05, accuracy=55.6, wps=5873.6, ups=6.77, wpb=867.9, bsz=32, num_updates=4200, lr=9.759e-06, gnorm=8.218, loss_scale=4096, train_wall=14, gb_free=11.8, wall=745]2023-06-09 10:44:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  85% 1322/1564 [03:27<00:34,  6.99it/s, loss=1.363, nll_loss=0.05, accuracy=54.1, wps=5836.2, ups=6.67, wpb=875.2, bsz=32, num_updates=4400, lr=9.53463e-06, gnorm=8.379, loss_scale=4096, train_wall=14, gb_free=11.3, wall=775]2023-06-09 10:44:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  96% 1508/1564 [03:55<00:08,  6.93it/s, loss=1.297, nll_loss=0.047, accuracy=58.9, wps=5731.4, ups=6.51, wpb=880.2, bsz=32, num_updates=4600, lr=9.32505e-06, gnorm=8.269, loss_scale=4096, train_wall=15, gb_free=11.8, wall=805]2023-06-09 10:45:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003: 100% 1563/1564 [04:03<00:00,  7.59it/s, loss=1.297, nll_loss=0.047, accuracy=58.9, wps=5731.4, ups=6.51, wpb=880.2, bsz=32, num_updates=4600, lr=9.32505e-06, gnorm=8.269, loss_scale=4096, train_wall=15, gb_free=11.8, wall=805]2023-06-09 10:45:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 10:45:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 15.35it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   8% 5/62 [00:00<00:03, 17.99it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  11% 7/62 [00:00<00:03, 14.80it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  15% 9/62 [00:00<00:03, 14.50it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  19% 12/62 [00:00<00:02, 17.07it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  23% 14/62 [00:00<00:02, 17.60it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  27% 17/62 [00:00<00:02, 18.32it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  32% 20/62 [00:01<00:02, 19.90it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  37% 23/62 [00:01<00:01, 21.76it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  42% 26/62 [00:01<00:01, 21.80it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  47% 29/62 [00:01<00:01, 21.90it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  52% 32/62 [00:01<00:01, 22.70it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  56% 35/62 [00:01<00:01, 22.71it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  61% 38/62 [00:01<00:01, 22.41it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  66% 41/62 [00:02<00:01, 20.97it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  71% 44/62 [00:02<00:00, 20.73it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  76% 47/62 [00:02<00:00, 21.30it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  81% 50/62 [00:02<00:00, 22.91it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  87% 54/62 [00:02<00:00, 25.81it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  92% 57/62 [00:02<00:00, 26.61it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  97% 60/62 [00:02<00:00, 26.63it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 10:45:17 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 1.633 | nll_loss 0.056 | accuracy 41.5 | wps 20488.5 | wpb 931.1 | bsz 31.8 | num_updates 4670 | best_accuracy 41.5\n","2023-06-09 10:45:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4670 updates\n","2023-06-09 10:45:17 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda/checkpoint_best.pt\n","2023-06-09 10:45:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda/checkpoint_best.pt\n","2023-06-09 10:45:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/eda/checkpoint_best.pt (epoch 3 @ 4670 updates, score 41.5) (writing took 38.3724548109999 seconds)\n","2023-06-09 10:45:56 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2023-06-09 10:45:56 | INFO | train | epoch 003 | loss 1.372 | nll_loss 0.051 | accuracy 54.5 | wps 4728 | ups 5.45 | wpb 866.9 | bsz 32 | num_updates 4670 | lr 9.2549e-06 | gnorm 7.87 | loss_scale 2048 | train_wall 235 | gb_free 11.9 | wall 857\n","2023-06-09 10:45:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 10:45:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1564\n","epoch 004:   0% 0/1564 [00:00<?, ?it/s]2023-06-09 10:45:56 | INFO | fairseq.trainer | begin training epoch 4\n","2023-06-09 10:45:56 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 004:   5% 79/1564 [00:17<04:22,  5.66it/s, loss=1.285, nll_loss=0.047, accuracy=59.8, wps=1455.9, ups=1.68, wpb=866.3, bsz=32, num_updates=4700, lr=9.22531e-06, gnorm=8.732, loss_scale=2048, train_wall=17, gb_free=11.6, wall=865]2023-06-09 10:46:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  13% 209/1564 [00:42<03:28,  6.50it/s, loss=1.226, nll_loss=0.046, accuracy=60.9, wps=4197.2, ups=4.9, wpb=857.1, bsz=32, num_updates=4800, lr=9.12871e-06, gnorm=9.386, loss_scale=2048, train_wall=20, gb_free=11.8, wall=885]2023-06-09 10:46:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  25% 395/1564 [01:11<02:51,  6.81it/s, loss=1.187, nll_loss=0.044, accuracy=63.3, wps=5458.9, ups=6.28, wpb=868.7, bsz=32, num_updates=5000, lr=8.94427e-06, gnorm=9.568, loss_scale=2048, train_wall=15, gb_free=11.9, wall=919]2023-06-09 10:47:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  34% 529/1564 [01:31<02:36,  6.62it/s, loss=1.207, nll_loss=0.045, accuracy=62.8, wps=5841.3, ups=6.74, wpb=866.5, bsz=32, num_updates=5100, lr=8.85615e-06, gnorm=9.821, loss_scale=2048, train_wall=14, gb_free=11.6, wall=934]2023-06-09 10:47:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  46% 714/1564 [01:58<02:00,  7.07it/s, loss=1.186, nll_loss=0.044, accuracy=63.8, wps=5774.7, ups=6.73, wpb=857.9, bsz=31.9, num_updates=5300, lr=8.68744e-06, gnorm=10.42, loss_scale=2048, train_wall=14, gb_free=11.9, wall=964]2023-06-09 10:47:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  58% 908/1564 [02:28<01:49,  6.00it/s, loss=1.157, nll_loss=0.042, accuracy=65.2, wps=5797.2, ups=6.65, wpb=871.8, bsz=32, num_updates=5500, lr=8.52803e-06, gnorm=10.819, loss_scale=2048, train_wall=15, gb_free=10.5, wall=994]2023-06-09 10:48:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  69% 1080/1564 [02:53<01:06,  7.28it/s, loss=1.14, nll_loss=0.042, accuracy=65.5, wps=5807.2, ups=6.76, wpb=858.9, bsz=32, num_updates=5700, lr=8.37708e-06, gnorm=10.728, loss_scale=2048, train_wall=14, gb_free=11.3, wall=1024]2023-06-09 10:48:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  78% 1223/1564 [03:15<00:47,  7.15it/s, loss=1.149, nll_loss=0.042, accuracy=64.2, wps=5689.5, ups=6.54, wpb=870, bsz=32, num_updates=5800, lr=8.30455e-06, gnorm=10.192, loss_scale=2048, train_wall=15, gb_free=11.7, wall=1039]2023-06-09 10:49:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  87% 1360/1564 [03:35<00:28,  7.13it/s, loss=1.117, nll_loss=0.041, accuracy=66, wps=5624.1, ups=6.47, wpb=869.2, bsz=32, num_updates=6000, lr=8.16497e-06, gnorm=10.687, loss_scale=2048, train_wall=15, gb_free=11.8, wall=1070]2023-06-09 10:49:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  96% 1496/1564 [03:56<00:10,  6.37it/s, loss=1.107, nll_loss=0.041, accuracy=66.2, wps=5686.3, ups=6.59, wpb=862.3, bsz=32, num_updates=6100, lr=8.09776e-06, gnorm=11.42, loss_scale=2048, train_wall=15, gb_free=11.7, wall=1085]2023-06-09 10:49:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004: 100% 1563/1564 [04:06<00:00,  6.88it/s, loss=1.086, nll_loss=0.04, accuracy=66.6, wps=5875.7, ups=6.69, wpb=878.2, bsz=32, num_updates=6200, lr=8.03219e-06, gnorm=11.222, loss_scale=2048, train_wall=14, gb_free=11.7, wall=1100]2023-06-09 10:50:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 10:50:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 19.39it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   8% 5/62 [00:00<00:02, 21.25it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  13% 8/62 [00:00<00:02, 22.33it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  18% 11/62 [00:00<00:02, 22.81it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  23% 14/62 [00:00<00:01, 24.05it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  27% 17/62 [00:00<00:01, 25.13it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  32% 20/62 [00:00<00:01, 25.50it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  37% 23/62 [00:00<00:01, 26.19it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  42% 26/62 [00:01<00:01, 25.19it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  47% 29/62 [00:01<00:01, 24.30it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  52% 32/62 [00:01<00:01, 24.36it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  56% 35/62 [00:01<00:01, 24.23it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  61% 38/62 [00:01<00:01, 23.45it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  66% 41/62 [00:01<00:00, 22.55it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  71% 44/62 [00:01<00:00, 21.88it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  76% 47/62 [00:02<00:00, 22.24it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  81% 50/62 [00:02<00:00, 23.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  87% 54/62 [00:02<00:00, 25.94it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  92% 57/62 [00:02<00:00, 25.81it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  97% 60/62 [00:02<00:00, 25.80it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 10:50:04 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 1.816 | nll_loss 0.062 | accuracy 41.6 | wps 22733.1 | wpb 931.1 | bsz 31.8 | num_updates 6224 | best_accuracy 41.6\n","2023-06-09 10:50:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6224 updates\n","2023-06-09 10:50:04 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda/checkpoint_best.pt\n","2023-06-09 10:50:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda/checkpoint_best.pt\n","2023-06-09 10:50:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/eda/checkpoint_best.pt (epoch 4 @ 6224 updates, score 41.6) (writing took 39.02274566300002 seconds)\n","2023-06-09 10:50:43 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2023-06-09 10:50:43 | INFO | train | epoch 004 | loss 1.16 | nll_loss 0.043 | accuracy 64.4 | wps 4680.4 | ups 5.4 | wpb 866.8 | bsz 32 | num_updates 6224 | lr 8.01669e-06 | gnorm 10.391 | loss_scale 2048 | train_wall 237 | gb_free 11.9 | wall 1145\n","2023-06-09 10:50:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 10:50:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1564\n","epoch 005:   0% 0/1564 [00:00<?, ?it/s]2023-06-09 10:50:43 | INFO | fairseq.trainer | begin training epoch 5\n","2023-06-09 10:50:43 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 005:   6% 88/1564 [00:18<04:44,  5.18it/s, loss=1.019, nll_loss=0.037, accuracy=71.3, wps=1410.5, ups=1.63, wpb=865.1, bsz=31.8, num_updates=6300, lr=7.96819e-06, gnorm=11.231, loss_scale=4096, train_wall=19, gb_free=11.6, wall=1161]2023-06-09 10:51:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  17% 272/1564 [00:53<03:28,  6.18it/s, loss=0.986, nll_loss=0.037, accuracy=71.2, wps=4382.3, ups=5.08, wpb=863.3, bsz=32, num_updates=6400, lr=7.90569e-06, gnorm=12.239, loss_scale=2048, train_wall=19, gb_free=11.9, wall=1181]2023-06-09 10:51:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  26% 401/1564 [01:12<02:58,  6.52it/s, loss=0.992, nll_loss=0.036, accuracy=71.3, wps=5728.5, ups=6.57, wpb=871.3, bsz=32, num_updates=6600, lr=7.78499e-06, gnorm=11.709, loss_scale=2048, train_wall=15, gb_free=11.9, wall=1214]2023-06-09 10:51:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  35% 544/1564 [01:33<02:31,  6.73it/s, loss=0.972, nll_loss=0.036, accuracy=71.6, wps=5799.2, ups=6.66, wpb=870.6, bsz=32, num_updates=6700, lr=7.72667e-06, gnorm=11.87, loss_scale=2048, train_wall=14, gb_free=11.8, wall=1229]2023-06-09 10:52:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  44% 683/1564 [01:55<02:03,  7.11it/s, loss=1.007, nll_loss=0.037, accuracy=70.2, wps=5680.6, ups=6.55, wpb=867.2, bsz=31.9, num_updates=6900, lr=7.61387e-06, gnorm=12.45, loss_scale=4096, train_wall=15, gb_free=11.8, wall=1260]2023-06-09 10:52:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  54% 837/1564 [02:18<02:02,  5.94it/s, loss=0.965, nll_loss=0.036, accuracy=71.3, wps=5809.4, ups=6.74, wpb=862.1, bsz=32, num_updates=7000, lr=7.55929e-06, gnorm=12.272, loss_scale=2048, train_wall=14, gb_free=11.9, wall=1274]2023-06-09 10:53:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  63% 985/1564 [02:40<01:21,  7.10it/s, loss=0.936, nll_loss=0.035, accuracy=73, wps=5817.1, ups=6.71, wpb=867.4, bsz=32, num_updates=7200, lr=7.45356e-06, gnorm=11.749, loss_scale=4096, train_wall=14, gb_free=11.9, wall=1304]2023-06-09 10:53:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  72% 1125/1564 [03:00<01:05,  6.66it/s, loss=0.984, nll_loss=0.036, accuracy=71.3, wps=5902.8, ups=6.8, wpb=867.9, bsz=32, num_updates=7300, lr=7.40233e-06, gnorm=12.113, loss_scale=2048, train_wall=14, gb_free=11.6, wall=1319]2023-06-09 10:53:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  84% 1321/1564 [03:29<00:33,  7.16it/s, loss=0.958, nll_loss=0.035, accuracy=72.3, wps=5835.9, ups=6.64, wpb=878.3, bsz=32, num_updates=7500, lr=7.30297e-06, gnorm=12.439, loss_scale=4096, train_wall=15, gb_free=11.8, wall=1349]2023-06-09 10:54:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  93% 1456/1564 [03:49<00:15,  6.91it/s, loss=0.96, nll_loss=0.036, accuracy=71.7, wps=5777.9, ups=6.69, wpb=863.2, bsz=31.9, num_updates=7600, lr=7.25476e-06, gnorm=12.251, loss_scale=2048, train_wall=14, gb_free=11.9, wall=1364]2023-06-09 10:54:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005: 100% 1563/1564 [04:06<00:00,  7.08it/s, loss=0.949, nll_loss=0.035, accuracy=71.6, wps=5729.7, ups=6.62, wpb=865.2, bsz=31.9, num_updates=7700, lr=7.2075e-06, gnorm=12.04, loss_scale=2048, train_wall=15, gb_free=11.7, wall=1379]2023-06-09 10:54:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 10:54:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 005 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 19.80it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   6% 4/62 [00:00<00:02, 19.50it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  11% 7/62 [00:00<00:02, 20.23it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  16% 10/62 [00:00<00:02, 21.73it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  21% 13/62 [00:00<00:02, 24.25it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  26% 16/62 [00:00<00:01, 25.49it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  31% 19/62 [00:00<00:01, 25.51it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  35% 22/62 [00:00<00:01, 26.20it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  42% 26/62 [00:01<00:01, 27.77it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  47% 29/62 [00:01<00:01, 26.52it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  52% 32/62 [00:01<00:01, 26.45it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  56% 35/62 [00:01<00:01, 26.52it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  61% 38/62 [00:01<00:00, 26.34it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  66% 41/62 [00:01<00:00, 25.82it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  71% 44/62 [00:01<00:00, 26.37it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  77% 48/62 [00:01<00:00, 27.36it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  84% 52/62 [00:01<00:00, 29.04it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  90% 56/62 [00:02<00:00, 30.08it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  97% 60/62 [00:02<00:00, 28.78it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 10:54:52 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 2.121 | nll_loss 0.072 | accuracy 41.9 | wps 24908.1 | wpb 931.1 | bsz 31.8 | num_updates 7778 | best_accuracy 41.9\n","2023-06-09 10:54:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7778 updates\n","2023-06-09 10:54:52 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda/checkpoint_best.pt\n","2023-06-09 10:55:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda/checkpoint_best.pt\n","2023-06-09 10:55:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/eda/checkpoint_best.pt (epoch 5 @ 7778 updates, score 41.9) (writing took 27.98863250300019 seconds)\n","2023-06-09 10:55:20 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n","2023-06-09 10:55:20 | INFO | train | epoch 005 | loss 0.978 | nll_loss 0.036 | accuracy 71.3 | wps 4869.4 | ups 5.62 | wpb 866.8 | bsz 32 | num_updates 7778 | lr 7.17127e-06 | gnorm 12.051 | loss_scale 2048 | train_wall 237 | gb_free 11.9 | wall 1421\n","2023-06-09 10:55:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 10:55:20 | INFO | fairseq_cli.train | done training in 1418.0 seconds\n"]}],"source":["import tensorflow as tf\n","with tf.device('/device:GPU:0'):\n","    ! CUDA_VISIBLE_DEVICES=0 fairseq-train /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda \\\n","          --save-dir /content/drive/MyDrive/NLP/checkpoints/fiction/eda \\\n","          --reset-optimizer  \\\n","          --reset-dataloader  \\\n","          --reset-meters  \\\n","          --best-checkpoint-metric accuracy  \\\n","          --maximize-best-checkpoint-metric  \\\n","          --no-epoch-checkpoints \\\n","          --find-unused-parameters \\\n","          --distributed-world-size 1 \\\n","          --task sentence_prediction  \\\n","          --num-classes 3  \\\n","          --init-token 0  \\\n","          --separator-token 2   \\\n","          --max-positions 512  \\\n","          --shorten-method \"truncate\"  \\\n","          --arch roberta \\\n","          --dropout 0.1  \\\n","          --attention-dropout 0.1  \\\n","          --weight-decay 0.1  \\\n","          --criterion sentence_prediction  \\\n","          --classification-head-name 'mnli' \\\n","          --optimizer adam  \\\n","          --adam-betas '(0.9, 0.98)'  \\\n","          --adam-eps 1e-06  \\\n","          --clip-norm 0.0  \\\n","          --lr-scheduler inverse_sqrt  \\\n","          --lr 1e-05 \\\n","          --fp16  \\\n","          --fp16-init-scale 4  \\\n","          --threshold-loss-scale 1  \\\n","          --fp16-scale-window 128  \\\n","          --batch-size 32  \\\n","          --required-batch-size-multiple 1  \\\n","          --max-tokens 4400 \\\n","          --update-freq 1  \\\n","          --max-update 123873 \\\n","          --max-epoch 5 \\\n","          --seed 100"]},{"cell_type":"code","source":["from fairseq.models.roberta import RobertaModel\n","roberta = RobertaModel.from_pretrained(\n","    '/content/drive/MyDrive/NLP/checkpoints/fiction/eda',\n","    checkpoint_file='checkpoint_best.pt',\n","    data_name_or_path='/content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda'\n",")\n","roberta.eval()"],"metadata":{"id":"7S3_eW5wsGrm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["genre = ['slate','fiction','telephone','travel','government']\n","for g in genre:\n","  input0 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input0', \"r\")\n","  input1 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input1', \"r\")\n","  label = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.label', \"r\")\n","\n","  accuracy = 0\n","  total = 0\n","  for (x1, x2, y) in zip(input0, input1, label):\n","    tokens = roberta.encode(x1, x2)\n","    idx = roberta.predict('mnli', tokens).argmax().item()\n","    dictionary = roberta.task.label_dictionary\n","    pred = dictionary[idx + dictionary.nspecial]\n","    total = total + 1\n","    if  (pred == y.strip()) :\n","      accuracy = accuracy + 1\n","\n","  print(g,\": \",accuracy)\n","  print(g,\": \",total)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cu1fezmoi10r","executionInfo":{"status":"ok","timestamp":1686316500089,"user_tz":-120,"elapsed":1511800,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}},"outputId":"d738f2a0-d475-4046-b250-b12451c1f3f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["slate :  768\n","slate :  2000\n","fiction :  793\n","fiction :  2000\n","telephone :  801\n","telephone :  2000\n","travel :  750\n","travel :  2000\n","government :  747\n","government :  2000\n"]}]},{"cell_type":"code","source":["genre = ['verbatim','facetoface','oup','nineeleven','letters']\n","for g in genre:\n","  input0 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input0', \"r\")\n","  input1 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input1', \"r\")\n","  label = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.label', \"r\")\n","\n","  accuracy = 0\n","  total = 0\n","  for (x1, x2, y) in zip(input0, input1, label):\n","    tokens = roberta.encode(x1, x2)\n","    idx = roberta.predict('mnli', tokens).argmax().item()\n","    dictionary = roberta.task.label_dictionary\n","    pred = dictionary[idx + dictionary.nspecial]\n","    total = total + 1\n","    if  (pred == y.strip()) :\n","      accuracy = accuracy + 1\n","\n","  print(g,\": \",accuracy)\n","  print(g,\": \",total)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w6BnjpWroWUL","executionInfo":{"status":"ok","timestamp":1686492176125,"user_tz":-120,"elapsed":886653,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}},"outputId":"30a43338-9d71-4783-c348-f3326951ab27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["verbatim :  416\n","verbatim :  1000\n","facetoface :  385\n","facetoface :  1000\n","oup :  396\n","oup :  1000\n","nineeleven :  382\n","nineeleven :  1000\n","letters :  374\n","letters :  1000\n"]}]},{"cell_type":"markdown","source":["# UDA"],"metadata":{"id":"OS6OiMa_i2O8"}},{"cell_type":"code","source":["import tensorflow as tf\n","with tf.device('/device:GPU:0'):\n","    ! CUDA_VISIBLE_DEVICES=0 fairseq-train /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_uda \\\n","          --save-dir /content/drive/MyDrive/NLP/checkpoints/fiction/uda \\\n","          --reset-optimizer  \\\n","          --reset-dataloader  \\\n","          --reset-meters  \\\n","          --best-checkpoint-metric accuracy  \\\n","          --maximize-best-checkpoint-metric  \\\n","          --no-epoch-checkpoints \\\n","          --find-unused-parameters \\\n","          --distributed-world-size 1 \\\n","          --task sentence_prediction  \\\n","          --num-classes 3  \\\n","          --init-token 0  \\\n","          --separator-token 2   \\\n","          --max-positions 512  \\\n","          --shorten-method \"truncate\"  \\\n","          --arch roberta \\\n","          --dropout 0.1  \\\n","          --attention-dropout 0.1  \\\n","          --weight-decay 0.1  \\\n","          --criterion sentence_prediction  \\\n","          --classification-head-name 'mnli' \\\n","          --optimizer adam  \\\n","          --adam-betas '(0.9, 0.98)'  \\\n","          --adam-eps 1e-06  \\\n","          --clip-norm 0.0  \\\n","          --lr-scheduler inverse_sqrt  \\\n","          --lr 1e-05 \\\n","          --fp16  \\\n","          --fp16-init-scale 4  \\\n","          --threshold-loss-scale 1  \\\n","          --fp16-scale-window 128  \\\n","          --batch-size 32  \\\n","          --required-batch-size-multiple 1  \\\n","          --max-tokens 4400 \\\n","          --update-freq 1  \\\n","          --max-update 123873 \\\n","          --max-epoch 5 \\\n","          --seed 100"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QZJTZxYNofr0","executionInfo":{"status":"ok","timestamp":1686311449404,"user_tz":-120,"elapsed":1455254,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}},"outputId":"c33edb94-fc57-45a4-99ca-21c45ed14d91"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-09 11:26:37.001679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2023-06-09 11:26:38 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n","2023-06-09 11:26:41 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 100, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4400, 'batch_size': 32, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 123873, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/drive/MyDrive/NLP/checkpoints/fiction/uda', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=100, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=1.0, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4400, batch_size=32, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4400, batch_size_valid=32, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta', max_epoch=5, max_update=123873, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/content/drive/MyDrive/NLP/checkpoints/fiction/uda', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='/content/drive/MyDrive/NLP/mnli/fiction/orig/bin_uda', num_classes=3, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, d2v2_multi=False, classification_head_name='mnli', regression_target=False, report_mcc=False, report_acc_and_f1=False, report_pearson_and_spearman=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.1, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, max_positions=512, dropout=0.1, attention_dropout=0.1, no_seed_provided=False, encoder_layers=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_attention_heads=12, activation_dropout=0.0, pooler_dropout=0.0, max_source_positions=512, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta'), 'task': {'_name': 'sentence_prediction', 'data': '/content/drive/MyDrive/NLP/mnli/fiction/orig/bin_uda', 'num_classes': 3, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 512, 'regression_target': False, 'classification_head_name': 'mnli', 'seed': 100, 'd2v2_multi': False}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'mnli', 'regression_target': False, 'report_mcc': False, 'report_acc_and_f1': False, 'report_pearson_and_spearman': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2023-06-09 11:26:41 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n","2023-06-09 11:26:41 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n","2023-06-09 11:26:45 | INFO | fairseq_cli.train | RobertaModel(\n","  (encoder): RobertaEncoder(\n","    (sentence_encoder): TransformerEncoder(\n","      (dropout_module): FairseqDropout()\n","      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","      (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (layers): ModuleList(\n","        (0-11): 12 x TransformerEncoderLayerBase(\n","          (self_attn): MultiheadAttention(\n","            (dropout_module): FairseqDropout()\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout_module): FairseqDropout()\n","          (activation_dropout_module): FairseqDropout()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (lm_head): RobertaLMHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (classification_heads): ModuleDict(\n","    (mnli): RobertaClassificationHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","      (out_proj): Linear(in_features=768, out_features=3, bias=True)\n","    )\n","  )\n",")\n","2023-06-09 11:26:45 | INFO | fairseq_cli.train | task: SentencePredictionTask\n","2023-06-09 11:26:45 | INFO | fairseq_cli.train | model: RobertaModel\n","2023-06-09 11:26:45 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n","2023-06-09 11:26:45 | INFO | fairseq_cli.train | num. shared model params: 125,289,564 (num. trained: 125,289,564)\n","2023-06-09 11:26:45 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n","2023-06-09 11:26:45 | INFO | fairseq.data.data_utils | loaded 1,973 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_uda/input0/valid\n","2023-06-09 11:26:46 | INFO | fairseq.data.data_utils | loaded 1,973 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_uda/input1/valid\n","2023-06-09 11:26:46 | INFO | fairseq.data.data_utils | loaded 1,973 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_uda/label/valid\n","2023-06-09 11:26:46 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 1973\n","2023-06-09 11:26:53 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n","2023-06-09 11:26:53 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-09 11:26:53 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n","2023-06-09 11:26:53 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-09 11:26:53 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2023-06-09 11:26:53 | INFO | fairseq_cli.train | max tokens per device = 4400 and max sentences per device = 32\n","2023-06-09 11:26:53 | INFO | fairseq.trainer | Preparing to load checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/uda/checkpoint_last.pt\n","2023-06-09 11:26:53 | INFO | fairseq.trainer | No existing checkpoint found /content/drive/MyDrive/NLP/checkpoints/fiction/uda/checkpoint_last.pt\n","2023-06-09 11:26:53 | INFO | fairseq.trainer | loading train data for epoch 1\n","2023-06-09 11:26:55 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_uda/input0/train\n","2023-06-09 11:26:56 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_uda/input1/train\n","2023-06-09 11:26:56 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_uda/label/train\n","2023-06-09 11:26:56 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 50000\n","2023-06-09 11:26:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 11:26:56 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-09 11:26:56 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-09 11:26:56 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-09 11:26:56 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n","2023-06-09 11:26:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 11:26:56 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-09 11:26:56 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-09 11:26:56 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-09 11:26:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1584\n","epoch 001:   0% 0/1584 [00:00<?, ?it/s]2023-06-09 11:26:57 | INFO | fairseq.trainer | begin training epoch 1\n","2023-06-09 11:26:57 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 001:  93% 1477/1584 [03:53<00:16,  6.68it/s, loss=1.568, nll_loss=0.054, accuracy=38.4, wps=5835.2, ups=6.36, wpb=918.2, bsz=31.4, num_updates=1400, lr=3.5e-06, gnorm=3.891, loss_scale=4096, train_wall=15, gb_free=11.6, wall=225]2023-06-09 11:30:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 001: 100% 1583/1584 [04:10<00:00,  6.66it/s, loss=1.577, nll_loss=0.054, accuracy=36.8, wps=5910.8, ups=6.38, wpb=926, bsz=31.6, num_updates=1500, lr=3.75e-06, gnorm=4.4, loss_scale=4096, train_wall=15, gb_free=11.9, wall=241]2023-06-09 11:31:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 11:31:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 18.35it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   6% 4/62 [00:00<00:03, 19.00it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  11% 7/62 [00:00<00:02, 20.00it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  16% 10/62 [00:00<00:02, 21.48it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  21% 13/62 [00:00<00:02, 23.92it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  26% 16/62 [00:00<00:01, 24.57it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  31% 19/62 [00:00<00:01, 24.82it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  35% 22/62 [00:00<00:01, 25.90it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  42% 26/62 [00:01<00:01, 26.98it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  47% 29/62 [00:01<00:01, 26.24it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  52% 32/62 [00:01<00:01, 26.01it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  56% 35/62 [00:01<00:01, 26.11it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  61% 38/62 [00:01<00:00, 26.39it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  66% 41/62 [00:01<00:00, 26.30it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  71% 44/62 [00:01<00:00, 26.61it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  77% 48/62 [00:01<00:00, 27.58it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  84% 52/62 [00:02<00:00, 29.00it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  90% 56/62 [00:02<00:00, 30.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  95% 59/62 [00:02<00:00, 29.71it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset: 100% 62/62 [00:02<00:00, 29.46it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 11:31:10 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.587 | nll_loss 0.054 | accuracy 33.8 | wps 24732.5 | wpb 931.1 | bsz 31.8 | num_updates 1583\n","2023-06-09 11:31:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1583 updates\n","2023-06-09 11:31:10 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/uda/checkpoint_best.pt\n","2023-06-09 11:31:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/uda/checkpoint_best.pt\n","2023-06-09 11:31:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/uda/checkpoint_best.pt (epoch 1 @ 1583 updates, score 33.8) (writing took 24.483963230999507 seconds)\n","2023-06-09 11:31:34 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2023-06-09 11:31:34 | INFO | train | epoch 001 | loss 1.584 | nll_loss 0.054 | accuracy 35.9 | wps 5338.2 | ups 5.78 | wpb 923.2 | bsz 31.6 | num_updates 1583 | lr 3.9575e-06 | gnorm 4.499 | loss_scale 4096 | train_wall 243 | gb_free 11.8 | wall 281\n","2023-06-09 11:31:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 11:31:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1584\n","epoch 002:   0% 0/1584 [00:00<?, ?it/s]2023-06-09 11:31:34 | INFO | fairseq.trainer | begin training epoch 2\n","2023-06-09 11:31:34 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 002:   4% 58/1584 [00:11<09:56,  2.56it/s, loss=1.569, nll_loss=0.054, accuracy=38, wps=2149.3, ups=2.34, wpb=919.7, bsz=31.6, num_updates=1600, lr=4e-06, gnorm=3.986, loss_scale=4096, train_wall=15, gb_free=11.9, wall=284]2023-06-09 11:31:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  13% 200/1584 [00:36<03:26,  6.72it/s, loss=1.571, nll_loss=0.054, accuracy=39.1, wps=4724.4, ups=5.12, wpb=922.4, bsz=31.6, num_updates=1700, lr=4.25e-06, gnorm=4.091, loss_scale=4096, train_wall=19, gb_free=11.9, wall=303]2023-06-09 11:32:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  21% 332/1584 [00:58<03:13,  6.49it/s, loss=1.553, nll_loss=0.052, accuracy=40.2, wps=5779.2, ups=6.25, wpb=924.5, bsz=31.2, num_updates=1900, lr=4.75e-06, gnorm=4.17, loss_scale=4096, train_wall=15, gb_free=11.8, wall=337]2023-06-09 11:32:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  35% 561/1584 [01:33<02:26,  6.97it/s, loss=1.544, nll_loss=0.053, accuracy=41.8, wps=5888.2, ups=6.41, wpb=918.5, bsz=31.4, num_updates=2100, lr=5.25e-06, gnorm=4.223, loss_scale=8192, train_wall=15, gb_free=11.9, wall=368]2023-06-09 11:33:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  44% 701/1584 [01:55<02:20,  6.28it/s, loss=1.548, nll_loss=0.053, accuracy=41, wps=5871.8, ups=6.39, wpb=918.3, bsz=31.4, num_updates=2200, lr=5.5e-06, gnorm=4.623, loss_scale=4096, train_wall=15, gb_free=11.9, wall=384]2023-06-09 11:33:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  60% 958/1584 [02:35<01:29,  6.96it/s, loss=1.529, nll_loss=0.052, accuracy=43.6, wps=6018.9, ups=6.46, wpb=931.1, bsz=31.9, num_updates=2500, lr=6.25e-06, gnorm=4.337, loss_scale=8192, train_wall=15, gb_free=11.6, wall=431]2023-06-09 11:34:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8192.0\n","epoch 002:  61% 963/1584 [02:36<01:30,  6.87it/s, loss=1.529, nll_loss=0.052, accuracy=43.6, wps=6018.9, ups=6.46, wpb=931.1, bsz=31.9, num_updates=2500, lr=6.25e-06, gnorm=4.337, loss_scale=8192, train_wall=15, gb_free=11.6, wall=431]2023-06-09 11:34:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  71% 1127/1584 [03:01<01:06,  6.90it/s, loss=1.514, nll_loss=0.052, accuracy=44.1, wps=6109.9, ups=6.7, wpb=911.7, bsz=31.4, num_updates=2700, lr=6.75e-06, gnorm=4.738, loss_scale=8192, train_wall=15, gb_free=11.8, wall=462]2023-06-09 11:34:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  81% 1280/1584 [03:24<00:47,  6.47it/s, loss=1.508, nll_loss=0.052, accuracy=43.7, wps=6027.5, ups=6.54, wpb=921, bsz=32, num_updates=2800, lr=7e-06, gnorm=5.239, loss_scale=4096, train_wall=15, gb_free=11.8, wall=477]2023-06-09 11:34:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  90% 1421/1584 [03:46<00:25,  6.46it/s, loss=1.513, nll_loss=0.052, accuracy=45.9, wps=5814.7, ups=6.29, wpb=925, bsz=31.7, num_updates=2900, lr=7.25e-06, gnorm=5.303, loss_scale=4096, train_wall=15, gb_free=11.4, wall=493]2023-06-09 11:35:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002: 100% 1579/1584 [04:11<00:00,  6.55it/s, loss=1.498, nll_loss=0.052, accuracy=46.2, wps=5919.7, ups=6.37, wpb=929.6, bsz=32, num_updates=3100, lr=7.75e-06, gnorm=5.814, loss_scale=4096, train_wall=15, gb_free=11.4, wall=524]2023-06-09 11:35:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002: 100% 1583/1584 [04:11<00:00,  7.66it/s, loss=1.498, nll_loss=0.052, accuracy=46.2, wps=5919.7, ups=6.37, wpb=929.6, bsz=32, num_updates=3100, lr=7.75e-06, gnorm=5.814, loss_scale=4096, train_wall=15, gb_free=11.4, wall=524]2023-06-09 11:35:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 11:35:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 18.66it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   6% 4/62 [00:00<00:02, 19.35it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  11% 7/62 [00:00<00:02, 20.72it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  16% 10/62 [00:00<00:02, 21.94it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  21% 13/62 [00:00<00:02, 23.85it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  26% 16/62 [00:00<00:01, 24.83it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  31% 19/62 [00:00<00:01, 25.39it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  35% 22/62 [00:00<00:01, 24.42it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  40% 25/62 [00:01<00:01, 24.17it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  45% 28/62 [00:01<00:01, 22.17it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  50% 31/62 [00:01<00:01, 22.25it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  55% 34/62 [00:01<00:01, 22.40it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  60% 37/62 [00:01<00:01, 21.74it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  65% 40/62 [00:01<00:01, 21.14it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  69% 43/62 [00:01<00:00, 20.97it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  74% 46/62 [00:02<00:00, 22.35it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  79% 49/62 [00:02<00:00, 22.45it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  85% 53/62 [00:02<00:00, 25.08it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  90% 56/62 [00:02<00:00, 26.19it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  95% 59/62 [00:02<00:00, 26.66it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset: 100% 62/62 [00:02<00:00, 26.18it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 11:35:49 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 1.573 | nll_loss 0.054 | accuracy 40 | wps 21916.4 | wpb 931.1 | bsz 31.8 | num_updates 3156 | best_accuracy 40\n","2023-06-09 11:35:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3156 updates\n","2023-06-09 11:35:49 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/uda/checkpoint_best.pt\n","2023-06-09 11:36:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/uda/checkpoint_best.pt\n","2023-06-09 11:36:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/uda/checkpoint_best.pt (epoch 2 @ 3156 updates, score 40.0) (writing took 41.62375760800023 seconds)\n","2023-06-09 11:36:31 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2023-06-09 11:36:31 | INFO | train | epoch 002 | loss 1.534 | nll_loss 0.052 | accuracy 42.4 | wps 4899.4 | ups 5.31 | wpb 923.2 | bsz 31.6 | num_updates 3156 | lr 7.89e-06 | gnorm 4.693 | loss_scale 4096 | train_wall 244 | gb_free 11.6 | wall 577\n","2023-06-09 11:36:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 11:36:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1584\n","epoch 003:   0% 0/1584 [00:00<?, ?it/s]2023-06-09 11:36:31 | INFO | fairseq.trainer | begin training epoch 3\n","2023-06-09 11:36:31 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 003:   8% 129/1584 [00:26<05:56,  4.09it/s, loss=1.475, nll_loss=0.05, accuracy=47.9, wps=1444, ups=1.58, wpb=913.9, bsz=31.1, num_updates=3200, lr=8e-06, gnorm=5.865, loss_scale=4096, train_wall=18, gb_free=11.7, wall=588]2023-06-09 11:36:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  16% 261/1584 [00:51<02:55,  7.53it/s, loss=1.435, nll_loss=0.049, accuracy=51.5, wps=4543, ups=4.86, wpb=935.2, bsz=31.7, num_updates=3400, lr=8.5e-06, gnorm=7.452, loss_scale=4096, train_wall=20, gb_free=11.5, wall=627]2023-06-09 11:37:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  25% 391/1584 [01:11<02:52,  6.90it/s, loss=1.424, nll_loss=0.049, accuracy=50.6, wps=5999.3, ups=6.5, wpb=923.5, bsz=32, num_updates=3500, lr=8.75e-06, gnorm=6.973, loss_scale=4096, train_wall=15, gb_free=11.9, wall=642]2023-06-09 11:37:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  33% 523/1584 [01:32<02:52,  6.15it/s, loss=1.434, nll_loss=0.049, accuracy=51.3, wps=5929.3, ups=6.45, wpb=919, bsz=31.5, num_updates=3600, lr=9e-06, gnorm=6.371, loss_scale=4096, train_wall=15, gb_free=11.3, wall=658]2023-06-09 11:38:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  42% 658/1584 [01:53<02:23,  6.46it/s, loss=1.397, nll_loss=0.047, accuracy=52.8, wps=5787.3, ups=6.26, wpb=924.8, bsz=31.4, num_updates=3800, lr=9.5e-06, gnorm=7.303, loss_scale=4096, train_wall=15, gb_free=11.9, wall=690]2023-06-09 11:38:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  50% 790/1584 [02:14<02:20,  5.66it/s, loss=1.431, nll_loss=0.049, accuracy=51.6, wps=5740.7, ups=6.3, wpb=911.9, bsz=31.4, num_updates=3900, lr=9.75e-06, gnorm=7.363, loss_scale=4096, train_wall=15, gb_free=11.4, wall=706]2023-06-09 11:38:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  58% 919/1584 [02:34<01:43,  6.40it/s, loss=1.419, nll_loss=0.049, accuracy=51.1, wps=6003.6, ups=6.54, wpb=918, bsz=31.7, num_updates=4000, lr=1e-05, gnorm=7.041, loss_scale=4096, train_wall=15, gb_free=11.9, wall=721]2023-06-09 11:39:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  67% 1055/1584 [02:54<01:21,  6.51it/s, loss=1.368, nll_loss=0.047, accuracy=54.7, wps=6163.7, ups=6.65, wpb=926.3, bsz=31.8, num_updates=4200, lr=9.759e-06, gnorm=7.289, loss_scale=8192, train_wall=15, gb_free=11.7, wall=752]2023-06-09 11:39:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  69% 1100/1584 [03:01<01:05,  7.42it/s, loss=1.368, nll_loss=0.047, accuracy=54.7, wps=6163.7, ups=6.65, wpb=926.3, bsz=31.8, num_updates=4200, lr=9.759e-06, gnorm=7.289, loss_scale=8192, train_wall=15, gb_free=11.7, wall=752]2023-06-09 11:39:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  86% 1368/1584 [03:43<00:35,  6.10it/s, loss=1.374, nll_loss=0.047, accuracy=54, wps=5915.6, ups=6.45, wpb=917.8, bsz=31.1, num_updates=4500, lr=9.42809e-06, gnorm=7.141, loss_scale=4096, train_wall=15, gb_free=11.8, wall=799]2023-06-09 11:40:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  95% 1500/1584 [04:03<00:13,  6.31it/s, loss=1.345, nll_loss=0.046, accuracy=55.7, wps=5810.6, ups=6.33, wpb=918, bsz=31.5, num_updates=4600, lr=9.32505e-06, gnorm=7.074, loss_scale=4096, train_wall=15, gb_free=11.6, wall=814]2023-06-09 11:40:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003: 100% 1583/1584 [04:16<00:00,  6.56it/s, loss=1.347, nll_loss=0.046, accuracy=55.4, wps=5951.3, ups=6.46, wpb=921.3, bsz=31.4, num_updates=4700, lr=9.22531e-06, gnorm=7.649, loss_scale=4096, train_wall=15, gb_free=11.6, wall=830]2023-06-09 11:40:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 11:40:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 17.40it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   8% 5/62 [00:00<00:02, 20.83it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  13% 8/62 [00:00<00:02, 21.76it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  18% 11/62 [00:00<00:02, 22.81it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  23% 14/62 [00:00<00:01, 24.27it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  27% 17/62 [00:00<00:01, 25.05it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  32% 20/62 [00:00<00:01, 25.83it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  37% 23/62 [00:00<00:01, 26.94it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  42% 26/62 [00:01<00:01, 27.38it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  47% 29/62 [00:01<00:01, 25.83it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  52% 32/62 [00:01<00:01, 26.32it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  56% 35/62 [00:01<00:01, 26.38it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  61% 38/62 [00:01<00:00, 26.19it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  66% 41/62 [00:01<00:00, 25.92it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  71% 44/62 [00:01<00:00, 26.29it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  77% 48/62 [00:01<00:00, 27.28it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  84% 52/62 [00:01<00:00, 29.20it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  90% 56/62 [00:02<00:00, 30.01it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  95% 59/62 [00:02<00:00, 29.80it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset: 100% 62/62 [00:02<00:00, 29.57it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 11:40:50 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 1.592 | nll_loss 0.054 | accuracy 45.1 | wps 24957.9 | wpb 931.1 | bsz 31.8 | num_updates 4729 | best_accuracy 45.1\n","2023-06-09 11:40:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4729 updates\n","2023-06-09 11:40:50 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/uda/checkpoint_best.pt\n","2023-06-09 11:41:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/uda/checkpoint_best.pt\n","2023-06-09 11:41:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/uda/checkpoint_best.pt (epoch 3 @ 4729 updates, score 45.1) (writing took 48.07974532099979 seconds)\n","2023-06-09 11:41:38 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2023-06-09 11:41:38 | INFO | train | epoch 003 | loss 1.397 | nll_loss 0.048 | accuracy 53 | wps 4720.8 | ups 5.11 | wpb 923 | bsz 31.6 | num_updates 4729 | lr 9.19698e-06 | gnorm 7.166 | loss_scale 4096 | train_wall 248 | gb_free 11.7 | wall 885\n","2023-06-09 11:41:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 11:41:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1584\n","epoch 004:   0% 0/1584 [00:00<?, ?it/s]2023-06-09 11:41:38 | INFO | fairseq.trainer | begin training epoch 4\n","2023-06-09 11:41:38 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 004:   3% 46/1584 [00:09<05:32,  4.63it/s]2023-06-09 11:41:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 004:   5% 75/1584 [00:16<04:32,  5.54it/s, loss=1.279, nll_loss=0.044, accuracy=59, wps=1288.8, ups=1.4, wpb=918.5, bsz=31.4, num_updates=4800, lr=9.12871e-06, gnorm=8.074, loss_scale=4096, train_wall=20, gb_free=11.7, wall=901]2023-06-09 11:41:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  15% 232/1584 [00:46<03:22,  6.67it/s, loss=1.26, nll_loss=0.044, accuracy=59.9, wps=4554.6, ups=4.96, wpb=918.2, bsz=31.7, num_updates=4900, lr=9.03508e-06, gnorm=8.587, loss_scale=2048, train_wall=19, gb_free=11.9, wall=921]2023-06-09 11:42:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  28% 438/1584 [01:19<02:46,  6.87it/s, loss=1.208, nll_loss=0.042, accuracy=61.2, wps=5981.4, ups=6.5, wpb=920.2, bsz=31.7, num_updates=5100, lr=8.85615e-06, gnorm=8.911, loss_scale=4096, train_wall=15, gb_free=11.9, wall=953]2023-06-09 11:42:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  37% 591/1584 [01:43<02:28,  6.70it/s, loss=1.2, nll_loss=0.04, accuracy=61.6, wps=5834.8, ups=6.32, wpb=923.4, bsz=31.1, num_updates=5300, lr=8.68744e-06, gnorm=9.91, loss_scale=4096, train_wall=15, gb_free=11.7, wall=985]2023-06-09 11:43:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  51% 811/1584 [02:16<02:05,  6.14it/s, loss=1.213, nll_loss=0.042, accuracy=62.7, wps=5903.1, ups=6.46, wpb=914.5, bsz=31.5, num_updates=5500, lr=8.52803e-06, gnorm=10.223, loss_scale=4096, train_wall=15, gb_free=11.9, wall=1016]2023-06-09 11:43:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  64% 1012/1584 [02:48<01:26,  6.61it/s, loss=1.16, nll_loss=0.039, accuracy=64.1, wps=5939.7, ups=6.29, wpb=943.9, bsz=31.7, num_updates=5700, lr=8.37708e-06, gnorm=9.906, loss_scale=4096, train_wall=15, gb_free=11.9, wall=1048]2023-06-09 11:44:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  76% 1206/1584 [03:18<00:57,  6.60it/s, loss=1.159, nll_loss=0.04, accuracy=64.5, wps=6062.1, ups=6.55, wpb=925.6, bsz=31.8, num_updates=5900, lr=8.23387e-06, gnorm=9.413, loss_scale=4096, train_wall=15, gb_free=11.7, wall=1079]2023-06-09 11:44:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  88% 1393/1584 [03:47<00:29,  6.48it/s, loss=1.138, nll_loss=0.039, accuracy=65.8, wps=5960.4, ups=6.44, wpb=925.4, bsz=31.6, num_updates=6100, lr=8.09776e-06, gnorm=10.461, loss_scale=4096, train_wall=15, gb_free=10.8, wall=1110]2023-06-09 11:45:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  97% 1542/1584 [04:10<00:06,  6.97it/s, loss=1.158, nll_loss=0.04, accuracy=64.3, wps=5855.5, ups=6.47, wpb=904.5, bsz=31.4, num_updates=6200, lr=8.03219e-06, gnorm=10.47, loss_scale=2048, train_wall=15, gb_free=11.6, wall=1125]2023-06-09 11:45:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004: 100% 1583/1584 [04:16<00:00,  6.92it/s, loss=1.118, nll_loss=0.039, accuracy=66.4, wps=6053.4, ups=6.6, wpb=917.5, bsz=31.8, num_updates=6300, lr=7.96819e-06, gnorm=9.745, loss_scale=2048, train_wall=15, gb_free=11.7, wall=1141]2023-06-09 11:45:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 11:45:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 18.45it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   6% 4/62 [00:00<00:03, 18.62it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  11% 7/62 [00:00<00:02, 20.44it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  16% 10/62 [00:00<00:02, 21.79it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  21% 13/62 [00:00<00:02, 23.89it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  26% 16/62 [00:00<00:01, 24.15it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  31% 19/62 [00:00<00:01, 23.20it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  35% 22/62 [00:00<00:01, 22.75it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  40% 25/62 [00:01<00:01, 23.36it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  45% 28/62 [00:01<00:01, 22.00it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  50% 31/62 [00:01<00:01, 22.16it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  55% 34/62 [00:01<00:01, 21.66it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  60% 37/62 [00:01<00:01, 21.42it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  65% 40/62 [00:01<00:01, 20.85it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  69% 43/62 [00:01<00:00, 21.40it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  74% 46/62 [00:02<00:00, 22.41it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  79% 49/62 [00:02<00:00, 22.41it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  85% 53/62 [00:02<00:00, 25.20it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  90% 56/62 [00:02<00:00, 26.11it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  95% 59/62 [00:02<00:00, 26.53it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset: 100% 62/62 [00:02<00:00, 26.54it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 11:45:57 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 1.773 | nll_loss 0.061 | accuracy 44.8 | wps 21666.7 | wpb 931.1 | bsz 31.8 | num_updates 6303 | best_accuracy 45.1\n","2023-06-09 11:45:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6303 updates\n","2023-06-09 11:45:57 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/uda/checkpoint_last.pt\n","2023-06-09 11:46:05 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/uda/checkpoint_last.pt\n","2023-06-09 11:46:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/uda/checkpoint_last.pt (epoch 4 @ 6303 updates, score 44.8) (writing took 8.208849399999963 seconds)\n","2023-06-09 11:46:05 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2023-06-09 11:46:05 | INFO | train | epoch 004 | loss 1.184 | nll_loss 0.04 | accuracy 63.1 | wps 5438.3 | ups 5.89 | wpb 923.1 | bsz 31.6 | num_updates 6303 | lr 7.96629e-06 | gnorm 9.628 | loss_scale 2048 | train_wall 248 | gb_free 11.5 | wall 1152\n","2023-06-09 11:46:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 11:46:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1584\n","epoch 005:   0% 0/1584 [00:00<?, ?it/s]2023-06-09 11:46:05 | INFO | fairseq.trainer | begin training epoch 5\n","2023-06-09 11:46:05 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 005:   6% 91/1584 [00:15<04:12,  5.90it/s]2023-06-09 11:46:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  15% 230/1584 [00:42<03:23,  6.65it/s, loss=1.017, nll_loss=0.035, accuracy=69.9, wps=4663.7, ups=5.05, wpb=923.4, bsz=32, num_updates=6500, lr=7.84465e-06, gnorm=11.691, loss_scale=2048, train_wall=19, gb_free=11.8, wall=1189]2023-06-09 11:46:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  26% 409/1584 [01:11<02:45,  7.09it/s, loss=1.021, nll_loss=0.034, accuracy=70.2, wps=5791.6, ups=6.22, wpb=931.5, bsz=31.4, num_updates=6700, lr=7.72667e-06, gnorm=12.093, loss_scale=4096, train_wall=16, gb_free=11.9, wall=1222]2023-06-09 11:47:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  35% 558/1584 [01:34<02:37,  6.52it/s, loss=1.002, nll_loss=0.035, accuracy=70.2, wps=5836.7, ups=6.39, wpb=913.3, bsz=31.7, num_updates=6800, lr=7.66965e-06, gnorm=11.814, loss_scale=2048, train_wall=15, gb_free=11.8, wall=1238]2023-06-09 11:47:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  47% 737/1584 [02:02<02:11,  6.44it/s, loss=1.007, nll_loss=0.034, accuracy=70, wps=6001, ups=6.41, wpb=936.9, bsz=31.9, num_updates=7000, lr=7.55929e-06, gnorm=11.52, loss_scale=4096, train_wall=15, gb_free=11.9, wall=1269]2023-06-09 11:48:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  55% 877/1584 [02:24<01:54,  6.20it/s, loss=1.003, nll_loss=0.034, accuracy=70.3, wps=5774.4, ups=6.25, wpb=923.2, bsz=31.5, num_updates=7100, lr=7.50587e-06, gnorm=11.843, loss_scale=2048, train_wall=16, gb_free=11.9, wall=1285]2023-06-09 11:48:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  65% 1022/1584 [02:47<01:51,  5.03it/s, loss=0.99, nll_loss=0.034, accuracy=70.4, wps=5981.4, ups=6.5, wpb=920.8, bsz=31.6, num_updates=7300, lr=7.40233e-06, gnorm=11.533, loss_scale=2048, train_wall=15, gb_free=11.7, wall=1316]2023-06-09 11:48:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  81% 1282/1584 [03:27<00:42,  7.06it/s, loss=0.94, nll_loss=0.032, accuracy=72.4, wps=5919, ups=6.39, wpb=925.6, bsz=31.4, num_updates=7500, lr=7.30297e-06, gnorm=11.917, loss_scale=4096, train_wall=15, gb_free=11.8, wall=1348]2023-06-09 11:49:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 005:  83% 1312/1584 [03:32<00:46,  5.86it/s, loss=0.997, nll_loss=0.034, accuracy=70.6, wps=5925.8, ups=6.41, wpb=924.6, bsz=31.4, num_updates=7600, lr=7.25476e-06, gnorm=11.405, loss_scale=4096, train_wall=15, gb_free=11.6, wall=1363]2023-06-09 11:49:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  91% 1444/1584 [03:52<00:22,  6.29it/s, loss=0.969, nll_loss=0.033, accuracy=70.5, wps=5780, ups=6.3, wpb=917.7, bsz=31.2, num_updates=7700, lr=7.2075e-06, gnorm=12.413, loss_scale=2048, train_wall=15, gb_free=11.7, wall=1379]2023-06-09 11:49:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005: 100% 1583/1584 [04:14<00:00,  7.04it/s, loss=0.991, nll_loss=0.034, accuracy=70.3, wps=5890.3, ups=6.39, wpb=921.9, bsz=31.9, num_updates=7800, lr=7.16115e-06, gnorm=12.078, loss_scale=2048, train_wall=15, gb_free=11.7, wall=1395]2023-06-09 11:50:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 11:50:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 005 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 15.05it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   6% 4/62 [00:00<00:03, 16.09it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  11% 7/62 [00:00<00:03, 18.08it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  15% 9/62 [00:00<00:03, 17.22it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  18% 11/62 [00:00<00:02, 17.93it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  23% 14/62 [00:00<00:02, 19.33it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  26% 16/62 [00:00<00:02, 19.13it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  29% 18/62 [00:00<00:02, 18.99it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  34% 21/62 [00:01<00:02, 19.85it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  39% 24/62 [00:01<00:01, 20.52it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  44% 27/62 [00:01<00:01, 20.40it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  48% 30/62 [00:01<00:01, 20.31it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  53% 33/62 [00:01<00:01, 20.55it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  58% 36/62 [00:01<00:01, 20.10it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  63% 39/62 [00:02<00:01, 20.00it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  68% 42/62 [00:02<00:00, 20.09it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  73% 45/62 [00:02<00:00, 21.14it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  77% 48/62 [00:02<00:00, 22.65it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  84% 52/62 [00:02<00:00, 25.62it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  90% 56/62 [00:02<00:00, 27.56it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  95% 59/62 [00:02<00:00, 27.95it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset: 100% 62/62 [00:02<00:00, 28.06it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 11:50:23 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 1.995 | nll_loss 0.068 | accuracy 44.8 | wps 20460.2 | wpb 931.1 | bsz 31.8 | num_updates 7877 | best_accuracy 45.1\n","2023-06-09 11:50:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7877 updates\n","2023-06-09 11:50:23 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/uda/checkpoint_last.pt\n","2023-06-09 11:50:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/uda/checkpoint_last.pt\n","2023-06-09 11:50:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/uda/checkpoint_last.pt (epoch 5 @ 7877 updates, score 44.8) (writing took 7.627423953999823 seconds)\n","2023-06-09 11:50:30 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n","2023-06-09 11:50:30 | INFO | train | epoch 005 | loss 0.992 | nll_loss 0.034 | accuracy 70.8 | wps 5486.7 | ups 5.94 | wpb 923.5 | bsz 31.6 | num_updates 7877 | lr 7.12606e-06 | gnorm 11.816 | loss_scale 4096 | train_wall 246 | gb_free 10.6 | wall 1417\n","2023-06-09 11:50:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 11:50:30 | INFO | fairseq_cli.train | done training in 1413.3 seconds\n"]}]},{"cell_type":"code","source":["from fairseq.models.roberta import RobertaModel\n","roberta = RobertaModel.from_pretrained(\n","    '/content/drive/MyDrive/NLP/checkpoints/fiction/uda',\n","    checkpoint_file='checkpoint_best.pt',\n","    data_name_or_path='/content/drive/MyDrive/NLP/mnli/fiction/orig/bin_uda'\n",")\n","roberta.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eFLi0adgsMvE","executionInfo":{"status":"ok","timestamp":1686492218521,"user_tz":-120,"elapsed":42400,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}},"outputId":"101a19ad-31b9-4ac4-8ec0-a8a3ac61fcb6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RobertaHubInterface(\n","  (model): RobertaModel(\n","    (encoder): RobertaEncoder(\n","      (sentence_encoder): TransformerEncoder(\n","        (dropout_module): FairseqDropout()\n","        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","        (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n","        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (layers): ModuleList(\n","          (0-11): 12 x TransformerEncoderLayerBase(\n","            (self_attn): MultiheadAttention(\n","              (dropout_module): FairseqDropout()\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout_module): FairseqDropout()\n","            (activation_dropout_module): FairseqDropout()\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","      )\n","      (lm_head): RobertaLMHead(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (classification_heads): ModuleDict(\n","      (mnli): RobertaClassificationHead(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (out_proj): Linear(in_features=768, out_features=3, bias=True)\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["genre = ['slate','fiction','telephone','travel','government']\n","for g in genre:\n","  input0 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input0', \"r\")\n","  input1 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input1', \"r\")\n","  label = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.label', \"r\")\n","\n","  accuracy = 0\n","  total = 0\n","  for (x1, x2, y) in zip(input0, input1, label):\n","    tokens = roberta.encode(x1, x2)\n","    idx = roberta.predict('mnli', tokens).argmax().item()\n","    dictionary = roberta.task.label_dictionary\n","    pred = dictionary[idx + dictionary.nspecial]\n","    total = total + 1\n","    if  (pred == y.strip()) :\n","      accuracy = accuracy + 1\n","\n","\n","  print(g,\": \",accuracy)\n","  print(g,\": \",total)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rtiv-ZHFrRGy","executionInfo":{"status":"ok","timestamp":1686318189236,"user_tz":-120,"elapsed":1503071,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}},"outputId":"36f1a04b-735a-4fb7-e7a8-cb80444d7710"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["slate :  785\n","slate :  2000\n","fiction :  867\n","fiction :  2000\n","telephone :  829\n","telephone :  2000\n","travel :  793\n","travel :  2000\n","government :  821\n","government :  2000\n"]}]},{"cell_type":"code","source":["genre = ['verbatim','facetoface','oup','nineeleven','letters']\n","for g in genre:\n","  input0 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input0', \"r\")\n","  input1 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input1', \"r\")\n","  label = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.label', \"r\")\n","\n","  accuracy = 0\n","  total = 0\n","  for (x1, x2, y) in zip(input0, input1, label):\n","    tokens = roberta.encode(x1, x2)\n","    idx = roberta.predict('mnli', tokens).argmax().item()\n","    dictionary = roberta.task.label_dictionary\n","    pred = dictionary[idx + dictionary.nspecial]\n","    total = total + 1\n","    if  (pred == y.strip()) :\n","      accuracy = accuracy + 1\n","\n","  print(g,\": \",accuracy)\n","  print(g,\": \",total)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vBgE7N1RocBJ","executionInfo":{"status":"ok","timestamp":1686493196117,"user_tz":-120,"elapsed":977612,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}},"outputId":"b60f20c1-274a-4b91-b5f7-389a1dabfc76"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["verbatim :  428\n","verbatim :  1000\n","facetoface :  424\n","facetoface :  1000\n","oup :  399\n","oup :  1000\n","nineeleven :  420\n","nineeleven :  1000\n","letters :  413\n","letters :  1000\n"]}]},{"cell_type":"markdown","source":["# SSMBA"],"metadata":{"id":"umQRVFHni6HK"}},{"cell_type":"code","source":["import tensorflow as tf\n","with tf.device('/device:GPU:0'):\n","    ! CUDA_VISIBLE_DEVICES=0 fairseq-train /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_ssmba2 \\\n","          --save-dir /content/drive/MyDrive/NLP/checkpoints/fiction/ssmba \\\n","          --reset-optimizer  \\\n","          --reset-dataloader  \\\n","          --reset-meters  \\\n","          --best-checkpoint-metric accuracy  \\\n","          --maximize-best-checkpoint-metric  \\\n","          --no-epoch-checkpoints \\\n","          --find-unused-parameters \\\n","          --distributed-world-size 1 \\\n","          --task sentence_prediction  \\\n","          --num-classes 3  \\\n","          --init-token 0  \\\n","          --separator-token 2   \\\n","          --max-positions 512  \\\n","          --shorten-method \"truncate\"  \\\n","          --arch roberta \\\n","          --dropout 0.1  \\\n","          --attention-dropout 0.1  \\\n","          --weight-decay 0.1  \\\n","          --criterion sentence_prediction  \\\n","          --classification-head-name 'mnli' \\\n","          --optimizer adam  \\\n","          --adam-betas '(0.9, 0.98)'  \\\n","          --adam-eps 1e-06  \\\n","          --clip-norm 0.0  \\\n","          --lr-scheduler inverse_sqrt  \\\n","          --lr 1e-05 \\\n","          --fp16  \\\n","          --fp16-init-scale 4  \\\n","          --threshold-loss-scale 1  \\\n","          --fp16-scale-window 128  \\\n","          --batch-size 32  \\\n","          --required-batch-size-multiple 1  \\\n","          --max-tokens 4400 \\\n","          --update-freq 1  \\\n","          --max-update 123873 \\\n","          --max-epoch 5 \\\n","          --seed 100"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_jowZhVmq_Mq","executionInfo":{"status":"ok","timestamp":1686324713695,"user_tz":-120,"elapsed":1413714,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}},"outputId":"9d13f1eb-4761-4887-fcf9-cbb29f6db5cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-09 15:08:32.890071: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2023-06-09 15:08:34 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n","2023-06-09 15:08:39 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 100, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4400, 'batch_size': 32, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 123873, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/drive/MyDrive/NLP/checkpoints/fiction/ssmba', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=100, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=1.0, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4400, batch_size=32, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4400, batch_size_valid=32, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta', max_epoch=5, max_update=123873, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/content/drive/MyDrive/NLP/checkpoints/fiction/ssmba', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='/content/drive/MyDrive/NLP/mnli/fiction/orig/bin_ssmba2', num_classes=3, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, d2v2_multi=False, classification_head_name='mnli', regression_target=False, report_mcc=False, report_acc_and_f1=False, report_pearson_and_spearman=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.1, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, max_positions=512, dropout=0.1, attention_dropout=0.1, no_seed_provided=False, encoder_layers=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_attention_heads=12, activation_dropout=0.0, pooler_dropout=0.0, max_source_positions=512, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta'), 'task': {'_name': 'sentence_prediction', 'data': '/content/drive/MyDrive/NLP/mnli/fiction/orig/bin_ssmba2', 'num_classes': 3, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 512, 'regression_target': False, 'classification_head_name': 'mnli', 'seed': 100, 'd2v2_multi': False}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'mnli', 'regression_target': False, 'report_mcc': False, 'report_acc_and_f1': False, 'report_pearson_and_spearman': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2023-06-09 15:08:39 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n","2023-06-09 15:08:39 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n","2023-06-09 15:08:43 | INFO | fairseq_cli.train | RobertaModel(\n","  (encoder): RobertaEncoder(\n","    (sentence_encoder): TransformerEncoder(\n","      (dropout_module): FairseqDropout()\n","      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","      (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (layers): ModuleList(\n","        (0-11): 12 x TransformerEncoderLayerBase(\n","          (self_attn): MultiheadAttention(\n","            (dropout_module): FairseqDropout()\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout_module): FairseqDropout()\n","          (activation_dropout_module): FairseqDropout()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (lm_head): RobertaLMHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (classification_heads): ModuleDict(\n","    (mnli): RobertaClassificationHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","      (out_proj): Linear(in_features=768, out_features=3, bias=True)\n","    )\n","  )\n",")\n","2023-06-09 15:08:43 | INFO | fairseq_cli.train | task: SentencePredictionTask\n","2023-06-09 15:08:43 | INFO | fairseq_cli.train | model: RobertaModel\n","2023-06-09 15:08:43 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n","2023-06-09 15:08:43 | INFO | fairseq_cli.train | num. shared model params: 125,289,564 (num. trained: 125,289,564)\n","2023-06-09 15:08:43 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n","2023-06-09 15:08:44 | INFO | fairseq.data.data_utils | loaded 1,973 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_ssmba2/input0/valid\n","2023-06-09 15:08:44 | INFO | fairseq.data.data_utils | loaded 1,973 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_ssmba2/input1/valid\n","2023-06-09 15:08:45 | INFO | fairseq.data.data_utils | loaded 1,973 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_ssmba2/label/valid\n","2023-06-09 15:08:45 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 1973\n","2023-06-09 15:08:51 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n","2023-06-09 15:08:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-09 15:08:51 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n","2023-06-09 15:08:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-09 15:08:51 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2023-06-09 15:08:51 | INFO | fairseq_cli.train | max tokens per device = 4400 and max sentences per device = 32\n","2023-06-09 15:08:51 | INFO | fairseq.trainer | Preparing to load checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/ssmba/checkpoint_last.pt\n","2023-06-09 15:08:51 | INFO | fairseq.trainer | No existing checkpoint found /content/drive/MyDrive/NLP/checkpoints/fiction/ssmba/checkpoint_last.pt\n","2023-06-09 15:08:51 | INFO | fairseq.trainer | loading train data for epoch 1\n","2023-06-09 15:08:52 | INFO | fairseq.data.data_utils | loaded 46,793 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_ssmba2/input0/train\n","2023-06-09 15:08:53 | INFO | fairseq.data.data_utils | loaded 46,793 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_ssmba2/input1/train\n","2023-06-09 15:08:53 | INFO | fairseq.data.data_utils | loaded 46,793 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_ssmba2/label/train\n","2023-06-09 15:08:53 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 46793\n","2023-06-09 15:08:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 15:08:53 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-09 15:08:53 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-09 15:08:53 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-09 15:08:54 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n","2023-06-09 15:08:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 15:08:54 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-09 15:08:54 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-09 15:08:54 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-09 15:08:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1464\n","epoch 001:   0% 0/1464 [00:00<?, ?it/s]2023-06-09 15:08:55 | INFO | fairseq.trainer | begin training epoch 1\n","2023-06-09 15:08:55 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 001:  98% 1441/1464 [03:46<00:03,  6.09it/s, loss=1.568, nll_loss=0.051, accuracy=37.8, wps=6227.8, ups=6.36, wpb=979.1, bsz=32, num_updates=1400, lr=3.5e-06, gnorm=3.98, loss_scale=4096, train_wall=15, gb_free=11.9, wall=224]2023-06-09 15:12:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 001: 100% 1463/1464 [03:49<00:00,  6.71it/s, loss=1.568, nll_loss=0.051, accuracy=37.8, wps=6227.8, ups=6.36, wpb=979.1, bsz=32, num_updates=1400, lr=3.5e-06, gnorm=3.98, loss_scale=4096, train_wall=15, gb_free=11.9, wall=224]2023-06-09 15:12:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 15:12:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 17.63it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   8% 5/62 [00:00<00:02, 20.26it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  13% 8/62 [00:00<00:02, 22.18it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  18% 11/62 [00:00<00:02, 23.60it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  23% 14/62 [00:00<00:01, 25.37it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  27% 17/62 [00:00<00:01, 26.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  32% 20/62 [00:00<00:01, 27.21it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  39% 24/62 [00:00<00:01, 28.51it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  44% 27/62 [00:01<00:01, 27.98it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  48% 30/62 [00:01<00:01, 28.23it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  53% 33/62 [00:01<00:01, 28.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  58% 36/62 [00:01<00:00, 27.18it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  63% 39/62 [00:01<00:00, 27.24it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  68% 42/62 [00:01<00:00, 27.36it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  73% 45/62 [00:01<00:00, 27.80it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  77% 48/62 [00:01<00:00, 28.19it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  84% 52/62 [00:01<00:00, 30.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  90% 56/62 [00:02<00:00, 30.94it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  97% 60/62 [00:02<00:00, 29.86it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 15:12:47 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.583 | nll_loss 0.054 | accuracy 37.8 | wps 25933.8 | wpb 931.1 | bsz 31.8 | num_updates 1463\n","2023-06-09 15:12:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1463 updates\n","2023-06-09 15:12:47 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/ssmba/checkpoint_best.pt\n","2023-06-09 15:12:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/ssmba/checkpoint_best.pt\n","2023-06-09 15:13:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/ssmba/checkpoint_best.pt (epoch 1 @ 1463 updates, score 37.8) (writing took 19.637036434000038 seconds)\n","2023-06-09 15:13:06 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2023-06-09 15:13:06 | INFO | train | epoch 001 | loss 1.583 | nll_loss 0.052 | accuracy 36.1 | wps 5723.5 | ups 5.89 | wpb 972.2 | bsz 32 | num_updates 1463 | lr 3.6575e-06 | gnorm 4.456 | loss_scale 4096 | train_wall 222 | gb_free 11.6 | wall 256\n","2023-06-09 15:13:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 15:13:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1464\n","epoch 002:   0% 0/1464 [00:00<?, ?it/s]2023-06-09 15:13:06 | INFO | fairseq.trainer | begin training epoch 2\n","2023-06-09 15:13:06 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 002:   9% 138/1464 [00:31<04:18,  5.13it/s, loss=1.559, nll_loss=0.051, accuracy=39.2, wps=4338, ups=4.46, wpb=971.8, bsz=31.9, num_updates=1600, lr=4e-06, gnorm=4.296, loss_scale=8192, train_wall=21, gb_free=11.8, wall=287]2023-06-09 15:13:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  26% 386/1464 [01:15<02:38,  6.82it/s, loss=1.537, nll_loss=0.051, accuracy=42.2, wps=5811.2, ups=5.97, wpb=973.1, bsz=32, num_updates=1800, lr=4.5e-06, gnorm=4.31, loss_scale=8192, train_wall=16, gb_free=11.8, wall=322]2023-06-09 15:14:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  36% 520/1464 [01:36<02:35,  6.08it/s, loss=1.544, nll_loss=0.051, accuracy=42.3, wps=5473, ups=5.62, wpb=973.9, bsz=31.9, num_updates=1900, lr=4.75e-06, gnorm=4.736, loss_scale=4096, train_wall=17, gb_free=11.9, wall=340]2023-06-09 15:14:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  48% 705/1464 [02:05<01:55,  6.55it/s, loss=1.52, nll_loss=0.05, accuracy=44.2, wps=6230.7, ups=6.4, wpb=973, bsz=32, num_updates=2100, lr=5.25e-06, gnorm=4.361, loss_scale=4096, train_wall=15, gb_free=11.8, wall=371]2023-06-09 15:15:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  58% 852/1464 [02:28<01:39,  6.14it/s, loss=1.525, nll_loss=0.05, accuracy=42.2, wps=6320.3, ups=6.49, wpb=973.6, bsz=32, num_updates=2300, lr=5.75e-06, gnorm=4.969, loss_scale=8192, train_wall=15, gb_free=11.7, wall=403]2023-06-09 15:15:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  70% 1026/1464 [02:56<01:14,  5.86it/s, loss=1.546, nll_loss=0.051, accuracy=41.7, wps=6047.3, ups=6.19, wpb=977, bsz=31.9, num_updates=2400, lr=6e-06, gnorm=5.604, loss_scale=4096, train_wall=16, gb_free=11.9, wall=419]2023-06-09 15:16:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  79% 1156/1464 [03:16<00:46,  6.57it/s, loss=1.512, nll_loss=0.05, accuracy=44.5, wps=6221, ups=6.46, wpb=963.3, bsz=31.9, num_updates=2600, lr=6.5e-06, gnorm=5.705, loss_scale=4096, train_wall=15, gb_free=11.9, wall=450]2023-06-09 15:16:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  89% 1302/1464 [03:39<00:26,  6.12it/s, loss=1.505, nll_loss=0.05, accuracy=45.7, wps=6151.8, ups=6.39, wpb=962.5, bsz=32, num_updates=2700, lr=6.75e-06, gnorm=6.07, loss_scale=4096, train_wall=15, gb_free=11.7, wall=466]2023-06-09 15:16:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  98% 1437/1464 [04:00<00:04,  6.36it/s, loss=1.496, nll_loss=0.049, accuracy=46.6, wps=5994.7, ups=6.12, wpb=979.5, bsz=31.9, num_updates=2800, lr=7e-06, gnorm=5.983, loss_scale=4096, train_wall=16, gb_free=11.9, wall=482]2023-06-09 15:17:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002: 100% 1463/1464 [04:04<00:00,  6.83it/s, loss=1.48, nll_loss=0.049, accuracy=47.3, wps=6203.3, ups=6.42, wpb=966.8, bsz=32, num_updates=2900, lr=7.25e-06, gnorm=6.264, loss_scale=4096, train_wall=15, gb_free=11.3, wall=498]2023-06-09 15:17:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 15:17:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 16.62it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   8% 5/62 [00:00<00:02, 20.23it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  13% 8/62 [00:00<00:02, 21.76it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  18% 11/62 [00:00<00:02, 23.14it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  23% 14/62 [00:00<00:01, 24.64it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  27% 17/62 [00:00<00:01, 25.55it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  32% 20/62 [00:00<00:01, 26.57it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  39% 24/62 [00:00<00:01, 28.06it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  44% 27/62 [00:01<00:01, 27.36it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  48% 30/62 [00:01<00:01, 27.60it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  53% 33/62 [00:01<00:01, 27.71it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  58% 36/62 [00:01<00:00, 26.87it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  63% 39/62 [00:01<00:00, 26.96it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  68% 42/62 [00:01<00:00, 27.43it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  73% 45/62 [00:01<00:00, 27.96it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  77% 48/62 [00:01<00:00, 28.42it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  84% 52/62 [00:01<00:00, 30.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  90% 56/62 [00:02<00:00, 30.87it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  97% 60/62 [00:02<00:00, 29.27it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 15:17:14 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 1.59 | nll_loss 0.054 | accuracy 40.4 | wps 25652.1 | wpb 931.1 | bsz 31.8 | num_updates 2918 | best_accuracy 40.4\n","2023-06-09 15:17:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2918 updates\n","2023-06-09 15:17:14 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/ssmba/checkpoint_best.pt\n","2023-06-09 15:17:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/ssmba/checkpoint_best.pt\n","2023-06-09 15:17:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/ssmba/checkpoint_best.pt (epoch 2 @ 2918 updates, score 40.4) (writing took 28.312609791999876 seconds)\n","2023-06-09 15:17:42 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2023-06-09 15:17:42 | INFO | train | epoch 002 | loss 1.527 | nll_loss 0.05 | accuracy 43.3 | wps 5132.4 | ups 5.28 | wpb 971.8 | bsz 32 | num_updates 2918 | lr 7.295e-06 | gnorm 5.118 | loss_scale 4096 | train_wall 236 | gb_free 11.8 | wall 531\n","2023-06-09 15:17:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 15:17:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1464\n","epoch 003:   0% 0/1464 [00:00<?, ?it/s]2023-06-09 15:17:42 | INFO | fairseq.trainer | begin training epoch 3\n","2023-06-09 15:17:42 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 003:   7% 102/1464 [00:21<04:54,  4.63it/s, loss=1.447, nll_loss=0.047, accuracy=50.1, wps=1934.5, ups=1.97, wpb=981, bsz=32, num_updates=3000, lr=7.5e-06, gnorm=7.224, loss_scale=4096, train_wall=19, gb_free=11.3, wall=549]2023-06-09 15:18:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:   8% 123/1464 [00:25<04:01,  5.56it/s, loss=1.447, nll_loss=0.047, accuracy=50.1, wps=1934.5, ups=1.97, wpb=981, bsz=32, num_updates=3000, lr=7.5e-06, gnorm=7.224, loss_scale=4096, train_wall=19, gb_free=11.3, wall=549]2023-06-09 15:18:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  20% 296/1464 [00:55<03:33,  5.47it/s, loss=1.421, nll_loss=0.047, accuracy=51.8, wps=5732.3, ups=5.87, wpb=976.5, bsz=32, num_updates=3200, lr=8e-06, gnorm=7.352, loss_scale=4096, train_wall=16, gb_free=11.9, wall=585]2023-06-09 15:18:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  32% 465/1464 [01:24<02:26,  6.81it/s, loss=1.434, nll_loss=0.048, accuracy=50.2, wps=5197.6, ups=5.43, wpb=956.8, bsz=31.9, num_updates=3300, lr=8.25e-06, gnorm=7.946, loss_scale=2048, train_wall=18, gb_free=11.9, wall=603]2023-06-09 15:19:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  41% 604/1464 [01:47<02:05,  6.85it/s, loss=1.396, nll_loss=0.046, accuracy=53.8, wps=6057.4, ups=6.26, wpb=967.4, bsz=32, num_updates=3500, lr=8.75e-06, gnorm=8.809, loss_scale=2048, train_wall=15, gb_free=11, wall=635]2023-06-09 15:19:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  50% 738/1464 [02:07<01:57,  6.17it/s, loss=1.386, nll_loss=0.045, accuracy=52.9, wps=6276.7, ups=6.44, wpb=974.2, bsz=31.9, num_updates=3600, lr=9e-06, gnorm=8.562, loss_scale=2048, train_wall=15, gb_free=11.8, wall=651]2023-06-09 15:19:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  64% 944/1464 [02:39<01:13,  7.03it/s, loss=1.357, nll_loss=0.045, accuracy=55, wps=6368.1, ups=6.58, wpb=968.2, bsz=32, num_updates=3800, lr=9.5e-06, gnorm=9.363, loss_scale=4096, train_wall=15, gb_free=11.7, wall=682]2023-06-09 15:20:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  82% 1201/1464 [03:20<00:39,  6.58it/s, loss=1.328, nll_loss=0.044, accuracy=56.8, wps=6166.7, ups=6.41, wpb=961.7, bsz=31.9, num_updates=4100, lr=9.8773e-06, gnorm=8.522, loss_scale=4096, train_wall=15, gb_free=11.8, wall=729]2023-06-09 15:21:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 003:  87% 1278/1464 [03:32<00:35,  5.25it/s, loss=1.328, nll_loss=0.044, accuracy=56.8, wps=6166.7, ups=6.41, wpb=961.7, bsz=31.9, num_updates=4100, lr=9.8773e-06, gnorm=8.522, loss_scale=4096, train_wall=15, gb_free=11.8, wall=729]2023-06-09 15:21:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  99% 1450/1464 [04:00<00:02,  5.08it/s, loss=1.316, nll_loss=0.043, accuracy=57, wps=6175.4, ups=6.31, wpb=978.9, bsz=32, num_updates=4300, lr=9.64486e-06, gnorm=9.81, loss_scale=2048, train_wall=15, gb_free=11.8, wall=762]2023-06-09 15:21:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003: 100% 1463/1464 [04:02<00:00,  6.14it/s, loss=1.316, nll_loss=0.043, accuracy=57, wps=6175.4, ups=6.31, wpb=978.9, bsz=32, num_updates=4300, lr=9.64486e-06, gnorm=9.81, loss_scale=2048, train_wall=15, gb_free=11.8, wall=762]2023-06-09 15:21:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 15:21:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   3% 2/62 [00:00<00:04, 13.97it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   6% 4/62 [00:00<00:04, 13.40it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  10% 6/62 [00:00<00:03, 15.49it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  13% 8/62 [00:00<00:03, 16.54it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  16% 10/62 [00:00<00:02, 17.63it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  21% 13/62 [00:00<00:02, 18.84it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  26% 16/62 [00:00<00:02, 19.40it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  29% 18/62 [00:01<00:02, 19.07it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  34% 21/62 [00:01<00:02, 19.82it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  39% 24/62 [00:01<00:01, 20.09it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  44% 27/62 [00:01<00:01, 20.61it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  48% 30/62 [00:01<00:01, 21.51it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  53% 33/62 [00:01<00:01, 22.14it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  58% 36/62 [00:01<00:01, 22.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  63% 39/62 [00:01<00:01, 22.53it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  68% 42/62 [00:02<00:00, 23.70it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  73% 45/62 [00:02<00:00, 24.97it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  77% 48/62 [00:02<00:00, 24.87it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  84% 52/62 [00:02<00:00, 27.58it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  90% 56/62 [00:02<00:00, 29.37it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  95% 59/62 [00:02<00:00, 29.43it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset: 100% 62/62 [00:02<00:00, 29.49it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 15:21:47 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 1.786 | nll_loss 0.061 | accuracy 41.1 | wps 21274.7 | wpb 931.1 | bsz 31.8 | num_updates 4372 | best_accuracy 41.1\n","2023-06-09 15:21:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4372 updates\n","2023-06-09 15:21:47 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/ssmba/checkpoint_best.pt\n","2023-06-09 15:21:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/ssmba/checkpoint_best.pt\n","2023-06-09 15:22:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/ssmba/checkpoint_best.pt (epoch 3 @ 4372 updates, score 41.1) (writing took 44.38678651499981 seconds)\n","2023-06-09 15:22:31 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2023-06-09 15:22:31 | INFO | train | epoch 003 | loss 1.369 | nll_loss 0.045 | accuracy 54.4 | wps 4882.8 | ups 5.02 | wpb 972.1 | bsz 32 | num_updates 4372 | lr 9.56511e-06 | gnorm 8.588 | loss_scale 2048 | train_wall 233 | gb_free 11.9 | wall 821\n","2023-06-09 15:22:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 15:22:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1464\n","epoch 004:   0% 0/1464 [00:00<?, ?it/s]2023-06-09 15:22:31 | INFO | fairseq.trainer | begin training epoch 4\n","2023-06-09 15:22:31 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 004:  12% 170/1464 [00:33<03:34,  6.04it/s, loss=1.202, nll_loss=0.039, accuracy=62.8, wps=4961.9, ups=5.08, wpb=976.1, bsz=32, num_updates=4500, lr=9.42809e-06, gnorm=11.176, loss_scale=4096, train_wall=19, gb_free=11.9, wall=846]2023-06-09 15:23:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  22% 318/1464 [00:56<03:03,  6.23it/s, loss=1.172, nll_loss=0.039, accuracy=63.4, wps=5695.6, ups=5.85, wpb=974.3, bsz=32, num_updates=4600, lr=9.32505e-06, gnorm=11.159, loss_scale=2048, train_wall=16, gb_free=11.8, wall=863]2023-06-09 15:23:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  34% 502/1464 [01:25<02:34,  6.24it/s, loss=1.223, nll_loss=0.041, accuracy=61.9, wps=6450.1, ups=6.69, wpb=964, bsz=32, num_updates=4800, lr=9.12871e-06, gnorm=12.266, loss_scale=2048, train_wall=14, gb_free=11.9, wall=895]2023-06-09 15:23:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  44% 642/1464 [01:47<02:22,  5.77it/s, loss=1.129, nll_loss=0.037, accuracy=65.4, wps=6169.3, ups=6.4, wpb=963.5, bsz=31.9, num_updates=5000, lr=8.94427e-06, gnorm=11.998, loss_scale=4096, train_wall=15, gb_free=11.9, wall=926]2023-06-09 15:24:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  53% 772/1464 [02:07<01:36,  7.19it/s, loss=1.162, nll_loss=0.038, accuracy=63.5, wps=6090.3, ups=6.24, wpb=975.8, bsz=32, num_updates=5100, lr=8.85615e-06, gnorm=11.933, loss_scale=2048, train_wall=15, gb_free=11.8, wall=942]2023-06-09 15:24:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  63% 924/1464 [02:30<01:31,  5.91it/s, loss=1.072, nll_loss=0.035, accuracy=67, wps=6242.8, ups=6.43, wpb=970.7, bsz=31.9, num_updates=5200, lr=8.77058e-06, gnorm=11.96, loss_scale=2048, train_wall=15, gb_free=11.7, wall=958]2023-06-09 15:25:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  73% 1067/1464 [02:53<01:02,  6.31it/s, loss=1.086, nll_loss=0.035, accuracy=66.9, wps=6065.8, ups=6.16, wpb=984, bsz=32, num_updates=5400, lr=8.60663e-06, gnorm=12.92, loss_scale=2048, train_wall=16, gb_free=11.5, wall=990]2023-06-09 15:25:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  82% 1201/1464 [03:15<00:42,  6.22it/s, loss=1.051, nll_loss=0.034, accuracy=68.2, wps=6104.1, ups=6.25, wpb=976.9, bsz=32, num_updates=5500, lr=8.52803e-06, gnorm=12.405, loss_scale=2048, train_wall=15, gb_free=11.8, wall=1006]2023-06-09 15:25:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  91% 1335/1464 [03:39<00:21,  6.04it/s, loss=1.039, nll_loss=0.034, accuracy=68.4, wps=5531.9, ups=5.65, wpb=979.6, bsz=32, num_updates=5600, lr=8.45154e-06, gnorm=12.842, loss_scale=2048, train_wall=17, gb_free=11.3, wall=1023]2023-06-09 15:26:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004: 100% 1463/1464 [03:59<00:00,  7.00it/s, loss=1.004, nll_loss=0.033, accuracy=70.8, wps=6217.6, ups=6.4, wpb=971.3, bsz=32, num_updates=5800, lr=8.30455e-06, gnorm=12.871, loss_scale=2048, train_wall=15, gb_free=11.8, wall=1056]2023-06-09 15:26:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 15:26:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 18.74it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   8% 5/62 [00:00<00:02, 19.79it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  13% 8/62 [00:00<00:02, 21.61it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  18% 11/62 [00:00<00:02, 22.65it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  23% 14/62 [00:00<00:01, 24.55it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  27% 17/62 [00:00<00:01, 25.70it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  32% 20/62 [00:00<00:01, 25.64it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  39% 24/62 [00:00<00:01, 27.49it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  44% 27/62 [00:01<00:01, 27.25it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  48% 30/62 [00:01<00:01, 27.21it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  53% 33/62 [00:01<00:01, 27.59it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  58% 36/62 [00:01<00:00, 27.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  63% 39/62 [00:01<00:00, 26.91it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  68% 42/62 [00:01<00:00, 26.85it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  73% 45/62 [00:01<00:00, 27.29it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  77% 48/62 [00:01<00:00, 27.57it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  84% 52/62 [00:01<00:00, 29.65it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  90% 56/62 [00:02<00:00, 30.81it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  97% 60/62 [00:02<00:00, 29.40it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 15:26:33 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 2.087 | nll_loss 0.071 | accuracy 42.2 | wps 25362.4 | wpb 931.1 | bsz 31.8 | num_updates 5827 | best_accuracy 42.2\n","2023-06-09 15:26:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5827 updates\n","2023-06-09 15:26:33 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/ssmba/checkpoint_best.pt\n","2023-06-09 15:26:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/ssmba/checkpoint_best.pt\n","2023-06-09 15:27:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/ssmba/checkpoint_best.pt (epoch 4 @ 5827 updates, score 42.2) (writing took 49.59944296999993 seconds)\n","2023-06-09 15:27:23 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2023-06-09 15:27:23 | INFO | train | epoch 004 | loss 1.113 | nll_loss 0.037 | accuracy 65.9 | wps 4854.2 | ups 4.99 | wpb 972.1 | bsz 32 | num_updates 5827 | lr 8.28529e-06 | gnorm 12.139 | loss_scale 4096 | train_wall 231 | gb_free 11.4 | wall 1112\n","2023-06-09 15:27:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 15:27:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1464\n","epoch 005:   0% 0/1464 [00:00<?, ?it/s]2023-06-09 15:27:23 | INFO | fairseq.trainer | begin training epoch 5\n","2023-06-09 15:27:23 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 005:   1% 13/1464 [00:03<04:58,  4.86it/s]2023-06-09 15:27:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  10% 152/1464 [00:29<04:39,  4.69it/s, loss=0.914, nll_loss=0.03, accuracy=73.3, wps=1349.5, ups=1.38, wpb=976.5, bsz=31.9, num_updates=5900, lr=8.23387e-06, gnorm=14.328, loss_scale=2048, train_wall=19, gb_free=11.6, wall=1128]2023-06-09 15:27:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  23% 332/1464 [00:59<02:52,  6.56it/s, loss=0.948, nll_loss=0.031, accuracy=71.7, wps=5786.3, ups=5.98, wpb=967.9, bsz=32, num_updates=6100, lr=8.09776e-06, gnorm=14.603, loss_scale=2048, train_wall=16, gb_free=11.7, wall=1164]2023-06-09 15:28:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  32% 462/1464 [01:20<02:51,  5.83it/s, loss=0.89, nll_loss=0.029, accuracy=74.4, wps=6206.1, ups=6.39, wpb=970.7, bsz=31.9, num_updates=6200, lr=8.03219e-06, gnorm=14.655, loss_scale=2048, train_wall=15, gb_free=11.6, wall=1179]2023-06-09 15:28:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  42% 615/1464 [01:43<02:10,  6.49it/s, loss=0.856, nll_loss=0.028, accuracy=75.1, wps=6293, ups=6.51, wpb=966, bsz=32, num_updates=6400, lr=7.90569e-06, gnorm=14.239, loss_scale=2048, train_wall=15, gb_free=11.4, wall=1210]2023-06-09 15:29:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  51% 746/1464 [02:03<01:56,  6.17it/s, loss=0.835, nll_loss=0.027, accuracy=75.7, wps=6242.9, ups=6.41, wpb=974.1, bsz=31.9, num_updates=6500, lr=7.84465e-06, gnorm=14.914, loss_scale=2048, train_wall=15, gb_free=11.6, wall=1225]2023-06-09 15:29:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  61% 891/1464 [02:26<01:28,  6.48it/s, loss=0.836, nll_loss=0.027, accuracy=75.3, wps=6203, ups=6.38, wpb=972.7, bsz=31.9, num_updates=6700, lr=7.72667e-06, gnorm=15.903, loss_scale=4096, train_wall=15, gb_free=11.9, wall=1257]2023-06-09 15:29:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  73% 1063/1464 [02:53<01:04,  6.17it/s, loss=0.828, nll_loss=0.027, accuracy=76.2, wps=6185.7, ups=6.34, wpb=976.3, bsz=32, num_updates=6800, lr=7.66965e-06, gnorm=14.945, loss_scale=2048, train_wall=15, gb_free=11.2, wall=1273]2023-06-09 15:30:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  82% 1195/1464 [03:15<00:41,  6.41it/s, loss=0.769, nll_loss=0.025, accuracy=77.7, wps=5843.6, ups=5.97, wpb=978.6, bsz=32, num_updates=7000, lr=7.55929e-06, gnorm=15.418, loss_scale=2048, train_wall=16, gb_free=11.1, wall=1305]2023-06-09 15:30:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  90% 1324/1464 [03:35<00:20,  6.95it/s, loss=0.794, nll_loss=0.026, accuracy=78.1, wps=6061.9, ups=6.25, wpb=970.5, bsz=31.9, num_updates=7100, lr=7.50587e-06, gnorm=15.693, loss_scale=2048, train_wall=16, gb_free=11.7, wall=1321]2023-06-09 15:30:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  99% 1454/1464 [03:56<00:01,  6.21it/s, loss=0.757, nll_loss=0.025, accuracy=78.6, wps=6081.3, ups=6.32, wpb=962.2, bsz=31.9, num_updates=7200, lr=7.45356e-06, gnorm=15.562, loss_scale=2048, train_wall=15, gb_free=11.9, wall=1337]2023-06-09 15:31:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005: 100% 1463/1464 [03:57<00:00,  7.09it/s, loss=0.757, nll_loss=0.025, accuracy=78.6, wps=6081.3, ups=6.32, wpb=962.2, bsz=31.9, num_updates=7200, lr=7.45356e-06, gnorm=15.562, loss_scale=2048, train_wall=15, gb_free=11.9, wall=1337]2023-06-09 15:31:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-09 15:31:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 005 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 18.37it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   8% 5/62 [00:00<00:02, 19.60it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  13% 8/62 [00:00<00:02, 20.21it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  18% 11/62 [00:00<00:02, 22.19it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  23% 14/62 [00:00<00:02, 23.79it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  27% 17/62 [00:00<00:01, 24.62it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  32% 20/62 [00:00<00:01, 26.00it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  39% 24/62 [00:00<00:01, 26.71it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  44% 27/62 [00:01<00:01, 26.77it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  48% 30/62 [00:01<00:01, 26.90it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  53% 33/62 [00:01<00:01, 26.93it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  58% 36/62 [00:01<00:00, 26.66it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  63% 39/62 [00:01<00:00, 26.52it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  68% 42/62 [00:01<00:00, 26.83it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  73% 45/62 [00:01<00:00, 27.34it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  77% 48/62 [00:01<00:00, 27.73it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  84% 52/62 [00:01<00:00, 29.48it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  90% 56/62 [00:02<00:00, 30.55it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  97% 60/62 [00:02<00:00, 29.20it/s]\u001b[A\n","                                                                        \u001b[A2023-06-09 15:31:23 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 2.3 | nll_loss 0.079 | accuracy 41.8 | wps 25055.8 | wpb 931.1 | bsz 31.8 | num_updates 7280 | best_accuracy 42.2\n","2023-06-09 15:31:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7280 updates\n","2023-06-09 15:31:23 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/ssmba/checkpoint_last.pt\n","2023-06-09 15:31:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/ssmba/checkpoint_last.pt\n","2023-06-09 15:31:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/ssmba/checkpoint_last.pt (epoch 5 @ 7280 updates, score 41.8) (writing took 11.884133286000178 seconds)\n","2023-06-09 15:31:35 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n","2023-06-09 15:31:35 | INFO | train | epoch 005 | loss 0.841 | nll_loss 0.028 | accuracy 75.8 | wps 5598.8 | ups 5.76 | wpb 972.2 | bsz 32 | num_updates 7280 | lr 7.41249e-06 | gnorm 15.066 | loss_scale 2048 | train_wall 229 | gb_free 11.9 | wall 1364\n","2023-06-09 15:31:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-09 15:31:35 | INFO | fairseq_cli.train | done training in 1360.4 seconds\n"]}]},{"cell_type":"code","source":["from fairseq.models.roberta import RobertaModel\n","roberta = RobertaModel.from_pretrained(\n","    '/content/drive/MyDrive/NLP/checkpoints/fiction/ssmba',\n","    checkpoint_file='checkpoint_best.pt',\n","    data_name_or_path='/content/drive/MyDrive/NLP/mnli/fiction/orig/bin_ssmba'\n",")\n","roberta.eval()"],"metadata":{"id":"9ceLszLurHst"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["genre = ['slate','fiction','telephone','travel','government']\n","for g in genre:\n","  input0 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input0', \"r\")\n","  input1 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input1', \"r\")\n","  label = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.label', \"r\")\n","\n","  accuracy = 0\n","  total = 0\n","  for (x1, x2, y) in zip(input0, input1, label):\n","    tokens = roberta.encode(x1, x2)\n","    idx = roberta.predict('mnli', tokens).argmax().item()\n","    dictionary = roberta.task.label_dictionary\n","    pred = dictionary[idx + dictionary.nspecial]\n","    total = total + 1\n","    if  (pred == y.strip()) :\n","      accuracy = accuracy + 1\n","\n","  print(g,\": \",accuracy)\n","  print(g,\": \",total)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"saHnLXhwrJfC","executionInfo":{"status":"ok","timestamp":1686326649074,"user_tz":-120,"elapsed":23554,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}},"outputId":"11d3081f-27a3-4251-a78b-e588a4b319bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["slate :  766\n","slate :  2000\n","fiction :  821\n","fiction :  2000\n","telephone :  799\n","telephone :  2000\n","travel :  737\n","travel :  2000\n","government :  749\n","government :  2000\n"]}]},{"cell_type":"code","source":["genre = ['verbatim','facetoface','oup','nineeleven','letters']\n","for g in genre:\n","  input0 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input0', \"r\")\n","  input1 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input1', \"r\")\n","  label = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.label', \"r\")\n","\n","  accuracy = 0\n","  total = 0\n","  for (x1, x2, y) in zip(input0, input1, label):\n","    tokens = roberta.encode(x1, x2)\n","    idx = roberta.predict('mnli', tokens).argmax().item()\n","    dictionary = roberta.task.label_dictionary\n","    pred = dictionary[idx + dictionary.nspecial]\n","    total = total + 1\n","    if  (pred == y.strip()) :\n","      accuracy = accuracy + 1\n","\n","  print(g,\": \",accuracy)\n","  print(g,\": \",total)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jJwcmU3Eof2r","executionInfo":{"status":"ok","timestamp":1686494157375,"user_tz":-120,"elapsed":910228,"user":{"displayName":"Louise Leibbrandt","userId":"08849155044420854120"}},"outputId":"a1b6ed52-469f-4a29-b358-41305408f915"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["verbatim :  415\n","verbatim :  1000\n","facetoface :  388\n","facetoface :  1000\n","oup :  398\n","oup :  1000\n","nineeleven :  381\n","nineeleven :  1000\n","letters :  382\n","letters :  1000\n"]}]},{"cell_type":"markdown","source":["# EDA Extended"],"metadata":{"id":"J4XiLKlDc15A"}},{"cell_type":"code","source":["import tensorflow as tf\n","with tf.device('/device:GPU:0'):\n","    ! CUDA_VISIBLE_DEVICES=0 fairseq-train /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda_extended \\\n","          --save-dir /content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended \\\n","          --reset-optimizer  \\\n","          --reset-dataloader  \\\n","          --reset-meters  \\\n","          --best-checkpoint-metric accuracy  \\\n","          --maximize-best-checkpoint-metric  \\\n","          --no-epoch-checkpoints \\\n","          --find-unused-parameters \\\n","          --distributed-world-size 1 \\\n","          --task sentence_prediction  \\\n","          --num-classes 3  \\\n","          --init-token 0  \\\n","          --separator-token 2   \\\n","          --max-positions 512  \\\n","          --shorten-method \"truncate\"  \\\n","          --arch roberta \\\n","          --dropout 0.1  \\\n","          --attention-dropout 0.1  \\\n","          --weight-decay 0.1  \\\n","          --criterion sentence_prediction  \\\n","          --classification-head-name 'mnli' \\\n","          --optimizer adam  \\\n","          --adam-betas '(0.9, 0.98)'  \\\n","          --adam-eps 1e-06  \\\n","          --clip-norm 0.0  \\\n","          --lr-scheduler inverse_sqrt  \\\n","          --lr 1e-05 \\\n","          --fp16  \\\n","          --fp16-init-scale 4  \\\n","          --threshold-loss-scale 1  \\\n","          --fp16-scale-window 128  \\\n","          --batch-size 32  \\\n","          --required-batch-size-multiple 1  \\\n","          --max-tokens 4400 \\\n","          --update-freq 1  \\\n","          --max-update 123873 \\\n","          --max-epoch 5 \\\n","          --seed 100"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pk6WaXuucqS6","executionInfo":{"status":"ok","timestamp":1686824361398,"user_tz":-120,"elapsed":1436530,"user":{"displayName":"Louise Leibbrandt","userId":"11762724927321883770"}},"outputId":"611774bf-79ee-4a81-8d8b-ae2cc98fa66a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-15 09:55:41.902053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2023-06-15 09:55:43 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n","2023-06-15 09:55:48 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 100, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4400, 'batch_size': 32, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 123873, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=100, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=1.0, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4400, batch_size=32, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4400, batch_size_valid=32, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta', max_epoch=5, max_update=123873, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='/content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda_extended', num_classes=3, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, d2v2_multi=False, classification_head_name='mnli', regression_target=False, report_mcc=False, report_acc_and_f1=False, report_pearson_and_spearman=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.1, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, max_positions=512, dropout=0.1, attention_dropout=0.1, no_seed_provided=False, encoder_layers=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_attention_heads=12, activation_dropout=0.0, pooler_dropout=0.0, max_source_positions=512, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta'), 'task': {'_name': 'sentence_prediction', 'data': '/content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda_extended', 'num_classes': 3, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 512, 'regression_target': False, 'classification_head_name': 'mnli', 'seed': 100, 'd2v2_multi': False}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'mnli', 'regression_target': False, 'report_mcc': False, 'report_acc_and_f1': False, 'report_pearson_and_spearman': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2023-06-15 09:55:49 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n","2023-06-15 09:55:49 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n","2023-06-15 09:55:52 | INFO | fairseq_cli.train | RobertaModel(\n","  (encoder): RobertaEncoder(\n","    (sentence_encoder): TransformerEncoder(\n","      (dropout_module): FairseqDropout()\n","      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","      (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (layers): ModuleList(\n","        (0-11): 12 x TransformerEncoderLayerBase(\n","          (self_attn): MultiheadAttention(\n","            (dropout_module): FairseqDropout()\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout_module): FairseqDropout()\n","          (activation_dropout_module): FairseqDropout()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (lm_head): RobertaLMHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (classification_heads): ModuleDict(\n","    (mnli): RobertaClassificationHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","      (out_proj): Linear(in_features=768, out_features=3, bias=True)\n","    )\n","  )\n",")\n","2023-06-15 09:55:52 | INFO | fairseq_cli.train | task: SentencePredictionTask\n","2023-06-15 09:55:52 | INFO | fairseq_cli.train | model: RobertaModel\n","2023-06-15 09:55:52 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n","2023-06-15 09:55:52 | INFO | fairseq_cli.train | num. shared model params: 125,289,564 (num. trained: 125,289,564)\n","2023-06-15 09:55:52 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n","2023-06-15 09:55:53 | INFO | fairseq.data.data_utils | loaded 1,973 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda_extended/input0/valid\n","2023-06-15 09:55:53 | INFO | fairseq.data.data_utils | loaded 1,973 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda_extended/input1/valid\n","2023-06-15 09:55:54 | INFO | fairseq.data.data_utils | loaded 1,973 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda_extended/label/valid\n","2023-06-15 09:55:54 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 1973\n","2023-06-15 09:56:01 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n","2023-06-15 09:56:01 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-15 09:56:01 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n","2023-06-15 09:56:01 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2023-06-15 09:56:01 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2023-06-15 09:56:01 | INFO | fairseq_cli.train | max tokens per device = 4400 and max sentences per device = 32\n","2023-06-15 09:56:01 | INFO | fairseq.trainer | Preparing to load checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended/checkpoint_last.pt\n","2023-06-15 09:56:01 | INFO | fairseq.trainer | No existing checkpoint found /content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended/checkpoint_last.pt\n","2023-06-15 09:56:01 | INFO | fairseq.trainer | loading train data for epoch 1\n","2023-06-15 09:56:01 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda_extended/input0/train\n","2023-06-15 09:56:02 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda_extended/input1/train\n","2023-06-15 09:56:03 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: /content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda_extended/label/train\n","2023-06-15 09:56:03 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 50000\n","2023-06-15 09:56:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-15 09:56:03 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-15 09:56:03 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-15 09:56:03 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-15 09:56:03 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n","2023-06-15 09:56:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-15 09:56:03 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2023-06-15 09:56:03 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2023-06-15 09:56:03 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2023-06-15 09:56:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1564\n","epoch 001:   0% 0/1564 [00:00<?, ?it/s]2023-06-15 09:56:04 | INFO | fairseq.trainer | begin training epoch 1\n","2023-06-15 09:56:04 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 001:  91% 1417/1564 [03:38<00:22,  6.51it/s, loss=1.57, nll_loss=0.058, accuracy=38.1, wps=5911.8, ups=6.78, wpb=872.6, bsz=32, num_updates=1400, lr=3.5e-06, gnorm=4.142, loss_scale=4096, train_wall=14, gb_free=11.8, wall=219]2023-06-15 09:59:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 001: 100% 1563/1564 [04:00<00:00,  7.58it/s, loss=1.565, nll_loss=0.057, accuracy=38.4, wps=5762.3, ups=6.6, wpb=872.5, bsz=32, num_updates=1500, lr=3.75e-06, gnorm=3.952, loss_scale=4096, train_wall=15, gb_free=11.9, wall=234]2023-06-15 10:00:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-15 10:00:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   3% 2/62 [00:00<00:05, 11.79it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   6% 4/62 [00:00<00:04, 11.86it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  11% 7/62 [00:00<00:03, 15.77it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  15% 9/62 [00:00<00:03, 16.82it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  19% 12/62 [00:00<00:02, 18.42it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  23% 14/62 [00:00<00:02, 18.51it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  27% 17/62 [00:00<00:02, 19.34it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  32% 20/62 [00:01<00:02, 20.27it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  37% 23/62 [00:01<00:01, 20.74it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  42% 26/62 [00:01<00:01, 21.16it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  47% 29/62 [00:01<00:01, 20.73it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  52% 32/62 [00:01<00:01, 20.90it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  56% 35/62 [00:01<00:01, 21.04it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  61% 38/62 [00:01<00:01, 21.22it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  66% 41/62 [00:02<00:00, 21.02it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  71% 44/62 [00:02<00:00, 21.00it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  76% 47/62 [00:02<00:00, 22.28it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  81% 50/62 [00:02<00:00, 23.26it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  87% 54/62 [00:02<00:00, 26.02it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  92% 57/62 [00:02<00:00, 26.29it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  97% 60/62 [00:02<00:00, 26.07it/s]\u001b[A\n","                                                                        \u001b[A2023-06-15 10:00:07 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.574 | nll_loss 0.054 | accuracy 37.2 | wps 20051.8 | wpb 931.1 | bsz 31.8 | num_updates 1563\n","2023-06-15 10:00:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1563 updates\n","2023-06-15 10:00:07 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended/checkpoint_best.pt\n","2023-06-15 10:00:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended/checkpoint_best.pt\n","2023-06-15 10:00:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended/checkpoint_best.pt (epoch 1 @ 1563 updates, score 37.2) (writing took 35.52493516300001 seconds)\n","2023-06-15 10:00:42 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2023-06-15 10:00:42 | INFO | train | epoch 001 | loss 1.582 | nll_loss 0.058 | accuracy 36.3 | wps 4954.3 | ups 5.68 | wpb 872.9 | bsz 32 | num_updates 1563 | lr 3.9075e-06 | gnorm 4.454 | loss_scale 8192 | train_wall 231 | gb_free 11.8 | wall 282\n","2023-06-15 10:00:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-15 10:00:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1564\n","epoch 002:   0% 0/1564 [00:00<?, ?it/s]2023-06-15 10:00:43 | INFO | fairseq.trainer | begin training epoch 2\n","2023-06-15 10:00:43 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 002:   5% 79/1564 [00:14<04:13,  5.86it/s, loss=1.557, nll_loss=0.058, accuracy=41.4, wps=1597.2, ups=1.86, wpb=860.8, bsz=32, num_updates=1600, lr=4e-06, gnorm=3.915, loss_scale=8192, train_wall=15, gb_free=11.9, wall=288]2023-06-15 10:00:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  16% 253/1564 [00:41<03:14,  6.75it/s, loss=1.55, nll_loss=0.057, accuracy=41.6, wps=5738.1, ups=6.54, wpb=877.6, bsz=32, num_updates=1800, lr=4.5e-06, gnorm=4.294, loss_scale=8192, train_wall=15, gb_free=11.4, wall=322]2023-06-15 10:01:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  24% 383/1564 [01:01<02:54,  6.75it/s, loss=1.556, nll_loss=0.056, accuracy=41.3, wps=5676.3, ups=6.44, wpb=881.5, bsz=31.9, num_updates=1900, lr=4.75e-06, gnorm=4.874, loss_scale=4096, train_wall=15, gb_free=11.8, wall=337]2023-06-15 10:01:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  33% 515/1564 [01:21<02:31,  6.95it/s, loss=1.536, nll_loss=0.057, accuracy=42.9, wps=5802.5, ups=6.7, wpb=865.5, bsz=32, num_updates=2000, lr=5e-06, gnorm=4.766, loss_scale=4096, train_wall=14, gb_free=11.8, wall=352]2023-06-15 10:02:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  42% 650/1564 [01:40<02:19,  6.55it/s, loss=1.527, nll_loss=0.056, accuracy=43.8, wps=5888, ups=6.79, wpb=867.1, bsz=31.9, num_updates=2200, lr=5.5e-06, gnorm=4.981, loss_scale=4096, train_wall=14, gb_free=11.6, wall=382]2023-06-15 10:02:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  51% 803/1564 [02:03<02:00,  6.31it/s, loss=1.529, nll_loss=0.056, accuracy=44.7, wps=5896.7, ups=6.72, wpb=877.6, bsz=32, num_updates=2300, lr=5.75e-06, gnorm=5.613, loss_scale=4096, train_wall=14, gb_free=11.5, wall=397]2023-06-15 10:02:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  61% 950/1564 [02:26<01:33,  6.54it/s, loss=1.522, nll_loss=0.056, accuracy=44.5, wps=5528.5, ups=6.38, wpb=866, bsz=31.9, num_updates=2500, lr=6.25e-06, gnorm=4.735, loss_scale=8192, train_wall=15, gb_free=11.8, wall=428]2023-06-15 10:03:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  69% 1084/1564 [02:47<01:11,  6.72it/s, loss=1.508, nll_loss=0.055, accuracy=44.8, wps=5596.2, ups=6.43, wpb=870.2, bsz=31.9, num_updates=2600, lr=6.5e-06, gnorm=5.695, loss_scale=4096, train_wall=15, gb_free=11.5, wall=443]2023-06-15 10:03:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  78% 1217/1564 [03:07<00:49,  7.05it/s, loss=1.498, nll_loss=0.055, accuracy=46.2, wps=5745, ups=6.64, wpb=865.6, bsz=32, num_updates=2700, lr=6.75e-06, gnorm=5.709, loss_scale=4096, train_wall=15, gb_free=11.2, wall=458]2023-06-15 10:03:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4096.0\n","epoch 002:  84% 1306/1564 [03:20<00:36,  7.16it/s, loss=1.511, nll_loss=0.055, accuracy=45.2, wps=5733, ups=6.5, wpb=882, bsz=32, num_updates=2800, lr=7e-06, gnorm=5.749, loss_scale=4096, train_wall=15, gb_free=11.7, wall=474]2023-06-15 10:04:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 002:  99% 1543/1564 [03:56<00:03,  6.42it/s, loss=1.489, nll_loss=0.054, accuracy=46.6, wps=5768.7, ups=6.53, wpb=883.6, bsz=32, num_updates=3000, lr=7.5e-06, gnorm=6.02, loss_scale=4096, train_wall=15, gb_free=11.9, wall=504]2023-06-15 10:04:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 002: 100% 1563/1564 [03:59<00:00,  6.87it/s, loss=1.484, nll_loss=0.054, accuracy=47.9, wps=5841.2, ups=6.64, wpb=879.6, bsz=32, num_updates=3100, lr=7.75e-06, gnorm=5.934, loss_scale=2048, train_wall=14, gb_free=11.9, wall=519]2023-06-15 10:04:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-15 10:04:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   3% 2/62 [00:00<00:04, 13.14it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   6% 4/62 [00:00<00:03, 15.85it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  11% 7/62 [00:00<00:03, 16.82it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  15% 9/62 [00:00<00:03, 15.33it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  19% 12/62 [00:00<00:02, 17.93it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  24% 15/62 [00:00<00:02, 19.47it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  29% 18/62 [00:00<00:02, 19.45it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  34% 21/62 [00:01<00:01, 20.64it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  39% 24/62 [00:01<00:01, 21.25it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  44% 27/62 [00:01<00:01, 21.16it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  48% 30/62 [00:01<00:01, 21.02it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  53% 33/62 [00:01<00:01, 21.36it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  58% 36/62 [00:01<00:01, 21.23it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  63% 39/62 [00:01<00:01, 21.13it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  68% 42/62 [00:02<00:00, 21.46it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  73% 45/62 [00:02<00:00, 21.87it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  77% 48/62 [00:02<00:00, 22.24it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  82% 51/62 [00:02<00:00, 24.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  89% 55/62 [00:02<00:00, 26.44it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  94% 58/62 [00:02<00:00, 26.42it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  98% 61/62 [00:02<00:00, 26.11it/s]\u001b[A\n","                                                                        \u001b[A2023-06-15 10:04:45 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 1.585 | nll_loss 0.054 | accuracy 40.8 | wps 20389.3 | wpb 931.1 | bsz 31.8 | num_updates 3116 | best_accuracy 40.8\n","2023-06-15 10:04:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3116 updates\n","2023-06-15 10:04:45 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended/checkpoint_best.pt\n","2023-06-15 10:04:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended/checkpoint_best.pt\n","2023-06-15 10:05:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended/checkpoint_best.pt (epoch 2 @ 3116 updates, score 40.8) (writing took 43.88563235799984 seconds)\n","2023-06-15 10:05:29 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2023-06-15 10:05:29 | INFO | train | epoch 002 | loss 1.523 | nll_loss 0.056 | accuracy 44.1 | wps 4738.6 | ups 5.43 | wpb 873.1 | bsz 32 | num_updates 3116 | lr 7.79e-06 | gnorm 5.207 | loss_scale 2048 | train_wall 230 | gb_free 11.9 | wall 568\n","2023-06-15 10:05:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-15 10:05:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1564\n","epoch 003:   0% 0/1564 [00:00<?, ?it/s]2023-06-15 10:05:29 | INFO | fairseq.trainer | begin training epoch 3\n","2023-06-15 10:05:29 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 003:   8% 122/1564 [00:21<03:37,  6.63it/s, loss=1.455, nll_loss=0.054, accuracy=50.2, wps=1341.8, ups=1.55, wpb=865.6, bsz=32, num_updates=3200, lr=8e-06, gnorm=7.441, loss_scale=2048, train_wall=17, gb_free=11.8, wall=584]2023-06-15 10:05:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  21% 336/1564 [01:00<02:50,  7.21it/s, loss=1.432, nll_loss=0.053, accuracy=51.6, wps=4613.6, ups=5.36, wpb=861.2, bsz=31.9, num_updates=3400, lr=8.5e-06, gnorm=7.751, loss_scale=4096, train_wall=18, gb_free=11.4, wall=620]2023-06-15 10:06:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  33% 514/1564 [01:27<02:23,  7.31it/s, loss=1.421, nll_loss=0.052, accuracy=51.5, wps=5789.9, ups=6.62, wpb=874.2, bsz=32, num_updates=3600, lr=9e-06, gnorm=7.156, loss_scale=4096, train_wall=15, gb_free=11.9, wall=652]2023-06-15 10:06:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  47% 732/1564 [02:00<02:11,  6.34it/s, loss=1.413, nll_loss=0.052, accuracy=52.8, wps=5930.8, ups=6.8, wpb=871.9, bsz=32, num_updates=3800, lr=9.5e-06, gnorm=7.577, loss_scale=4096, train_wall=14, gb_free=11.9, wall=681]2023-06-15 10:07:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  57% 888/1564 [02:23<01:44,  6.49it/s, loss=1.354, nll_loss=0.05, accuracy=54.4, wps=5814.3, ups=6.75, wpb=860.8, bsz=31.8, num_updates=4000, lr=1e-05, gnorm=8.051, loss_scale=4096, train_wall=14, gb_free=11.9, wall=712]  2023-06-15 10:07:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  67% 1055/1564 [02:48<01:15,  6.70it/s, loss=1.376, nll_loss=0.05, accuracy=54.6, wps=5725.1, ups=6.48, wpb=884.1, bsz=32, num_updates=4100, lr=9.8773e-06, gnorm=8.44, loss_scale=2048, train_wall=15, gb_free=11.8, wall=728]2023-06-15 10:08:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  80% 1258/1564 [03:19<00:42,  7.15it/s, loss=1.349, nll_loss=0.049, accuracy=55.5, wps=5812.1, ups=6.63, wpb=877, bsz=32, num_updates=4300, lr=9.64486e-06, gnorm=8.017, loss_scale=4096, train_wall=14, gb_free=11.7, wall=758]2023-06-15 10:08:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003:  92% 1438/1564 [03:46<00:18,  6.68it/s, loss=1.326, nll_loss=0.049, accuracy=57.8, wps=5756, ups=6.6, wpb=871.7, bsz=31.9, num_updates=4500, lr=9.42809e-06, gnorm=7.799, loss_scale=4096, train_wall=15, gb_free=11.8, wall=788]2023-06-15 10:09:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 003: 100% 1563/1564 [04:05<00:00,  7.73it/s, loss=1.308, nll_loss=0.047, accuracy=56.9, wps=5658.8, ups=6.38, wpb=887.3, bsz=32, num_updates=4600, lr=9.32505e-06, gnorm=8.514, loss_scale=2048, train_wall=15, gb_free=11.2, wall=803]2023-06-15 10:09:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-15 10:09:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 16.51it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   6% 4/62 [00:00<00:03, 18.12it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  11% 7/62 [00:00<00:02, 20.05it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  16% 10/62 [00:00<00:02, 21.40it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  21% 13/62 [00:00<00:02, 23.83it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  26% 16/62 [00:00<00:01, 24.57it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  31% 19/62 [00:00<00:01, 24.60it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  35% 22/62 [00:00<00:01, 25.19it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  40% 25/62 [00:01<00:01, 26.38it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  45% 28/62 [00:01<00:01, 24.94it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  50% 31/62 [00:01<00:01, 25.96it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  55% 34/62 [00:01<00:01, 26.42it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  60% 37/62 [00:01<00:00, 26.78it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  65% 40/62 [00:01<00:00, 25.86it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  69% 43/62 [00:01<00:00, 25.89it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  74% 46/62 [00:01<00:00, 26.23it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  79% 49/62 [00:01<00:00, 25.63it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  85% 53/62 [00:02<00:00, 27.41it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  90% 56/62 [00:02<00:00, 28.07it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  95% 59/62 [00:02<00:00, 27.43it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset: 100% 62/62 [00:02<00:00, 27.30it/s]\u001b[A\n","                                                                        \u001b[A2023-06-15 10:09:37 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 1.629 | nll_loss 0.056 | accuracy 42.1 | wps 23822.9 | wpb 931.1 | bsz 31.8 | num_updates 4672 | best_accuracy 42.1\n","2023-06-15 10:09:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4672 updates\n","2023-06-15 10:09:37 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended/checkpoint_best.pt\n","2023-06-15 10:09:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended/checkpoint_best.pt\n","2023-06-15 10:10:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended/checkpoint_best.pt (epoch 3 @ 4672 updates, score 42.1) (writing took 48.24718651199987 seconds)\n","2023-06-15 10:10:25 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2023-06-15 10:10:25 | INFO | train | epoch 003 | loss 1.378 | nll_loss 0.05 | accuracy 54 | wps 4575.7 | ups 5.24 | wpb 872.8 | bsz 32 | num_updates 4672 | lr 9.25292e-06 | gnorm 7.81 | loss_scale 2048 | train_wall 237 | gb_free 11.8 | wall 865\n","2023-06-15 10:10:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-15 10:10:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1564\n","epoch 004:   0% 0/1564 [00:00<?, ?it/s]2023-06-15 10:10:26 | INFO | fairseq.trainer | begin training epoch 4\n","2023-06-15 10:10:26 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 004:   2% 33/1564 [00:06<04:58,  5.14it/s, loss=1.305, nll_loss=0.048, accuracy=57.4, wps=1299.5, ups=1.49, wpb=873.6, bsz=32, num_updates=4700, lr=9.22531e-06, gnorm=8.587, loss_scale=4096, train_wall=16, gb_free=11.8, wall=871]2023-06-15 10:10:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  10% 164/1564 [00:34<06:02,  3.86it/s, loss=1.244, nll_loss=0.046, accuracy=60.2, wps=4312.1, ups=4.98, wpb=865.5, bsz=32, num_updates=4800, lr=9.12871e-06, gnorm=9.262, loss_scale=2048, train_wall=19, gb_free=11.7, wall=891]2023-06-15 10:11:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  19% 300/1564 [00:55<03:05,  6.82it/s, loss=1.213, nll_loss=0.044, accuracy=62.2, wps=4760.3, ups=5.36, wpb=888.1, bsz=31.9, num_updates=4900, lr=9.03508e-06, gnorm=9.916, loss_scale=2048, train_wall=18, gb_free=11.7, wall=909]2023-06-15 10:11:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  29% 459/1564 [01:18<02:46,  6.62it/s, loss=1.192, nll_loss=0.044, accuracy=63.5, wps=5765.7, ups=6.61, wpb=872.1, bsz=32, num_updates=5100, lr=8.85615e-06, gnorm=9.977, loss_scale=4096, train_wall=15, gb_free=11.8, wall=940]2023-06-15 10:11:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  39% 607/1564 [01:41<02:23,  6.67it/s, loss=1.19, nll_loss=0.044, accuracy=62.6, wps=5725.3, ups=6.57, wpb=870.8, bsz=32, num_updates=5200, lr=8.77058e-06, gnorm=9.938, loss_scale=2048, train_wall=15, gb_free=11.8, wall=955]2023-06-15 10:12:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  48% 755/1564 [02:03<02:00,  6.74it/s, loss=1.193, nll_loss=0.044, accuracy=62.3, wps=5689.2, ups=6.6, wpb=862.4, bsz=32, num_updates=5400, lr=8.60663e-06, gnorm=10.603, loss_scale=2048, train_wall=15, gb_free=11.9, wall=986]2023-06-15 10:12:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  58% 914/1564 [02:28<01:34,  6.87it/s, loss=1.165, nll_loss=0.042, accuracy=64.7, wps=5738.9, ups=6.53, wpb=878.4, bsz=32, num_updates=5500, lr=8.52803e-06, gnorm=11.022, loss_scale=2048, train_wall=15, gb_free=11.9, wall=1001]2023-06-15 10:12:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  68% 1056/1564 [02:49<01:14,  6.81it/s, loss=1.129, nll_loss=0.042, accuracy=65.5, wps=5827.5, ups=6.79, wpb=858.2, bsz=32, num_updates=5700, lr=8.37708e-06, gnorm=10.458, loss_scale=2048, train_wall=14, gb_free=11.9, wall=1032]2023-06-15 10:13:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  76% 1194/1564 [03:10<00:54,  6.83it/s, loss=1.149, nll_loss=0.042, accuracy=64.5, wps=5715.9, ups=6.55, wpb=872.6, bsz=32, num_updates=5800, lr=8.30455e-06, gnorm=10.8, loss_scale=2048, train_wall=15, gb_free=11.8, wall=1047]2023-06-15 10:13:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  86% 1341/1564 [03:32<00:34,  6.51it/s, loss=1.16, nll_loss=0.042, accuracy=63.6, wps=5771.5, ups=6.6, wpb=874.6, bsz=32, num_updates=6000, lr=8.16497e-06, gnorm=10.251, loss_scale=4096, train_wall=15, gb_free=11.7, wall=1077]2023-06-15 10:13:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004:  95% 1482/1564 [03:54<00:12,  6.47it/s, loss=1.095, nll_loss=0.04, accuracy=67.1, wps=5675.5, ups=6.54, wpb=867.7, bsz=32, num_updates=6100, lr=8.09776e-06, gnorm=10.614, loss_scale=2048, train_wall=15, gb_free=11.7, wall=1092]2023-06-15 10:14:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 004: 100% 1563/1564 [04:06<00:00,  6.44it/s, loss=1.116, nll_loss=0.04, accuracy=66.5, wps=5788.5, ups=6.52, wpb=887.4, bsz=32, num_updates=6200, lr=8.03219e-06, gnorm=10.561, loss_scale=2048, train_wall=15, gb_free=11.6, wall=1108]2023-06-15 10:14:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-15 10:14:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   3% 2/62 [00:00<00:04, 13.03it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   6% 4/62 [00:00<00:04, 12.70it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  10% 6/62 [00:00<00:04, 13.47it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  13% 8/62 [00:00<00:03, 15.35it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  16% 10/62 [00:00<00:03, 16.01it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  21% 13/62 [00:00<00:02, 18.18it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  26% 16/62 [00:00<00:02, 20.53it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  31% 19/62 [00:01<00:02, 20.98it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  35% 22/62 [00:01<00:01, 21.61it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  40% 25/62 [00:01<00:01, 22.52it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  45% 28/62 [00:01<00:01, 21.36it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  50% 31/62 [00:01<00:01, 21.09it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  55% 34/62 [00:01<00:01, 21.07it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  60% 37/62 [00:01<00:01, 21.06it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  65% 40/62 [00:02<00:01, 20.87it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  69% 43/62 [00:02<00:00, 20.67it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  74% 46/62 [00:02<00:00, 21.24it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  79% 49/62 [00:02<00:00, 21.64it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  85% 53/62 [00:02<00:00, 24.35it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  90% 56/62 [00:02<00:00, 24.95it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  95% 59/62 [00:02<00:00, 25.74it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset: 100% 62/62 [00:02<00:00, 26.51it/s]\u001b[A\n","                                                                        \u001b[A2023-06-15 10:14:35 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 1.769 | nll_loss 0.06 | accuracy 41 | wps 19934.8 | wpb 931.1 | bsz 31.8 | num_updates 6225 | best_accuracy 42.1\n","2023-06-15 10:14:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6225 updates\n","2023-06-15 10:14:35 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended/checkpoint_last.pt\n","2023-06-15 10:14:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended/checkpoint_last.pt\n","2023-06-15 10:14:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended/checkpoint_last.pt (epoch 4 @ 6225 updates, score 41.0) (writing took 8.369846697999947 seconds)\n","2023-06-15 10:14:43 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2023-06-15 10:14:43 | INFO | train | epoch 004 | loss 1.171 | nll_loss 0.043 | accuracy 63.8 | wps 5257 | ups 6.02 | wpb 872.9 | bsz 32 | num_updates 6225 | lr 8.01605e-06 | gnorm 10.232 | loss_scale 2048 | train_wall 237 | gb_free 11.9 | wall 1123\n","2023-06-15 10:14:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-15 10:14:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1564\n","epoch 005:   0% 0/1564 [00:00<?, ?it/s]2023-06-15 10:14:43 | INFO | fairseq.trainer | begin training epoch 5\n","2023-06-15 10:14:43 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 005:   6% 88/1564 [00:15<05:19,  4.61it/s, loss=1.053, nll_loss=0.039, accuracy=68.3, wps=3147.7, ups=3.62, wpb=869.2, bsz=31.8, num_updates=6300, lr=7.96819e-06, gnorm=11.148, loss_scale=4096, train_wall=15, gb_free=11.9, wall=1135]2023-06-15 10:14:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  14% 218/1564 [00:42<04:12,  5.34it/s, loss=0.98, nll_loss=0.036, accuracy=71, wps=4156.5, ups=4.78, wpb=870.4, bsz=32, num_updates=6400, lr=7.90569e-06, gnorm=12.207, loss_scale=2048, train_wall=20, gb_free=11.8, wall=1156]2023-06-15 10:15:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  24% 371/1564 [01:07<03:06,  6.39it/s, loss=1.055, nll_loss=0.039, accuracy=68.9, wps=4510, ups=5.22, wpb=864.1, bsz=32, num_updates=6500, lr=7.84465e-06, gnorm=11.642, loss_scale=2048, train_wall=18, gb_free=11.7, wall=1175]2023-06-15 10:15:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  33% 523/1564 [01:29<02:35,  6.69it/s, loss=1.011, nll_loss=0.037, accuracy=70.8, wps=5849, ups=6.69, wpb=874, bsz=32, num_updates=6700, lr=7.72667e-06, gnorm=11.443, loss_scale=2048, train_wall=14, gb_free=11.9, wall=1206]2023-06-15 10:16:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  43% 671/1564 [01:52<02:22,  6.24it/s, loss=1.019, nll_loss=0.037, accuracy=69.3, wps=5739.7, ups=6.56, wpb=874.5, bsz=31.9, num_updates=6800, lr=7.66965e-06, gnorm=12.148, loss_scale=2048, train_wall=15, gb_free=11.2, wall=1221]2023-06-15 10:16:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  53% 836/1564 [02:17<01:55,  6.30it/s, loss=0.98, nll_loss=0.036, accuracy=71.1, wps=5812.7, ups=6.69, wpb=868.9, bsz=32, num_updates=7000, lr=7.55929e-06, gnorm=12.197, loss_scale=2048, train_wall=14, gb_free=11.9, wall=1252]2023-06-15 10:17:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  63% 981/1564 [02:39<01:23,  6.96it/s, loss=0.935, nll_loss=0.034, accuracy=71.9, wps=5781.8, ups=6.61, wpb=874.8, bsz=32, num_updates=7200, lr=7.45356e-06, gnorm=11.792, loss_scale=4096, train_wall=15, gb_free=11.9, wall=1282]2023-06-15 10:17:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  73% 1146/1564 [03:03<01:03,  6.59it/s, loss=0.992, nll_loss=0.036, accuracy=70.8, wps=5868.2, ups=6.69, wpb=877.5, bsz=32, num_updates=7300, lr=7.40233e-06, gnorm=12.622, loss_scale=2048, train_wall=14, gb_free=11.9, wall=1297]2023-06-15 10:17:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  86% 1339/1564 [03:33<00:34,  6.55it/s, loss=0.977, nll_loss=0.036, accuracy=71.4, wps=5796.4, ups=6.6, wpb=878.8, bsz=32, num_updates=7500, lr=7.30297e-06, gnorm=12.704, loss_scale=4096, train_wall=15, gb_free=11.4, wall=1327]2023-06-15 10:18:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005:  95% 1483/1564 [03:55<00:13,  6.15it/s, loss=0.962, nll_loss=0.035, accuracy=71.3, wps=5658.3, ups=6.47, wpb=875, bsz=32, num_updates=7600, lr=7.25476e-06, gnorm=12.342, loss_scale=2048, train_wall=15, gb_free=11.9, wall=1343]2023-06-15 10:18:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0\n","epoch 005: 100% 1563/1564 [04:07<00:00,  6.73it/s, loss=0.939, nll_loss=0.034, accuracy=72.6, wps=5675.5, ups=6.51, wpb=872.4, bsz=31.9, num_updates=7700, lr=7.2075e-06, gnorm=12.075, loss_scale=2048, train_wall=15, gb_free=11.8, wall=1358]2023-06-15 10:18:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2023-06-15 10:18:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 005 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   3% 2/62 [00:00<00:03, 18.35it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   6% 4/62 [00:00<00:03, 19.05it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  11% 7/62 [00:00<00:02, 20.73it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  16% 10/62 [00:00<00:02, 21.80it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  21% 13/62 [00:00<00:02, 24.23it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  26% 16/62 [00:00<00:01, 24.81it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  31% 19/62 [00:00<00:01, 24.64it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  35% 22/62 [00:00<00:01, 25.33it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  42% 26/62 [00:01<00:01, 26.81it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  47% 29/62 [00:01<00:01, 25.69it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  52% 32/62 [00:01<00:01, 25.96it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  56% 35/62 [00:01<00:01, 25.79it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  61% 38/62 [00:01<00:00, 25.82it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  66% 41/62 [00:01<00:00, 25.71it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  71% 44/62 [00:01<00:00, 25.41it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  76% 47/62 [00:01<00:00, 26.38it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  82% 51/62 [00:02<00:00, 27.88it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  89% 55/62 [00:02<00:00, 29.59it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  94% 58/62 [00:02<00:00, 28.85it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  98% 61/62 [00:02<00:00, 28.05it/s]\u001b[A\n","                                                                        \u001b[A2023-06-15 10:18:53 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 2.078 | nll_loss 0.071 | accuracy 41.8 | wps 24371.1 | wpb 931.1 | bsz 31.8 | num_updates 7779 | best_accuracy 42.1\n","2023-06-15 10:18:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7779 updates\n","2023-06-15 10:18:53 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended/checkpoint_last.pt\n","2023-06-15 10:19:05 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended/checkpoint_last.pt\n","2023-06-15 10:19:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended/checkpoint_last.pt (epoch 5 @ 7779 updates, score 41.8) (writing took 11.475317810999968 seconds)\n","2023-06-15 10:19:05 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n","2023-06-15 10:19:05 | INFO | train | epoch 005 | loss 0.993 | nll_loss 0.036 | accuracy 70.6 | wps 5185.9 | ups 5.94 | wpb 872.9 | bsz 32 | num_updates 7779 | lr 7.17081e-06 | gnorm 11.992 | loss_scale 2048 | train_wall 238 | gb_free 11.8 | wall 1384\n","2023-06-15 10:19:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2023-06-15 10:19:05 | INFO | fairseq_cli.train | done training in 1381.3 seconds\n"]}]},{"cell_type":"code","source":["from fairseq.models.roberta import RobertaModel\n","roberta = RobertaModel.from_pretrained(\n","    '/content/drive/MyDrive/NLP/checkpoints/fiction/eda_extended',\n","    checkpoint_file='checkpoint_best.pt',\n","    data_name_or_path='/content/drive/MyDrive/NLP/mnli/fiction/orig/bin_eda_extended'\n",")\n","roberta.eval()"],"metadata":{"id":"2CEV_BAkdiqQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["genre = ['slate','fiction','telephone','travel','government','verbatim','facetoface','oup','nineeleven','letters']\n","for g in genre:\n","  input0 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input0', \"r\")\n","  input1 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input1', \"r\")\n","  label = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.label', \"r\")\n","\n","  accuracy = 0\n","  total = 0\n","  for (x1, x2, y) in zip(input0, input1, label):\n","    tokens = roberta.encode(x1, x2)\n","    idx = roberta.predict('mnli', tokens).argmax().item()\n","    dictionary = roberta.task.label_dictionary\n","    pred = dictionary[idx + dictionary.nspecial]\n","    total = total + 1\n","    if  (pred == y.strip()) :\n","      accuracy = accuracy + 1\n","\n","  print(g,\": \",accuracy)\n","  print(g,\": \",total)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qi0-EKxIdozE","executionInfo":{"status":"ok","timestamp":1686829333085,"user_tz":-120,"elapsed":3969095,"user":{"displayName":"Louise Leibbrandt","userId":"11762724927321883770"}},"outputId":"83f98a09-aaa7-44ef-be80-8f3f0e69745b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["slate :  730\n","slate :  2000\n","fiction :  782\n","fiction :  2000\n","telephone :  747\n","telephone :  2000\n","travel :  735\n","travel :  2000\n","government :  703\n","government :  2000\n","verbatim :  402\n","verbatim :  1000\n","facetoface :  389\n","facetoface :  1000\n","oup :  383\n","oup :  1000\n","nineeleven :  389\n","nineeleven :  1000\n","letters :  357\n","letters :  1000\n"]}]},{"cell_type":"code","source":["genre = ['slate','fiction','telephone','travel','government','verbatim','facetoface','oup','nineeleven','letters']\n","for g in genre:\n","  input0 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input0', \"r\")\n","  input1 = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.input1', \"r\")\n","  label = open('/content/drive/MyDrive/NLP/mnli/'+g+'/orig/test.raw.label', \"r\")\n","\n","  accuracy = 0\n","  total = 0\n","  for (x1, x2, y) in zip(input0, input1, label):\n","    tokens = roberta.encode(x1, x2)\n","    idx = roberta.predict('mnli', tokens).argmax().item()\n","    dictionary = roberta.task.label_dictionary\n","    pred = dictionary[idx + dictionary.nspecial]\n","    total = total + 1\n","    if  (pred == y.strip()) :\n","      accuracy = accuracy + 1\n","\n","  print(g,\": \",accuracy)\n","  print(g,\": \",total)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lvsnFbYgGYLK","executionInfo":{"status":"ok","timestamp":1686920214165,"user_tz":-120,"elapsed":1317224,"user":{"displayName":"Louise Leibbrandt","userId":"11762724927321883770"}},"outputId":"1feb05c1-62dd-4184-e571-8481917b7877"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["slate :  766\n","slate :  2000\n","fiction :  758\n","fiction :  2000\n","telephone :  793\n","telephone :  2000\n","travel :  781\n","travel :  2000\n","government :  733\n","government :  2000\n","verbatim :  401\n","verbatim :  1000\n","facetoface :  378\n","facetoface :  1000\n","oup :  403\n","oup :  1000\n","nineeleven :  373\n","nineeleven :  1000\n","letters :  382\n","letters :  1000\n"]}]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}